[
  {
    "objectID": "shared/deadlines.html",
    "href": "shared/deadlines.html",
    "title": "Deadline information",
    "section": "",
    "text": "TB1: Thursday 11:00\nTB2: to be decided"
  },
  {
    "objectID": "shared/deadlines.html#weekly-meeting",
    "href": "shared/deadlines.html#weekly-meeting",
    "title": "Deadline information",
    "section": "",
    "text": "TB1: Thursday 11:00\nTB2: to be decided"
  },
  {
    "objectID": "shared/deadlines.html#deadlines",
    "href": "shared/deadlines.html#deadlines",
    "title": "Deadline information",
    "section": "Deadlines",
    "text": "Deadlines\n\nWeek 9: Interim report submitted\nWeek 11/12: Feedback on progress so far\nWeek 18: Practical work finished\nWeek 19/20: Analysis of results finished. Start of report write up. Results presented\nWeek 22: Final reports submitted\nWeek 24/25: Final interviews"
  },
  {
    "objectID": "shared/ftnotes/interim-report-structure.html",
    "href": "shared/ftnotes/interim-report-structure.html",
    "title": "Dissipation Learning in Active Matter",
    "section": "",
    "text": "Generalities [700 words]\nModelling of active matter [300 words]\n\nparticle-based (active Brownian particles or active OrnsteinUhlenbeck particles)\nfield theories (Model B+ , Tjhung et al 2023)\n\nLattice models ( Soto, Telo de Gama) [200 words]\nFocus on Persistent Exclusion Process (Background and main results) [200 words]\nAnticipate the overall goal (linking structure to activity), mention the precedent of Rassolov et al. 2022 [200 words]\nPreparatory work up to now [500 words]\n\nbrief reference to the in-house code\nvalidation of the code\ntests in different conditions\nfirst measurements (orientation, cluster sizes, cluster distributions)\ncompare with Soto?\n\nlook back critically: discussion [200 words]\nPlan for the future [200 words]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/ftnotes/Colab.html",
    "href": "shared/ftnotes/Colab.html",
    "title": "Link to colab notebook",
    "section": "",
    "text": "Link to colab notebook\nThis is an experimental notebook for the training of the convolutional neural network onto the data from the Persistence Exclusion Process\nhttps://colab.research.google.com/drive/1B0vXEyb2GG4Mjc5dbd45LIrfcQbReFCr?usp=sharing\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/gitinfo.html",
    "href": "shared/gitinfo.html",
    "title": "Useful Commands and Information for Shared Github Activity",
    "section": "",
    "text": "Useful Commands and Information for Shared Github Activity\n\n\nTable of Contents\n\nTable of Contents\nIntroduction\nCloning\nBasic Git Commands\nBasic Quarto Commands\n\n\n\n1. Introduction\nThis page is an attempt to take commands that pop up in the course of sharing our activity (whether it’s passing it collectively or publishing it to an easily accessible site). Everything here is subject to change and addition as we discover more commands that need remembering. This page should be as standardised and explanatory as possible; when deemed necessary, examples should be given alongside the basic command structure.\n\n\n2. Cloning\nWhen cloning a repository, use:\ngit clone git@github.com:&lt;user&gt;/&lt;repo&gt;\nIf you didn’t use ssh url while cloning, change remote before pushing:\ngit remote set-url origin git@github.com:&lt;user&gt;/&lt;repo&gt;\nFor instance, cloning the persistent exclusion process example script:\ngit remote set-url origin git@github:dlactivematter/persistent-exclusion-process.git\nIf in doubt, use the following to show all the available remotes:\ngit remote -v\n\n\n3. Basic Git Commands\nCheck which branch you are on (and also other available branches):\ngit status\nAdd new file to current branch\ngit add &lt;filename&gt;\nCommit new file to current branch (prompts adding a commit comment)\ngit commit &lt;filename&gt; \nPush all changes to branch\ngit push origin &lt;branch&gt;\n\n\n4. Basic Quarto Commands\nNote: only works on .qmd files\nPreview .qmd document in browser\nquarto preview &lt;filename&gt;\nRender all .qmd files into html (usually not necessary)\nquarto render\nRender all .qmd files into html and publish them to the gh-pages branch\nquarto publish gh-pages\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/cp_network.html",
    "href": "notebooks/cp_network.html",
    "title": "Preliminary CNN Training and Analysis",
    "section": "",
    "text": "This is a brief example of the methodology used throughout the CNN training and analysis part of this project."
  },
  {
    "objectID": "notebooks/cp_network.html#setting-up-the-models-architecture",
    "href": "notebooks/cp_network.html#setting-up-the-models-architecture",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Setting up the model’s architecture",
    "text": "Setting up the model’s architecture\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', strides=(3,3), activation='relu', input_shape=shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', input_shape=shape))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\n#model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\nmodel.add(Dense(units=128, activation='relu'))\n\nwith options({\"layout_optimizer\": False}):\n    model.add(Dropout(0.2))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='linear'))\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 43, 43, 3)         30        \n                                                                 \n batch_normalization (Batch  (None, 43, 43, 3)         12        \n Normalization)                                                  \n                                                                 \n conv2d_1 (Conv2D)           (None, 43, 43, 3)         84        \n                                                                 \n batch_normalization_1 (Bat  (None, 43, 43, 3)         12        \n chNormalization)                                                \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 3)         0         \n D)                                                              \n                                                                 \n conv2d_2 (Conv2D)           (None, 14, 14, 6)         168       \n                                                                 \n batch_normalization_2 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n conv2d_3 (Conv2D)           (None, 14, 14, 6)         330       \n                                                                 \n batch_normalization_3 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 4, 4, 6)           0         \n g2D)                                                            \n                                                                 \n dense (Dense)               (None, 4, 4, 128)         896       \n                                                                 \n dropout (Dropout)           (None, 4, 4, 128)         0         \n                                                                 \n dense_1 (Dense)             (None, 4, 4, 10)          1290      \n                                                                 \n flatten (Flatten)           (None, 160)               0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 161       \n                                                                 \n=================================================================\nTotal params: 3031 (11.84 KB)\nTrainable params: 2995 (11.70 KB)\nNon-trainable params: 36 (144.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "notebooks/cp_network.html#optimizer",
    "href": "notebooks/cp_network.html#optimizer",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Optimizer",
    "text": "Optimizer\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])"
  },
  {
    "objectID": "notebooks/cp_network.html#training-and-evaluation",
    "href": "notebooks/cp_network.html#training-and-evaluation",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Training and evaluation",
    "text": "Training and evaluation\nBefore training, these are the “predictions”:\n\nprediction = model.predict(x_val, batch_size=64)\nprint(\"Shape of prediction : \", np.shape(prediction))\n\nplt.plot(y_val, prediction.T[0], 'o', c='k', alpha=0.25)\nplt.plot(y_val, y_val, 'o', color='r')\n\nprint(\"Pearson's correlation coeff: \", pearsonr(y_val, prediction.T[0]).statistic)\nplt.xlabel(\"Input turning rate\")\nplt.ylabel(\"Predicted turning rate\")\nplt.axis(\"equal\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n2024-02-14 14:12:29.587687: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n313/313 [==============================] - 8s 25ms/step\nShape of prediction :  (20000, 1)\nPearson's correlation coeff:  -0.04497726280558032\n\n\n\n\n\n\ndemo_idx = 100\nplt.matshow(x_val[demo_idx])\nprint(\"Actual: \", y_val[demo_idx])\nprint(\"Predicted: \", prediction.T[0][demo_idx])\n\nActual:  0.034\nPredicted:  0.07371252\n\n\n\n\n\nWe can play with the architecture and see how the untrained predictions can change too."
  },
  {
    "objectID": "notebooks/cp_network.html#run-the-training",
    "href": "notebooks/cp_network.html#run-the-training",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Run the training",
    "text": "Run the training\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs=10,\n    verbose=True,\n    batch_size=64,\n    validation_data=(x_val, y_val)\n)\n\nEpoch 1/10\n781/782 [============================&gt;.] - ETA: 0s - loss: 0.0456 - accuracy: 0.0000e+00782/782 [==============================] - 54s 66ms/step - loss: 0.0456 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0000e+00\nEpoch 2/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - val_loss: 0.0202 - val_accuracy: 0.0000e+00\nEpoch 3/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - val_loss: 0.0207 - val_accuracy: 0.0000e+00\nEpoch 4/10\n782/782 [==============================] - 37s 48ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\nEpoch 5/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\nEpoch 6/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0200 - val_accuracy: 0.0000e+00\nEpoch 7/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\nEpoch 8/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - val_loss: 0.0197 - val_accuracy: 0.0000e+00\nEpoch 9/10\n782/782 [==============================] - 49s 63ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - val_loss: 0.0145 - val_accuracy: 0.0000e+00\nEpoch 10/10\n782/782 [==============================] - 69s 88ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n\n\n2024-02-14 14:13:58.918844: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3276800000 exceeds 10% of free system memory.\n2024-02-14 14:14:55.119286: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n\nprint(\"Evaluate on test data:\")\nresults = model.evaluate(x_val, y_val, batch_size=64, verbose=0)\nprint(\"Test loss:\", results[0])\nprint(\"Test accuracy:\", results[1])\n\nEvaluate on test data:\nTest loss: 0.01768036000430584\nTest accuracy: 0.0\n\n\n2024-02-14 14:22:51.483660: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory."
  },
  {
    "objectID": "activity_log/week9.html",
    "href": "activity_log/week9.html",
    "title": "Dissipation Learning in Active Matter",
    "section": "",
    "text": "MIPS\n2D: DiGregorio et al PRL 2018 3D: Turci & Wilding PRL 2021a\n\\[P_e=\\frac{\\sigma}{D_r}=\\frac{1}{\\alpha}\\]\nMarkov chains\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week8.html",
    "href": "activity_log/week8.html",
    "title": "Week 8",
    "section": "",
    "text": "Week 8\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nProject Plan\nTumbling Rate Clarifications\nDissipation and Structure\nFurther Information\nInterim Report\n\n\n\n1. Introduction\nThe main aim of this week is to finish the Interim Report. As this is not summative for us, the deadline has proven to not be strict - nonetheless I aim to finish it in time.\nThe secondary aim is to clarify various things that have come up throughout the past weeks, first and foremost the general plan of this project. The project plan is outlined below; we are currently still in the Characterisation phase. Percolation and pair-correlations are high priority in order to understand energy dissipation and to then transition into CNN construction.\n\n\n2. Project Plan\nThese are some notes taken during open discussion between us and our supervisor.\n\nCharacterisation - set the landscape of physical properties by analysing the given Persistent Exclusion Process model\n\n\ntimescales\n\\(\\rho\\)-\\(P_{tumble}\\) diagram\ncluster distibution\npercolation\npair-corelations -&gt; link to dissipation\n\n\nConstruction - make a minimal Convoluted Neural Network (CNN) model; retrieve physical properties from data analysis\n\n\ninput details\n\nignore orientations (experiment-like): only use particle positions\ninclude everything: particle orientations and positions\n\npull information from steady-state systems\nvalidate the architecture\n\nhow do we choose architecture options?\n\nmap size\nlayer number\n\nhow much data? can we minimise it?\nhow does it extrapolate?\n\nexplainability\n\ncompare with feature maps\ncheck with explicit ways of measuring dissipation\n\n\n\nExtension\n\n\nuse CNN on non-steady state systems\n\nchart applicability on transition into steady state\n\nextend CNN use to off-lattice models\n\n\n\n3. Tumbling Rate Clarifications\nIn Week 7, I mentioned some potential issues with \\(P_{tumble}\\). This was due to a misunderstanding of the role this variable plays. To reiterate, \\(P_{tumble}\\) is the probability that a particle will tumble in any direction, including the one it is already going into. This means that \\(P_{tumble}\\) is related, but not synonimous, to the rate of changing orientation, because the former has a 25% chance to effectively ‘tumble in place’. This is not an oversight of the model, it is a characteristic of analogising to random thermal fluctuations. Departure from equilibrium is caused as \\(P_{tumble}\\) departs from \\(P_{tumble}^{max}=1\\), irrespective of the probability of actually changing direction.\nThis clarification is useful to keep in mind as we begin talking about energy dissipation.\n\n\n4. Dissipation and Structure\nThere is a prominent link between the structure of an active matter system and energy dissipation. Here energy dissipation is understood as the breaking of detailed balance; we can follow the argument below to trace expression of energy dissipation in terms of structural characteristic \\(P_{tumble}\\):\n\\[P_{tumble}\\sim \\frac{1}{\\tau} \\sim \\frac{1}{P_e} \\sim \\frac{1}{v_{active}}\\] where \\(\\tau\\) is the persistence time, \\(P_e\\) is the Peclet number and \\(v_{active}\\) is active velocity; the latter can be rephrased as a force using a friction coefficient, thereby giving us a ‘negative friction’. Thus the link between dissipation and the tumbling rate is established: a decrease in tumbling rate is an increase in detailed balance breaking, which is an increase in dissipation.\nThere are specific relations we can use to infer dissipation; these will be analysed in subsequent weeks.\n\n\n6. Bin Count Grid\nUsing the cluster analysis shown in Weeks 5-6, we can obtain cluster distribution relations as we vary the tumbling rate and the density. For the following grid of particle distributions:\n\n\n\nCould not load PDF..\n\n\n\nwe get the following cluster grid:\n\n\n\nCould not load PDF..\n\n\n\n\n\n7. Interim Report\nThis is an almost finished version of the interim report, after applying some preliminary supervisor feedback. The finalised version will be done next week. There are some points that should be elaborated regarding CNN functionality, MIPS, percolation, pair correlation function as well as slightly revising the Discussion.  \n\nCould not load PDF..\n\n &lt;/embed&gt;\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week13-15.html",
    "href": "activity_log/week13-15.html",
    "title": "Weeks 13-15",
    "section": "",
    "text": "Weeks 13-15\n\n\n0. Table of Contents\n\nIntroduction\nCluster Convergence\nMachine Learning\nConvolutional Neural Networks (CNNs)\nDataset Rolling\nPreliminary CNN Experiment\nCNN Layers\nPreliminary CNN Tests on Rolled Data\nTraining and Predicting on Mixed Densities\nCurrent Problems\nMisc Notes\n\n\n\n1. Introduction\nThe main purpose of this week is to begin machine learning tests. The secondary purpose is to flesh out some details a\n\n\n2. Cluster Convergence\nOur cluster analysis code currently outputs various clusters organised by their sizes. In practice, we have analysed them by plotting their distributions within a snapshot using histograms (see, for example, Weeks 5-6 Cluster Analysis).\nThe initial thought was to maintain the bin distributions and chart how each individual bin varies over time. This would essentially involve a ‘rolling average’, where the bin values of iteration \\(n\\) would be compared with the average of the previous \\(k\\) iterations for every bin. This would mean a constant account of standard deviations for moving averages, and setting an acceptable threshold beyond which the system can be considered to stabilise. This is obviously complicated - firstly, the histogram bins will not always be constant; there are bins which will have zero clusters, or which will fluctuate even within the steady state regime by virtue of clusters slightly evaporating (past a bin threshold) and then growing back. This is also quite difficult to visualise.\nInstead, the simpler method is to plot the mean of these cluster sizes (weighted by virtue of their categorisation into clusters) and chart how this mean evolves in time. The principle of monitoring stabilisation is the same, but it can be done much simply and visually through a two-dimensional image.\n\n\n3. Machine Learning\nWith tools of analysis developed over the past 12 weeks (and the last few able to be fleshed out in the following weeks), we can move into the machine learning part of the project. This section has some general details about machine learning as a whole and the CNN we are using.\nThe principle of layered machine learning is that of receiving an input vector of datapoints (a ‘layer’ of ‘neurons’) which is fitted with a weight matrix and callibrated with a bias in order to generate successive learning layers; the whole process is given an ‘activation function’, which alters the results after a desired rule (which will be explained below). These weights are trained on the input data in order to correspond to some desired output data, with the intent of generalising this process to work on extended environments and cases. A useful way to express this in linear algebraic notation is:\n\\[\n\\overrightarrow{a}^{(n+1)}=\\sigma(\\textbf{W}\\overrightarrow{a}^{(n)}+\\overrightarrow{b})\n\\] where we take \\(\\overrightarrow{a}^{(n)}\\) to be the n layer of neurons, \\(\\textbf{W}\\) to be the weight matrix, \\(\\overrightarrow{b}\\) to the associated bias vector, and \\(\\sigma\\) to be the activation function. The weight matrix \\(\\textbf{W}\\) applies different weights to every neuron of the previous layer for each neuron of the current layer.\nThe activation function \\(\\sigma\\) is a function which is meant to bind the neuron values within certain restrictions. A common function, for instance, is the sigmoid function, which binds neurons between 0 and 1, flattening very high values in both negative and positive directions towards the former and the latter, respectively. It is illustrated below:\n\\[\n\\sigma_{sigmoid}=\\frac{1}{1+e^{-a}}\n\\]\nThe reasoning behind the sigmoid function is to treat neurons on a continuous gradient between fully activated (1) and fully deactivated (0), in loose analogy with biological neurological systems. The problem with the sigmoid function is precisely the flattening behaviour it displays at high values - a big part of the machine learning process (precisely the ‘learning’ aspect) is the adjustment of weights with the purposes of minimising the cost of deviation from the expected values (which will also be explained below). High weight values will not produce meaningful values which can then be fine-tuned by the system, so the fluctuations in learning improve much less in practice, especially with a lot of inputs and layers involved.\nWhat is often opted for instead, which is the activation function we are using in this project as well, is the Rectified Linear Unit (ReLU) function \\(\\sigma_{ReLU}\\); this function sets any negative number to 0, and any positive number to its own value. Under this function, varying the weights will always yield some feedback, i.e. learning, on how the algorithm can improve to better match expectations.\nHow does a machine learning algorithm figure out how to alter its weights? The brief answer is through an analysis of deviation, or cost. The machine is fed training data alongside the expected values, and it generates predicted values in turn based on the final layer of neurons. The algorithm obtains a cost function by summing the squares of differences between the expected output value of a neuron and its actual value. Not only does this function convey how well an algorithm performs on its training data (the bigger the value of the cost function is, the worse the algorithm is), but its principle can be used to compute a cost gradient - essentially, the algorithm can compute a cost function vector across all its weights and biases (which constitute its ‘dimensions’, in a calculus sense) and then take the gradient to determine how much each weight/bias parameter ought to change.\n\n\n4. Convolutional Neural Networks (CNNs)\nFor our purposes, running a machine learning algorithm for 128x128 lattice sites will get very resource-intensive very quickly. The input layer alone requires more than 16000 neurons, and subsequent layers will require an extremely high number of weights to compile into new neurons, as, per the equation above, each new neuron \\(a_k^{(n)}\\) will depend on all previous neurons \\(\\overrightarrow{a}^{(n-1)}\\). This results in poor scaling with increases in image size.\nFor this reason, alongside considerations of accuracy, this project employs a convolutional neural network. From hereon I will be denoting the number of a layer with \\(l\\). The main difference in principle is that the main successive layer within the network is a convolutional layer, wherein each neuron at layer \\(l\\) is only connected to a given neighbourhood of neurons in layer \\(l-1\\) through a kernel/filter (the entity analogous to the aforementioned weight matrix \\(\\textbf{W}\\)). This kernel ‘moves’ across the input map in strides, generating one neuron value by adding the sum of its respective weights.\nIn broad strokes, the weight function of a standard machine learning algorithm applies holistically to the entire previous layer map, whereas the kernel of a convolutional neural network applies locally to a region of the previous layer map. Both are subsequently subjected to an activation function before yielding a neuronal value.\nThe consequence of a moving kernel is that the (output) feature map of a convolutional layer will be smaller than the input map it analyses. Using many convolutional layers would therefore mean that the edges of the input map would be factored considerably less in the learning algorithm compared to the centre. The feature maps would get smaller and smaller - crucially, the more layers there are, the more focus is shifted towards the central part of the initial map.\nOne approach is to introduce padding. Essentially, padding constitutes inserting values at the edges of the output feature map in order to increase its overall size. In most cases, the aim is to preserve the size of the input map across convolution, and the edges are padded with zeroes (“zero-padding”). Note that the amount of padding \\(P\\) must align with a relational formula of the different sizes of the system in order to be able to construct a valid convolutional layer:\n\\[\n\\frac{V_{i} - F + 2P}{S}-1 \\in \\mathbb{Z}\n\\] where \\(V_i\\) is the input volume size (for square images), F is the receptive field size, and S is the stride.\nThe receptive field size, in turn, can be calculated generally using the following formula (taken from Araujo et al, 2019):\n\\[\nF_0 = \\sum_{l=1}^{L}((k_l-1)\\prod_{i=1}^{l-1}s_i)+1\n\\] where \\(k_l\\) is the kernel size of layer \\(l\\), \\(s_l\\) is the stride size of layer \\(l\\), and \\(L\\) is the final layer of the system.\nOr recursively using the following formula:\n\\[\nF_{l-1} = S_lF_l + (k_l-s_l)\n\\]\nFinally, the amount of kernels determines the depth of a feature map. Essentially, we can picture each kernel as generating a two-dimensional grid, which can be stacked along the z-axis (the depth-axis, in other words). This is important in order to preserve the depth of the original input map, pertinently stored in our case in colour, through an RGB depth of 3. Therefore, for all CNN algorithms which make use of particle orientation in our PEP system, we will need three filters in our convolutional layers.\n\n\n5. Dataset Rolling\nOne key trait of the in-house code has been a ‘rolling’ of the dataset. Essentially, once the iteration of a dataset is generated by the sampler.py function, it is stored under the header conf_{iteration}. This iteration is then ‘rolled’ (using the np.roll function) 12 times along the x-axis, essentially creating new datasets from the same generative process. So far, we have not been making use of this in-house code property in our data at all - this was intended specifically for the machine learning part of the project, so it was not brought up in our analysis (which was strictly focused on the non-rolled datasets). For reference, the rolled datasets are stored within the h5py file as conf_{iteration}_{roll}.\nBut this brings up an interesting point now that we are moving into the machine learning parts. The CNN will be training on all available datasets in order to draw its inferences. To what extent is introducing rolling data favourable to our predictions? On one hand, training the model on recycled data may give it a weighing bias towards the specific conditions of the datasets we present it with. There is the worry of overfitting, or overspecialisation to strictly the circumstances (\\(P_t\\), \\(\\rho\\) and \\((N_x,N_y)\\) combinations) it receives as input. On the other hand, the rolling provides more samples on which the CNN can train, with a much smaller computational cost. The end of this experiment involves large scale data manipulation and generation, and if rolling proves to be a reliable tool to train a model on, it will aid immensely in the generation aspect.\nOn this note, I have done some quick analysis on the data. As a rule of thumb, datasets with rolling take approximately 10 extra seconds to generate in comparison to the strictly unrolled datasets. This has been tested on my personal computer - it is possible the time difference is much smaller once a supercomputer is employed.\nThe file size contrast is considerable. Datasets which have surplus rolled iterations have approximately 284MB each. Datasets which do not only have about 66MB. Since all the files have been stored on my computer, this is considerable insofar as so far around 215MB per dataset have gone unused (though they will now be employed by the CNN). For example, to obtain the biggest cluster size distribution in Weeks 9-12 required 100 different datasets, resulting in about 19GB of data that was not called.\nData analysis aside, we will run the CNN using both options, and contrast the results for different scenarios. I have rewritten sampler.py to accommodate the requirements of unrolled dataset files (as sampler_no_roll.py). This code stores its datasets in a separate folder (data/no-roll).\n\n\n6. Preliminary CNN Experiment\nWe can start with a simple case to illustrate the principle. Taking a single density value of \\(\\rho=0.15\\), we can train the model on a few probability distributions. Running for 5 epochs and using only one convolutional layer, with 3 5x5 kernels with 3x3 strides, we get the following predictions:\n\nwhere the orange data points are the input turning rates, and the blue data points are the model predicitons.\nThe spreads are quite big on most probability values, and for the highest one the model is clearly not fitting properly at all. Nontheless, this is a good start for how simple the layer diagram is:\nINPUT =&gt; CONV =&gt; FLATTEN =&gt; FC (optimiser: adam ; 5 epochs)\nAfter some experimenting, below are the predictions made on the unrolled data, running for 10 epochs:\n\nWith the following layer diagram:\nINPUT =&gt; CONV =&gt; BN =&gt; CONV =&gt; BN =&gt; MAXPOOL =&gt; CONV =&gt; BN =&gt; CONV =&gt; BN =&gt; MAXPOOL =&gt; FC =&gt; DO =&gt; FC =&gt; FLATTEN =&gt; FC (optimiser: adam ; 10 epochs)\nThe training size is also important; this model is trained on one dataset for each probability distribution (so 5 datasets total). Each of these datasets has 1000 snapshots, leaving a total of 5000 data. We trained the model on 3000 snapshots, and then applied it to the last 2000.\nThe natural question is how the predictions change if we train on longer evolutions. Below is the graphical result of this, with the same (\\(\\rho\\),\\(P_t\\),(\\(N_x\\),\\(N_y\\))) configuration, but with 10 times the generated evolution steps (and therefore snapshots). This means the mode is now trained on 30000 snapshots, and then applied to the last 20000.\n\nWe can see the fitting drastically improve for the higher tumbling rates, though it’s clearly still lacking at the extremes of our turning rates.\nTo understand the general principle and methodology of our preliminary analysis, see the uploaded Preliminary Analysis Notebook example.\n\n\n7. CNN Layers\nQuick definitional list for the most commonly used CNN layers.\n\nConvolutional Layer (CONV): Standard convolution layer, mapping the input map through kernels in strides, and generating a feature map of lower dimensionality as a result.\nActivation Function Layer (RELU): Applies the activation function to the emergent neurons. Often left implicit after a CONV or FC layer.\nBatch Normalisation Layer (BN): Normalises layer inputs through re-centring and re-scaling.\nPooling Layer (POOL): Separates input into patches; replaces each patch in input with single value in output, which is often either…\n\nMAXPOOL: the maximum value within the pool\nAVGPOOL: the average value within the pool\n\nDense/Fully Connected Layer (FC): Standard neural network layer; maps each neuron of the feature map to every neuron of the input map.\nDropout Layer (DO): Nullifies contribution of some neurons towards next layer while keeping the rest intact.\nFlattening Layer (FLATTEN): Flattens feature map into a one-dimensional column.\n\n\n\n8. Preliminary CNN Tests on Rolled Data\nSo far we have trained the model strictly on non-rolled datasets. Below is an example of the same 1000 iteration per dataset scenario, but used on rolled datasets to facilitate more samples for the model to train on. As a result, there are 70000 samples in the dataset, 50000 of which are training data, and 20000 of which are validation data.\n\n\n\nPearson’s correlation coeff: 0.98276\n\n\nWe can see that the fitting is even better in the low iteration rolling case than in the high iteration no-rolling case. However, the \\(P_t=0.34\\) values are consistently predicted incorrectly across these two examples, which might suggest an underlying problem with the system.\n\n\n9. Training and Predicting on Mixed Densities\nThe next evident question is how the model holds up when we apply the above principles to a broad range of densities. In the figure below, the CNN runs the same framework on 100 different densities. Unfortunately, I have not been able to construct predictions for such a model yet, as the CUDA memory storage overwhelms my current machine and crashes. As a result, this will be done once the supercomputer system is fully set up.\n\n\n10. Current Problems\nWe are currently training on every snapshot of a lattice’s evolution. This means that our model trains on both steady state and non-steady state segments of a system. In the following weeks it will be worth separating these segments for the purposes of analysing how training the CNN system only on steady state postively or negatively affects its ability to derive tumbling rates.\n\n\n11. Misc Notes\nJ. Chodera - Equilibration in Monte Carlo and molecular dynamics\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week2.html",
    "href": "activity_log/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Week 2\n\n\n0. Table of Contents\n\n0. Table of Contents\n\n\n1. Introduction\n\n\n2. Deadline Schedule\n\n\n3. Text Information\n\n\n4. Motivation Report\n\n\n\n1. Introduction\nThe goal of week 2 is to further familiarise ourselves with the literature. We are to write a quick 500-word referenced report detailing motivations for studying active matter. Notes for this week will therefore be slightly sparser, primarily sifting through the goals of various papers (their detailed processes are to be examined at a later date). Some of these tidbits of information will be taken from the texts explored in week 1; as such, overlap with week 1 information is not only possible, but very likely. Repetition was preferred over disorganised and uncatalogued information, as these notes serve both as an activity log (which must document repeats occasionally) and as a catalogue of information as it is found.\nAnother essential goal of weeks 1 and 2 was to figure out the general examination schedule of the project. This in turn would influence planning the project out more properly, in anticipation of the deadline for the interrim report (other influences, naturally, are personal academic schedules; I personally anticipate being much busier during the first term, and therefore expect higher activity during the latter half of the project). This schedule was not given out to the 30cp version of the project page until very recently, and has therefore been appended in the activity log for this week.\nAbove is a table of contents. Below is a list of a few of the utilised texts, with some key information extracted. The 500-word referenced essay is given afterwards. At the very bottom is a very brief introduction to the Langevin equation. On an informational level, this is meant as a way to ease in into Brownian motion theory; on a technical level, this also gives a brief occasion to experiment with LaTex in markdown.\n\n\n2. Deadline Schedule\n\nWeek 1: Start of project\nWeek 9: Optional interim report submitted. Formative only. Deadline is Thursday 23rd November @ 12.30pm.\nWeek 19: Practical work (experimental or computational/theoretical) must finish by the end of the week. Friday 8th March.\nWeeks 20/21: Analysis of results obtained. Start write up of report. Results to be presented to supervisor and analysis discussed.\nWeek 22: Final Report submission. Deadline is Thursday 18th April @ 12.30pm.\nWeeks 23/24: Final interviews with Assessors and supervisors. Assessment: The final assessment is worth 100% of the total marks available.\n\n(This schedule will be added on the website as a separate tab as well.)\n\n\n3. Text information\n\nThe Mechanics and Statistics of Active Matter\nThis is a 2010 review of (at the time) recent progress within the field. Its main role for my current purposes is to obtain references to older papers (and therefore to their motivations), while also providing a starting definition for active matter.\n\nactive matter can be considered as a type of material\ntake active matter as condensed matter in a nonequilibrium regime\n\neach (autonomous/active) constituent takes direct energetic input\n\nenergy input is therefore homogenously distributed in system\ncompare to fluid motion, for instance: energy is not supplied to each individual particle, but rather is applied (e.g. kinetically) at the boundaries: this then causes particles to push others forward, but they don’t all have direct access to energy\nslightly related, Ramaswamy argues in one of his lectures that this is the key distinction of active matter, phrased as direct access to energy (in the fluid example above, the bulk particles have indirect access to energy)\n\nforce-free: forces exerted between particle and fluid cancel\n(self-propelled) motion is set by particle, not external fields\n\n\n\n\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton, Kistler et al.\nThis is a 2004 paper experimentally confirming a solution to the convection-diffusion relation applied to rod-shaped particles (with Pt and Au segments) moving autonomously in hydrogen peroxide solutions.\n\none of the big nanotechnology challenges is the conversion of stored chemical technology to motion\n\nthis is precisely what many biological active matter systems do\nstudying this would yield useful artifficial active matter systems\n\n\n\n\nActive matter: quantifying the departure from equilibrium, Flenner & Szamel\nThis is a 2020 paper examining active matter systems as they are moved further away from thermodynamic equilibrium.\n\nthe main motivational point here is that quantifying departure from equilibrium helps understand the difference between active matter and equilibium systems, and thus can help chart generalised models of how they both work\n\n\n\n\n4. Motivation Report\nActive matter is, broadly, a subcategory of matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel)[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4]). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n\n499 words. \n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n5. Langevin Equation\nThe Langevin equation represents the partly-random particle movement of a particle within a fluid:\n\\[m \\frac{dv}{dt}=-\\lambda \\mathbf{v} +\\boldsymbol{\\eta} (t)\\] where \\(\\textbf{v}\\) is particle velocity, \\(\\lambda\\) is the damping coefficient, m is the particle mass, \\(\\boldsymbol{\\eta}\\) is the noise term.\nThe noise term indicates collisions of the given particle with other molecules within the fluid. It is determined by a Gaussian distribution, and for the Boltzmann constant \\(k_{B}\\), temperature \\(T\\) and \\(\\eta_{i}\\) being the i-th component of vector \\(\\boldsymbol{\\eta}(t)\\), it is described by the following correlation function:\n\\[\\langle \\eta_{i} (t) \\eta_{j} (t')\\rangle = 2 \\lambda k_{B} T \\delta_{i,j} \\delta(t-t')\\]\nThis approximates that any given force (at time \\(t\\)) is uncorrelated with a force at any other time. The collision time with other molecules indicates this is not the case. However, within great collective motion this is broadly the case. The appearance of the damping coefficient \\(\\lambda\\) in the correlation function is, within an equilibrium system, an expression of the Einstein relation.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week5-6.html",
    "href": "activity_log/week5-6.html",
    "title": "Weeks 5-6",
    "section": "",
    "text": "Weeks 5-6\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nPersistent Exclusion Process Visualisation\nUtility Functions\nTotal Orientation Against Time\nCluster Analysis\nUpdated Motivational Report\n\n\n\n1. Introduction\nThese two weeks broadly overlapped with reading week in the arts department; I used a significant portion of this time to catch up on other projects. As such, I have elected to combine these two weeks. This specific log is nonetheless expected to be shorter than the previous ones.\nThe goals for this week are as follows:\n\nobtaining a working graph of varying tumble probability and orientation (by arranging screnshots with various values next to each other)\n\nmention a few issues in the code\n\nbegin cluster analysis\ncontinue work on the motivational report\n\n\n\n2. Persistent Exclusion Process Visualisation\nIt became apparent that the images obtained in Week 4 using the sampler.py script to generate static images and the view.py script to visualise certain of these static images showed less clustering when compared to the animation in Week 3. Upon closer analysis, the reasoning is due to the way orientation is set up. lattice.py (the script that sets up the lattice in which particles move) assigns orientation ‘0’ to the background, in order to assign it a color (black). It then assigns orientations to the moving particles, depending on orientation. However, the orientations assigned to particles are ‘0’, ‘1’, ‘2’, ‘3’. What this effectively does is give a quarter of the particles the same orientation as the background, thus rendering them invisible. This is a significant information loss.\nAnother issue is the way I have visualised static lattices in the past. In Week 4 I have opted for downloadable .pdf files, which comes in handy for quick downloading in an unchangeable format (with no potential information loss due to poor quality), but is slightly harder to format. As such the titles are slightly cut off (something I did not notice until much later). I will try to fix it, or otherwise I will switch pack to .png formats.\nBelow is a general graph (before having fixed the orientation problem) showing how clustering varies with varying tumble probability.\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nFor contrast, this is how a similar graph looks like after having fixed the orientation issues:\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nThese are different samples generated for the same values of \\(P_{tumble}\\) and \\(\\rho\\). The contrast given by recovering the last 25% of the particles is very stark - some graphs that formerly displayed individual clusters now prove to have them be connected.\nSome qualitative analysis:\n\nlow densities do not display clustering for any \\(P_{tumble}\\), as there are not enough particles in the environment to actually cluster\nonce density is increased, clustering does occur for certain tumble probability values, as long as density is not too large\nwhen density \\(\\rho\\) gets too large, clustering cannot be said to occur because particles occupy most of the simulation area (and in a sense, they form one big cluster which is trivial for our considerations; we’re exploring how autonomous behaviour creates clustering, whereas this clustering is given simply by sheer quantity)\n\nthis can be counterbalanced by increasing \\(P_{tumble}\\) up to a point - for the selected \\(\\rho\\) values this is (mostly) effective, but for a large enough \\(\\rho\\) no amount of tumbling will preserve discrete and separate clustering\n\n\nThere is still the task of making a 10x10 grid which shows a more elaborate change in clustering across a wider range of values. This can get quite cumbersome and needs to be rendered in a large enough size for the quality to be preserved.\n\n\n3. Utility Functions\nBelow is the script utils.py written by my lab partner, which features two utility functions which will come up occasionally in code. They unpack the h5py files in which the persistent exclusion process data is stored, and obtain unique iteration numbers or mean orientations.\n\"\"\"\nUtility functions\n\"\"\"\nimport re\nimport h5py\nimport numpy as np\n\n\ndef get_ds_iters(key_list: list) -&gt; list:\n    \"\"\"\n    Get all the unique iteration numbers\n\n    :param key_list: a list of all the possible dataset keys/names\n\n    :returns: a list of unique iterations\n    \"\"\"\n    iter_n = []\n    for val in key_list:\n        if re.search(\"^conf_\\d+$\", val):\n            iter_n.append(int(val[5:]))\n    return sorted(iter_n)\n\n\ndef get_mean_orientation(file) -&gt; list:\n    \"\"\"\n    Get the mean orientation at each iteration\n\n    :param file: the h5 file to open [str]\n    :returns: mean orientation [list] of length 1000\n\n    Go through all iteration\n    \"\"\"\n    hf = h5py.File(file, \"r\")\n    key_list = list(hf.keys())\n    iter_n = get_ds_iters(key_list)\n    ori = []\n    ori_acm = []\n    for idx, val in enumerate(iter_n):\n        sshot = np.array(hf[f\"conf_{val}\"]).flatten()\n        avg_ori = np.average(sshot[np.where(sshot != 0)[0]] - 1)\n        ori.append(avg_ori)\n        ori_acm.append(np.mean(ori))\n    return ori_acm\n\n\n4. Total Orientation Against Time\nThe next thing to do is to show how the total orientation of the system differs with time. As a quick preliminary demonstration, we can make use of the video.py function and the total orientation it displays for every frame. Below are two demonstrations for the same particle density, \\(\\rho=0.3\\)\n\n\\(\\rho=0.3; P_{tumble}=0.05\\)\n\n\n\n\n\n\n\nReal-Space Animation\nOrientation Frame Analysis\n\n\n\n\n\n\n\n\n\n\n\n\\(\\rho=0.3; P_{tumble}=0.2\\)\n\n\n\n\n\n\n\nReal-Space Animation\nOrientation Frame Analysis\n\n\n\n\n\n\n\n\n\nThe animations only run over 50 frames - this may be too small a sample size to gauge what’s actually going on. Nonetheless, some basic traits can be inferred:\n\nfor the \\(P_{tumble}=0.05\\) case, the persistent length is long enough that the system does not fundamentally change; fluctuations in total orientation are quite small, of the order \\(10^{-4}\\)\nfor the \\(P_{tumble}=0.2\\) case, the persistent length is shorter - this means that the initial state of the system is heavily chaotic, and begins changing rapidly as clusters form. We can observe a stabilisation effect, which slowly brings fluctuations to about the same order of magnitude as the one above\n\nnote that this is only qualitative analysis so far\n\n\n\n\n\n5. Cluster Analysis\nThe process consists of splitting the particles using neighbour analysis. This is done with scipy.ndimage, using a kernel that only engages with vertical and horizontal neighbours (the ‘1’ values) and disregards diagonal neighbours (the ‘0’ values). The justification for this is that particles only have freedom of movement along vertical and horizontal directions; clustering as such emerges when inner particles are prohibited from moving by outer particles. If a particle is present on the diagonal, it cannot be said to contribute to the cluster; as such, it is disregarded.\nThe code bellow constitutes the main analysis done in this section. It leverages h5py file manipulation and uses a locally-created function (utils.get_ds_iters) for obtaining individual iterations from within it. This code is heavily borrowed from my lab partner’s version (external link), which in turn makes use of our supervisor’s code.\nfrom scipy import ndimage\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport utils\nimport cmcrameri #for different cmaps\n\nPt=0.157 #tumble probability\nrho=0.3 #particle density\nfile = (\"../data/dataset_tumble_{}_{}.h5\".format(Pt,rho)) #change this to analyse different file\n\ncmap1 = plt.get_cmap(name=\"gnuplot\",lut=5) #cmap for first picture\ncmap2 = plt.get_cmap(name=\"cmc.tokyoS\") #cmap for second picture\nhfile = h5py.File(file,\"r\")\n\nfig, (regplot, clusterplot, clusterhistogram) = plt.subplots (1,3,figsize=(9,3),width_ratios=(1,1,1.3),constrained_layout=True)\n\niters = utils.get_ds_iters(hfile.keys())\nfig.suptitle(\"Cluster Analysis (P={}; rho={})\".format(Pt,rho))\n\n#plot regular graph\nimage = hfile[f\"conf_{iters[-1]}\"]\nregplot.matshow(image,cmap=cmap1)\n\n#plot cluster separation graph\nkernel = [[0,1,0],\n          [1,1,1],\n          [0,1,0]]\nlabelled, nlabels = ndimage.label(image,structure=kernel)\nclusterplot.matshow(labelled,cmap=cmap2)\n\n#plot histogram of obtained clusters\ncluster_sizes = np.bincount(labelled.flatten())[1:]\nbin_edges = np.linspace(cluster_sizes.min(),cluster_sizes.max(),100)\ncounts, _ = np.histogram(cluster_sizes,bins=bin_edges,density=True)\nclusterhistogram.grid(alpha=.4)\nclusterhistogram.set_axisbelow(True)\nclusterhistogram.scatter(bin_edges[:-1],counts,edgecolor=(0,0,0,1),facecolor=(0,0,0,.5))\nclusterhistogram.set_yscale(\"log\"), clusterhistogram.set_xscale(\"log\")\nclusterhistogram.set_xlabel(\"Cluster Size\")\n\nfig.colorbar(plt.cm.ScalarMappable(cmap=cmap1),ax=regplot)\nfig.colorbar(plt.cm.ScalarMappable(cmap=cmap2),ax=clusterplot)\nplt.show()\nBelow are a few results with varying $P_{tumble} and a fixed \\(\\rho=0.3\\)\n\n\n\n\n\n6. Updated Motivational Report\n\nFor full evolution history of the report, see the Motivational Report page.\nActive matter is, broadly, a subcategory of condensed matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a medium. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the world of living organisms, where energy is constantly dissipated in order to perform various biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2].\nThis biological emulation through physical models is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3]. Note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics (the autonomous individual behaviour of a human) under duress might deal holistically, rather than individually, with other human agents[4]. The issue is that most active matter systems only form individual relationships between agents, and do not account for the way an agent interacts with the group as a whole - the resulting individual behaviour is merely a summation of the agent’s response to each other agent around it. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds as a collective, and take actions in relation to the collective itself. This objection rests on the assumption that this holistic heuristic does not emerge from individual relations, of course (in which case mapping relationships strictly between individuals is unproblematic).\nThese insights lead to the exploration of various models. For flocks of birds, individual cogntive heuristics tend to suffice - self-propelled particles with adaptive movement patterns based on neighbours can accurately reproduce some migrational patterns [5]. Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[6]. Bacteria lends a great example for exploring the intertwining of phenomena to be emulated by active matter. Some strains (such as Bacillus subtilis) can be modelled using both direct physical interaction (between individuals) and long-distance biochemical signalling (within the collective), with complexity and clustering developing in response to harsh external conditions [7]. The latter interaction is called quorum sensing, the adaptation of the individual to local population density; this has developed into its own active matter branch of individual-to-collective behaviour [8]. Using such models, it is possible to recover the aforementioned human holistic cognitive heuristics [9].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic framework. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states. Exploring how these generalisations would hold as departure from equilibrium through various means is increased is then paramount[10]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[11].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[12]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n756 words.\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nNovel Type of Phase Transition in a System of Self-Driven Particles, Vicsek et al. (1995)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nFormation of complex bacterial colonies via self-generated vortices, Czirok et al. (1996)\nSelf-organization of active particles by quorum sensing rules, Bäuerle et al. (2018)\nQuorum sensing as a mechanism to harness the wisdom of the crowds, Moreno-Gámez et al. (2023)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the home page (or landing spot) for the Dissipative Learning in Active Matter project! This page links to a few useful pages for the project as a whole. For weekly lab notes, see the activity log, also available on the sidebar.\nSome generally useful links for day to day sharing and work:\n\nDeadlines : A quick list of deadlines for this project.\nRepository information : Info on how to set up the repository.\nCommands Information : Useful commands for navigating the repository and publishing pages."
  },
  {
    "objectID": "index.html#information",
    "href": "index.html#information",
    "title": "Home",
    "section": "",
    "text": "Welcome to the home page (or landing spot) for the Dissipative Learning in Active Matter project! This page links to a few useful pages for the project as a whole. For weekly lab notes, see the activity log, also available on the sidebar.\nSome generally useful links for day to day sharing and work:\n\nDeadlines : A quick list of deadlines for this project.\nRepository information : Info on how to set up the repository.\nCommands Information : Useful commands for navigating the repository and publishing pages."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html",
    "title": "Preliminary CNN Training and Analysis",
    "section": "",
    "text": "This is a brief example of the methodology used throughout the CNN training and analysis part of this project."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#setting-up-the-models-architecture",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#setting-up-the-models-architecture",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Setting up the model’s architecture",
    "text": "Setting up the model’s architecture\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', strides=(3,3), activation='relu', input_shape=shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', input_shape=shape))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\n#model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\nmodel.add(Dense(units=128, activation='relu'))\n\nwith options({\"layout_optimizer\": False}):\n    model.add(Dropout(0.2))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='linear'))\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 43, 43, 3)         30        \n                                                                 \n batch_normalization (Batch  (None, 43, 43, 3)         12        \n Normalization)                                                  \n                                                                 \n conv2d_1 (Conv2D)           (None, 43, 43, 3)         84        \n                                                                 \n batch_normalization_1 (Bat  (None, 43, 43, 3)         12        \n chNormalization)                                                \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 3)         0         \n D)                                                              \n                                                                 \n conv2d_2 (Conv2D)           (None, 14, 14, 6)         168       \n                                                                 \n batch_normalization_2 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n conv2d_3 (Conv2D)           (None, 14, 14, 6)         330       \n                                                                 \n batch_normalization_3 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 4, 4, 6)           0         \n g2D)                                                            \n                                                                 \n dense (Dense)               (None, 4, 4, 128)         896       \n                                                                 \n dropout (Dropout)           (None, 4, 4, 128)         0         \n                                                                 \n dense_1 (Dense)             (None, 4, 4, 10)          1290      \n                                                                 \n flatten (Flatten)           (None, 160)               0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 161       \n                                                                 \n=================================================================\nTotal params: 3031 (11.84 KB)\nTrainable params: 2995 (11.70 KB)\nNon-trainable params: 36 (144.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#optimizer",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#optimizer",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Optimizer",
    "text": "Optimizer\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])"
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#training-and-evaluation",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#training-and-evaluation",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Training and evaluation",
    "text": "Training and evaluation\nBefore training, these are the “predictions”:\n\nprediction = model.predict(x_val, batch_size=64)\nprint(\"Shape of prediction : \", np.shape(prediction))\n\nplt.plot(y_val, prediction.T[0], 'o', c='k', alpha=0.25)\nplt.plot(y_val, y_val, 'o', color='r')\n\nprint(\"Pearson's correlation coeff: \", pearsonr(y_val, prediction.T[0]).statistic)\nplt.xlabel(\"Input turning rate\")\nplt.ylabel(\"Predicted turning rate\")\nplt.axis(\"equal\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n2024-02-14 14:12:29.587687: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n313/313 [==============================] - 8s 25ms/step\nShape of prediction :  (20000, 1)\nPearson's correlation coeff:  -0.04497726280558032\n\n\n\n\n\n\ndemo_idx = 100\nplt.matshow(x_val[demo_idx])\nprint(\"Actual: \", y_val[demo_idx])\nprint(\"Predicted: \", prediction.T[0][demo_idx])\n\nActual:  0.034\nPredicted:  0.07371252\n\n\n\n\n\nWe can play with the architecture and see how the untrained predictions can change too."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#run-the-training",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/cp_network.html#run-the-training",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Run the training",
    "text": "Run the training\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs=10,\n    verbose=True,\n    batch_size=64,\n    validation_data=(x_val, y_val)\n)\n\nEpoch 1/10\n781/782 [============================&gt;.] - ETA: 0s - loss: 0.0456 - accuracy: 0.0000e+00782/782 [==============================] - 54s 66ms/step - loss: 0.0456 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0000e+00\nEpoch 2/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - val_loss: 0.0202 - val_accuracy: 0.0000e+00\nEpoch 3/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - val_loss: 0.0207 - val_accuracy: 0.0000e+00\nEpoch 4/10\n782/782 [==============================] - 37s 48ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\nEpoch 5/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\nEpoch 6/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0200 - val_accuracy: 0.0000e+00\nEpoch 7/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\nEpoch 8/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - val_loss: 0.0197 - val_accuracy: 0.0000e+00\nEpoch 9/10\n782/782 [==============================] - 49s 63ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - val_loss: 0.0145 - val_accuracy: 0.0000e+00\nEpoch 10/10\n782/782 [==============================] - 69s 88ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n\n\n2024-02-14 14:13:58.918844: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3276800000 exceeds 10% of free system memory.\n2024-02-14 14:14:55.119286: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n\nprint(\"Evaluate on test data:\")\nresults = model.evaluate(x_val, y_val, batch_size=64, verbose=0)\nprint(\"Test loss:\", results[0])\nprint(\"Test accuracy:\", results[1])\n\nEvaluate on test data:\nTest loss: 0.01768036000430584\nTest accuracy: 0.0\n\n\n2024-02-14 14:22:51.483660: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory."
  },
  {
    "objectID": "structured_information/landing.html",
    "href": "structured_information/landing.html",
    "title": "Information Glossary",
    "section": "",
    "text": "This is the landing page for a standardised collection of information regarding active matter. A lot of it will consist of information taken from the activity log (which should still serve as the main documentation of progress) and restructuring it in a more accessible way for ease of use."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html",
    "title": "Import packages",
    "section": "",
    "text": "import numpy as np\nimport h5py\nimport glob\nimport re\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\nnp.set_printoptions(precision=3, suppress=True)\n\n2024-02-13 22:12:19.448389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 22:12:19.448407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 22:12:19.448897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-13 22:12:19.452929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#setting-up-the-models-architecture",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#setting-up-the-models-architecture",
    "title": "Import packages",
    "section": "Setting up the model’s architecture",
    "text": "Setting up the model’s architecture\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', strides=(3,3), activation='relu', input_shape=shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', input_shape=shape))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\n#model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\nmodel.add(Dense(units=128, activation='relu'))\n\nwith options({\"layout_optimizer\": False}):\n    model.add(Dropout(0.2))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='linear'))\n\n2024-02-13 22:12:32.657885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2824 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:07:00.0, compute capability: 7.5\n2024-02-13 22:12:32.658004: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 43, 43, 3)         30        \n                                                                 \n batch_normalization (BatchN  (None, 43, 43, 3)        12        \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 43, 43, 3)         84        \n                                                                 \n batch_normalization_1 (Batc  (None, 43, 43, 3)        12        \n hNormalization)                                                 \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 14, 14, 3)        0         \n )                                                               \n                                                                 \n conv2d_2 (Conv2D)           (None, 14, 14, 6)         168       \n                                                                 \n batch_normalization_2 (Batc  (None, 14, 14, 6)        24        \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 14, 14, 6)         330       \n                                                                 \n batch_normalization_3 (Batc  (None, 14, 14, 6)        24        \n hNormalization)                                                 \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 4, 4, 6)          0         \n 2D)                                                             \n                                                                 \n dense (Dense)               (None, 4, 4, 128)         896       \n                                                                 \n dropout (Dropout)           (None, 4, 4, 128)         0         \n                                                                 \n dense_1 (Dense)             (None, 4, 4, 10)          1290      \n                                                                 \n flatten (Flatten)           (None, 160)               0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 161       \n                                                                 \n=================================================================\nTotal params: 3,031\nTrainable params: 2,995\nNon-trainable params: 36\n_________________________________________________________________"
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#optimizer",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#optimizer",
    "title": "Import packages",
    "section": "Optimizer",
    "text": "Optimizer\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])"
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#training-and-evaluation",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#training-and-evaluation",
    "title": "Import packages",
    "section": "Training and evaluation",
    "text": "Training and evaluation\nBefore training, these are the “predictions”:\n\nprediction = model.predict(x_val, batch_size=64)\nprint(\"Shape of prediction : \", np.shape(prediction))\n\nplt.plot(y_val, prediction.T[0], 'o', c='k', alpha=0.25)\nplt.plot(y_val, y_val, 'o', color='r')\n\nprint(\"Pearson's correlation coeff: \", pearsonr(y_val, prediction.T[0]).statistic)\nplt.xlabel(\"Input turning rate\")\nplt.ylabel(\"Predicted turning rate\")\nplt.axis(\"equal\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n32/32 [==============================] - 0s 3ms/step\nShape of prediction :  (2000, 1)\nPearson's correlation coeff:  -0.08972670146476468\n\n\n\n\n\n\ndemo_idx = 100\nplt.matshow(x_val[demo_idx])\nprint(\"Actual: \", y_val[demo_idx])\nprint(\"Predicted: \", prediction.T[0][demo_idx])\n\nActual:  0.157\nPredicted:  -0.11679656\n\n\n\n\n\nWe can play with the architecture and see how the untrained predictions can change too."
  },
  {
    "objectID": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#run-the-training",
    "href": "persistent-exclusion-process/persistent-exclusion-process/notebooks/np_network.html#run-the-training",
    "title": "Import packages",
    "section": "Run the training",
    "text": "Run the training\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs=10,\n    verbose=True,\n    batch_size=64,\n    validation_data=(x_val, y_val)\n)\n\nEpoch 1/10\n125/125 [==============================] - 4s 9ms/step - loss: 0.0948 - accuracy: 0.0000e+00 - val_loss: 0.1232 - val_accuracy: 0.0000e+00\nEpoch 2/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0600 - accuracy: 0.0000e+00 - val_loss: 0.1177 - val_accuracy: 0.0000e+00\nEpoch 3/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0527 - accuracy: 0.0000e+00 - val_loss: 0.1093 - val_accuracy: 0.0000e+00\nEpoch 4/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0462 - accuracy: 0.0000e+00 - val_loss: 0.0787 - val_accuracy: 0.0000e+00\nEpoch 5/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0423 - accuracy: 0.0000e+00 - val_loss: 0.0880 - val_accuracy: 0.0000e+00\nEpoch 6/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0420 - accuracy: 0.0000e+00 - val_loss: 0.0803 - val_accuracy: 0.0000e+00\nEpoch 7/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0394 - accuracy: 0.0000e+00 - val_loss: 0.0593 - val_accuracy: 0.0000e+00\nEpoch 8/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 0.0701 - val_accuracy: 0.0000e+00\nEpoch 9/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0654 - val_accuracy: 0.0000e+00\nEpoch 10/10\n125/125 [==============================] - 1s 7ms/step - loss: 0.0354 - accuracy: 0.0000e+00 - val_loss: 0.0334 - val_accuracy: 0.0000e+00\n\n\n2024-02-13 22:12:50.481364: I external/local_xla/xla/service/service.cc:168] XLA service 0x700b5e37b3d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-02-13 22:12:50.481385: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n2024-02-13 22:12:50.485964: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707862370.555941   87052 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n\n\nprint(\"Evaluate on test data:\")\nresults = model.evaluate(x_val, y_val, batch_size=64, verbose=0)\nprint(\"Test loss:\", results[0])\nprint(\"Test accuracy:\", results[1])\n\nEvaluate on test data:\nTest loss: 0.07958496361970901\nTest accuracy: 0.0"
  },
  {
    "objectID": "activity_log/week1.html",
    "href": "activity_log/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Week 1\n\n\n0. Table of Contents\n\nTable of Contents\nGoals\nCommunication/Recording Method\nInformation Gathering\n\n\n\n1. Goals\nThe main goal of week 1 is to get an introductory grasp of the literature regarding active matter.\nThe secondary goal of week 1 is to set up a communication and recording method for sharing files and information easily throughout this project.\n\n\n2. Communication/Recording Method\nCommunication is done through a github repository. Both myself and my lab partner have personal branches: cp and np respectively. I will try to stick to the convention of referencing my contributions in github commits and code comments as “cp”. We also have a shared branch, where we upload collective work. Per the demands of this assignment, all progress will be documented individually, despite the collaborative nature of the project. There may therefore be repeats between the activity log and various files in the shared branch; wherever possible, the shared folder will be referenced rather than copied (such as the storing of programs and data).\nThere is also the individual gh-pages branch, which hosts a github website which should contain both shared and individual work. This may be the cleanest method of accessing the activity log, though presumably an exported pdf will be required for examination. Nonetheless, I link (embedded) all the relevant pages below:\n\ngithub repository (note that this is private, so it requires an access invite)\nwebsite landing page\n\npersonal activity log\n\n\nThere is also the matter of convention. We have set up weekly supervisor meetings on Thursday. As such, the way weeks are kept track of is slightly unconventional, purely for pragmatic reasons. Week 1, for instance, began on the first Thursday of the university year, and ended on the second Thursday of it. It might therefore seem strange to see major edits in the github repository for, say, the Week 2 activity log on Wednesday night in what would normally be called week 3. I will think of ways to make this easier to keep track of, but the system works for now. There will (hopefully) be an addendum to this paragraph if/when I clarify the record-keeping system.\n\n\n3. Information Gathering\nTaking information from the following articles:\nI. The 2020 motile active roadmap\n\nIntroduction\nActive Brownian particles: from collective phenomena to fundamental physics\n\n\nRun-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers\n\n\nI. The 2020 motile active roadmap, Gompper et al.\n\na. Introduction, Gompper & Roland\nActive matter is a class of nonequilibrium systems composed of a large number of autonomous agents\n\npersistently out of equilibrium (constituents continuously consume energy)\n\nabsence of equilibrium concepts\n\ndetailed balance\nGibbs ensemble & free energy\ntime-reversal symmetry\n\n\ntherefore theories must be constructed on:\n\nsymmetries:\n\npolar shape (regarding polarity of molecules)\nnematic shape (regarding molecules that are aligned loosely parallel)\ninteractions of agents\n\nconservation laws\ndynamic rules\n\nexamples are agent-based standard models\n\nactive Brownian particles\nsquirmers (complemented by continuum field theory)\n\naim is creation of artificial active matter (synthetic micro/nanomachines)\n\nlook to biological active matter\n\npropulsion mechanisms (rotation, translation and periodic altering of shape)\n\ncilia\nflagella\n\nnavigation strategies\n\nchemotaxis: movement/orientation along chemical concentration gradient (toward or away from stimulus)\nphototaxis: movement/orientation towards or away from light\n\n\n(what is the scale lower limit of such behaviour?)\nas such, suggested synthetic micro/nanomachines can utilise:\n\nphoresis\n\ndiffusiophoresis: motion of species A in response to concentration gradient in colloidal species B\nthermophoresis: motion in mixture of particles along temperature gradient (tendency of light molecules to hot and heavy particles to cold)\n\n\n\nswarming: spontaneous self-organisation of active agents in large numbers -&gt; emergent coordinated collective motion on various length scales\n\ndetermined by\n\nagent shape\nsteric interactions\nsensing\nfluctuations\nenvironmentally-mediated interactions\n\nnovel phenomena:\n\nmotility-induced phase separation\nactive turbulence\n\n\n\n\n\nb. Active Brownian particles: from collective phenomena to fundamental physics, Speck\nActive matter makes use of “persistence of motion”; locally broken symmetry, rather than a global preferred direction.\nSynthetic active matter employs particle shape, ultrasound, etc. to facilitate movement.\nJanus particles as important experimental strategy of locomotion: two hemispheres with different surface properties. Example: colloidal particles\n\nsolvent containing H2O2\ncoat one hemisphere in catalyst for H2O2\nresulting local concentration leads to individual particle propulsion along symmetry axis\n\naxis undergoes rotational diffusion due to fluctuation\n\nsingle-particle trajectories with a persistence length analogous to polymers(?)\n\n\n\nActive Brownian Particles (ABPs)\n\npersistent motion\nparticle interactions\n\nshort range\ntypically repulsive\n\nparticles aggregate into clusters (even without cohesive forces)\n\ndynamic feedback between speed and density: motility-induced phase separation (MIPS)\n\n\n\n\n\nII. Run-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers, Soto & Golestanian\nBacteria biofilms constitute development into multicellular communities through aggregation; exhibit novel properties:\n\ndifferentiation\ndelegation of function\n\nFree bacteria, upon interacting with surfaces, adapt to the new conditions by adopting different motility modes\n\nby contrast, biofilms can nucleate when a number of bacteria settle down near a surface and become completely localised\n\nnucleation: first step in formation of a new thermodynamic phase or structure via self-assembly/self-organisation within a substance or mixture\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week4.html",
    "href": "activity_log/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Week 4\n\n\nTable of Contents\n\nTable of Contents\nIntroduction\nMotivational Report Feedback\nVarying Sample Size and Tumble Probability\nMiscellaneous and Unsorted Notes\n\n\n\n1. Introduction\nThe aim of this week is to consolidate the work of previous weeks and to build up qualitative understanding from it. The tasks for the following few weeks are:\n\nLook over supervisor feedback for the motivational report from last week\nMake a 5x10 configuration list of persistent exclusion process particle system from Week 3, varying tumble probability and particle number.\nCombine results with lab partner into a 10x10 grid (qualitatively showing how clustering changes as particle density and tumble probability are altered)\nPlot total orientation against time for various cases\nLook into clustering analysis\n\nOnly the first two points are expected to be done this week.\n\n\n2. Motivation Report Feedback\nI have created a separate website for monitoring development of the Motivational Report here, also accessible through the sidebar. I will address every piece of feedback below.\nMy original context is presented, with the supervisor comments hyperlinked and prefaced by “FT”. Underneath I add my own comments in the form of bullet points. Only the commented parts of the report are shown.\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation\n\nHere I was looking for a broad category to place active matter into; matter systems is indeed too vague. I would have been better off calling it a subcategory of soft matter systems.\nI don’t know exactly where I got the mechanical equilibrium confusion. I may have read some very specific thing that I generalised, but yes, dissipation ought to happen - one of the most important aspects of active matter is the requirement of supplying each autonomous agent with a steady energy supply which they steadily (or perhaps not so steadily in more complex models) use up.\n\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\n\nI forgot that ‘natural world’ in English tends to refer more to general physical processes rather than specifically living organisms; I’ll try to be more specific regarding what active matter models help with understanding.\nFrom the brief look I managed to take at the literature, it seems that discussion of human behaviour in terms of physical systems is quite contentious. In hindsight, I should spend more than a sentence explaining this: the ‘cognitive heuristics’ argument for holism refers to the way humans deal with other humans in immediate crises. Many models will have an individual agent deal with other (in some way) adjacent agents individually; that is to say, it defines its relationship to each agent in turn, and then computes its behaviour. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds (still) as a collective, and take actions in relation to the collective itself. At the time of writing this, it is unclear to me whether there are any active matter models that apply this ‘holistic’ method; the writers I cited, I believe, were criticising the models that do not attempt to do so. This is the case with the basic models I have engaged with so far (such as ABPs). It’s hard to imagine (though not impossible) how such a model can be implemented, but I don’t doubt that newer human-tracking physical models might work in this direction.\nEither way, I’ll look into Andrea Cavagna’s work. I’m interested in exploring this point more in detail.\n\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\n\nI take the point that ‘state’ is the wrong word; another loss in translation. I did mean a more general thermodynamic framework; thermodynamic ‘state’ implies thermal equilibrium, which is exactly what active matter does not have!\nI do get a bit long-winded here; I’ll try to rephrase this paragraph a bit and make sentences more readable\n\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\nYes, I’ll look into examples of pattern formation, as those tend to be quite demonstrative of what active matter study can do.\n\n\n\n3. Varying Sample Size and Tumble Probability\nThere are two independent ways of generating graphical results. The first is to generate frames and animate them into a GIF file, which is done through the script video.py (see Week 3). The other way is what is explored this week: generate a data set with sampler.py and then use view.py to obtain snapshots.\nThe benefits of this are that datasets are stored for reference alongside the images, unlike the gif computation (which only stored the animation). This is useful for record keeping, as well as for understanding the way file storage is organised. The datasets are stored in h5py formats, a way to store huge amounts of data in a compressed manner - the downside is that the understanding process of how data is stored is les straightforward. So far, we have elected to keep a quite unoptimised version of the code for our purposes until we get a firmer grasp of the way the datasets work.\nWe have modified the code slightly to fit our purposes - made a view variables more explicit and standardised some parts. Furthermore, we’ve made the sampler vary with density and tumble probability - our aim is for each of us to generate 50 datasets, with 10 shared density values varied over 5 individual tumble probability values. The end goal, as stated in point 3 in the Introduction, to combine these into a 100 dataset grid.\nEach combination of data is stored in its own .h5 file. They are all indexed by a pandas dataframe, which keeps track of their ascribed particle density, tumble probability, particle speed and iteration count.\nThe particle density \\(\\rho_{p}\\) is defined as a percentage, such that the total number of particles \\(n_{p}\\) is defined by:\n\\[n_{p}=\\rho_{p}n_{x}n_{y}\\] where \\(n_{x}\\) and \\(n_{y}\\) are the dimensions of the lattice sites: the number of lattice sites along the x and y directions, respectively.\nAs defined in Week 3, the tumble probability \\(P_{tumble}\\) is the probability at any given time cycle that a specific particle will change direction. } Below are some preliminary examples of varying the density \\(\\rho_{p} \\in [0.1,0.3,0.5]\\) for a constant tumble probability of \\(P_{tumble} \\cong 0.34\\).\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\nIt’s hard to see exactly how clustering forms at such a high particle density. As such, below are some more examples that vary the density \\(\\rho \\in [0.1,0.2,0.3]\\), under the constant tumble probability of \\(P_{tumble} \\cong 0.073\\). The reasoning is that checking lower particle densities will help avoid noise that gets in the way of clustering, and checking lower tumble probabilities will allow more clustering to occur.\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\nClusters can be observed, especially in the \\(\\rho_p = 0.2\\) case. They are arguably also visible in the \\(\\rho_p = 0.3\\) case, although it’s hard to disentangle clusters from one another - this will be very important later, once we start employing cluster analysis.\nSome other things that were not mentioned last week. In all these diagrams and animations, colours indicate orientations. Clustering can therefore be seen by observing the orientation of exterior particles pushing them into an existing chunk, which is then consequently ‘trapped’.\n\n\n4. Miscellaneous and Unsorted Notes\nThere are a few overarching (long-term) goals to pursue right now.\n\n0. Prepare a dataset for CNN\n\n\n1. grid 2D ABP and bin the density\n\n\n2. Explore PDEs\n\\[\\rho_{1}= D_{1}\\nabla \\rho_{1} + f_{1} (\\rho_{1},\\rho_{2},\\rho_{3}) \\] \\[\\rho_{2}= D_{2}\\nabla \\rho_{2} + f_{2} (\\rho_{1},\\rho_{2},\\rho_{3}) \\] \\[\\rho_{3}= D_{3}\\nabla \\rho_{3} + f_{1} (\\rho_{1},\\rho_{2},\\rho_{3}) \\]\nSome other short-term ideas to explore in the following weeks:\n\nSee radial distribution function (probability) - see for Leonard Jones Fluid\nAttempt a reconstruction of features of systems without breaking any symmetries.\nPlot umble rate against predicted tumble rate\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week7.html",
    "href": "activity_log/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Week 7\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nPotential Troubles with Tumbling\nRevisions for Last Week\nInterim Report Research\nInterim Report Progress\n\n\n\n1. Introduction\nThe aim of this week is mostly to work on the upcoming interim report. As such, most of this week’s activity constitutes in gathering data from various papers. A further potential problem in the code was also spotted with regards to how the tumbling rate is established. Beyond that, some minor revisions for the previous week were mentioned.\n\n\n2. Potential Troubles with Tumbling\nExamining the code a bit further, we noticed a peculiarity of the tumbling process - on the offchance that a particle does decide to tumble (the probability of which is \\(P_{tumble}\\), it to picks one of the four orientations (0,1,2,3) with equal chance to switch its orientation to. This is an issue, in that the particle has a 25% chance to “tumble in place”. This means that \\(P_{tumble}\\) has been consistently off by a factor. The real tumbling rate is:\n\\[\nP_{tumble}^{actual}=\\frac{3}{4}P_{tumble}\n\\]\nFor now, we will operate with the flawed \\(P_{tumble}\\). I will mention when this is fixed; for this week, at least, \\(P_{tumble}\\) will refer to the flawed tumbling rate, and \\(P_{tumble}^{actual}\\) will refer to the actual tumbling rate.\n\n\n3. Revisions for Last Week\nI misunderstood the effect of large densities within run-and-tumble models and what it means for clusters: it isn’t that cluster analysis cannot be done due to every particle being joined in the same cluster. Rather, this is where percolation analysis becomes important - will look into it next week further.\nThere is also the matter of units be established: length can be measured in lattice sites, such as by establishing a lattice site convention as \\(a\\).\nThis is technically not a revision, but I’ve updated the cluster analysis grouping graph from last week to also work in log scale. Here is an example, with the histogram bin fitted for log 2:\n\nThis is quite similar with the fitting done by Soto and Golestanian in their 2014 paper “Run-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers”. More comparison and research is needed, though.\n\n\n4. Interim Report Research\nI’ve done a lot of research into the Persistent Exclusion Process, some of its underlying principles (such as the simple exclusion interaction, run and tumble dynamics and the zero range process), Active Brownian Particles (ABP), and the like. I would usually list a classificatory list with sorted information; however, due to illness and most of my project time being allotted to working on the interim report itself, I could not make the list this week.\nI will instead make a small list of what I have to look into next week, in anticipation of the deadline:\n\ngeneral convolutional neural network usage\nfurther active matter theories\n\nactive matter field theories\n\nModel B\nModel B+\nModel H, potentially\n\ncollision active matter theories\n\nActive Ornstein Uhlenbeck Particles (AOUP)\n\ncompare and contrast with ABPs\n\n\n\ncorrelation between dissipation and spatial patterns\n\nhow to study entropy through clusters\nlink structure with activity\n\nfurther research percolation\n\n\n\n5. Interim Report Progress\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nMany things here are subject to change. This is a combination of the Motivational Report from before and the research we have done this week. There are a few places which have to be completed, and the Discussion section is particularly lackluster as of now. The report is currently 1695 words without references.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/landing.html",
    "href": "activity_log/landing.html",
    "title": "Activity log",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\n\nWeeks 13-15\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\n\nWeeks 5-6\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\n\nWeeks 10-13\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "activity_log/week3.html",
    "href": "activity_log/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Week 3\n\n\n0. Table of Contents\n\nTable of Contents\nGoals\nConfiguring BlueCrystal4\nSmall Github Digression\nPersistent Exclusion Process Code\nCode Examination\n\n\n\n1. Goals\nThe main aim of this week is to begin reproducing a simple active matter system in code. To this end, a suitable environment to work in must be found and utilised. For now we are primarily sticking to python.\nWe have gained access to the Bristol supercomputers BlueCrystal4 and BluePebble (I have access to the former, my lab partner has access to the latter). We will attempt to apply the scripts to the supercomputers.\nIn Week 2 it was suggested that I would go more in depth in statistical analysis. This has been put aside for now, in the interest of getting a feel for how ABP simulations work in practice.\n\n\n2. Configuring BlueCrystal4\nI already have access to BlueCrystal4 (BC4) due to one of my other modules. For that particular module, I am programming in C/C++; as such, I have already configured (installed and loaded) modules compatible with it. These modules are generally installed on a user instance, and loaded using the following bash command: module load &lt;path&gt;. For example, for loading the necessary files for Intel’s C/C++ compiler (icc), I use the following: module load languages/intel/2020-u4.\nThe issue that arises is that, at least initially, we have decided to work in Python. While I plan to attempt to translate (and speed up) our later codes in C, it is preferable to agree on a shared language while we get our bearings (and Python is easier, for all its faults). I therefore need use both C and python modules; hopefully this will not cause conflicts as long as they are not loaded simultaneously. The loading can easily be done in the .bashrc script on the user home instance, which will keep the module loaded for any and all processes. This is inadvised in the BC4 user manual, however - like I said, conflicts can occur. It is much healthier and safer to instead load modules in the .sh script that will send a request to the job queue. This will be elaborated on later.\nYou load/add modules with module load or module add. I have used the former for C and the latter for Python, though strictly to differentiate the two better. Below is a quick list of the different module commands I use:\nmodule avail #Lists available modules; can combine with grep to search for a specific one\n\nmodule load languages/intel/2020-u4 #Loads icc for C/C++\nmodule add languages/anaconda3/2022.11-3.9.13-tensorflow-2.11 #Loads anaconda tensor flow package\nmodule add languages/anaconda3/2020−3.8.5 #Loads full anaconda package\ncurl https://pyenv.run | bash #Loads python environment (non-anaconda alternative)\n\n#NOTE: only one of these should be done for any instance!\n\nmodule list #Lists loaded modules\nTested a quick “Hello, World!” program just to make sure that python runs correctly; it does, at least for now.\n\n\n3. Small Github Digression\nWe have added a shared file regarding various commands for Github and Quarto. It can now be found here, as well as in the landing page, currently under the name ‘Commands Information’.\n\n\n4. Persistent Exclusion Process Code\nThe code models particles moving across various lattice sites, with a ‘tumble’ probability. This is the probability that the particle will change its direction at every iteration (or time ‘tick’). This means that the tumble variable is inversely proportional to the persistence length of the environment.\nNote that this is a (relatively) simple algorithm: every particle has the same chance to change direction (in a more accurate model they would more likely follow a probability distribution instead).Nonetheless, it’s a good start for getting a feel for one of the core topics of this project: the way persistence length influences collective behaviour. In this case, the clustering phenomena is heavily affected.\nBelow are some GIF files with varying tumble speeds \\(P_{tumble}\\) applied to the model. Note that the animation runs at 6 frames per second, and computes 50 frames total.\n\n\\(P_{tumble}=0.0005\\)\n\n\n\nptumble_0.0005\n\n\nThe persistence length is so small here that almost all particles form into clusters.\n\n\n\\(P_{tumble}=0.001\\)\n\n\n\nptumble_0.001\n\n\nThe clustering is still noticeable here, but there are many more free particles roaming already (with a persistence length twice as big as the previous one).\n\n\n\\(P_{tumble}=0.01\\)\n\n\n\nptumble_0.01\n\n\nClusters are far less frequent, but still noticeable. This is a tenfold increase in persistence length compared to the previous case.\n\n\n\\(P_{tumble}=0.1\\)\n\n\n\nptumble_0.1\n\n\nBarely any clusters form here, and when they do, it’s only for a few frames.\n\n\n\\(P_{tumble}=0.33\\)\n\n\n\nptumble_0.33\n\n\nThere are no noticeable clusters.\nNote that if \\(P_{tumble}=1\\), there is no autonomous activity within the system. In other words, particles exercise simple inertial movement in frictionless environment.\n\n\n\n5. Code Examination\nWe have started looking at the code. It’s hard to standardise everything we have talked about into notes format; most of it is commented on the repository (formalised as in-line comments and docstrings). As the status of this repository is unknown at this time (whether it will be included in the final project submissions, that is; it is, after all, an example supplied by the supervisor, which we are analysing to get a better understaning of the topic), I will summarise to the best of my ability. (Note that this will be vastly incomplete, as we have only discussed a few scripts, and only partially!).\n\nlattice.py\n\nestablishes a lattice class\n\ncall each instance that fits within a class an object\n\nestablishes various attributes for the class\n\nNsites: the total amount of lattice sites within system\n‘Nparticles’: the total amount of particles moving around the system (with the stipulation that only one particle can fit in a lattice site at a given time)\nconnectivity=4: we are unsure what this is as of yet; operating under the assumption that this is the amount of neighbours a lattice site is connected to (and thus the amount of places a particle occupying a lattice site can jump to, depending on its orientation)\n\nset_square_connectivity subfunction\n\nrequires values of Nx and Ny (number of sites along x axis and number of sites along y axis) to rectangularly fit the number of total sites (Nx*Ny==Nsites)\ncreates a ‘neighbor table’ using 2D numpy arrays, with each row being an individual lattice site and each column being an individual neighbour (along collectivity)\nthis is flattened into a one-dimensional array, ran through construct_square_neighbor_table (discussed below), then the result is obtained in both a one-dimensional (flattened) version and in a restored two-dimensional version\n\n\nconstruct_square_neighbor_table\n\ntakes the neighbor table from lattice.py as outlined above\nthe fastest way to store data in C is through a one dimensional array called with pointers, as it ensures that all the data is contiguously stored in the same memory address area\n\nthe indexing is of note (and may be useful later)\n\ni,j lattice sites\n\ni goes through Nx positions (rows)\nj goes through Ny positions (columns)\n\nindex: i * Ny + j\n\n\n\n\n\n\n\n2D to 1D array sketch example\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week9-12.html",
    "href": "activity_log/week9-12.html",
    "title": "Weeks 10-13",
    "section": "",
    "text": "Weeks 10-13\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nQuanitfying Percolation\nCluster Convergence\n\n\n\n1. Introduction\n\n\n2. Quantifying Percolation\nWe can begin to quantify percolation by applying the cluster analysis outlined in Weeks 5-6 (specifically subsection 5). By obtaining the size of the biggest cluster, we can see how it varies for different densities - percolation occurs at higher sizes relative to the entire system size (which as of now is still 128x128 lattice sites). The figure below shows such an analysis for different tumbling rates \\(P_t\\). \nWe can see that the highest possible cluster stagnates around zero for lower densities in all graphs. This means that, while clustering does begin to occur, there are many unconnected and small clusters rather than one big cluster reaching across the entire system. Take the clustering analysis picture from Weeks 5-6 below, for instance; we notice obvious non-zero clusters, but they are all individually relatively small to the entire system. This is an evident non-percolation regime.\n\n\n\ntest\n\n\nWhere percolation does start being called into question is when the relative percentage of the cluster becomes comparable to the set density. Sticking with the \\(P_t=0.034\\) case, we can see that by \\(\\rho=0.8\\), the gradient of the plotted curve becomes distinctly linear. This is precisely because the biggest cluster in the system covers the exact density thereof.\nNote of course that the ratio need not exactly cover the density - there is an ambiguous regime where percolation feasibly occurs with most clustering being enmeshed within the same connected cluster, but simply with a lot of agents in constant movement. We can characterise this as the zone around the ‘inflection point’ within the above graphs.\nNote, as well, the inflection region, which is more spread out for a smaller tumbling rate, and conversely is sharper for higher tumbling probabilities.\nTo further examine percolation,\n\n\n\n\n Back to top"
  },
  {
    "objectID": "motivational-report/motivational-report.html",
    "href": "motivational-report/motivational-report.html",
    "title": "Motivational Report",
    "section": "",
    "text": "Motivational Report\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nOriginal Motivational Report\nSupervisor Feedback\nResponding to Supervisor Feedback\n\n\n\n1. Introduction\nThis is the landing spot for work on the Motivational Report. It was initially prompted in Week 2 as a way to get familiarised with the literature; it is expected to be a considerable part of the Interim Report, whose deadline is in Week 9.\n\n\n2. Original Motivational Report (Week 2)\n\nPasted from “Motivational Report” chapter in Week 2\nActive matter is, broadly, a subcategory of matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel)[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4]). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n499 words.\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n3. Supervisor Feedback (Week 3)\n\nComments prefaced with FT and hyperlinked, commented initially on week2.md in main repository (pure markdown version of Week2)\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation )[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature. FT very good\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\n502 words.\n\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n4. Responding to Supervisor Feedback Week 4\nMy original context is presented, with the supervisor comments hyperlinked and prefaced by “FT”. Underneath I add my own comments in the form of bullet points. Only the commented parts of the report are shown.\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation\n\nHere I was looking for a broad category to place active matter into; matter systems is indeed too vague. I would have been better off calling it a subcategory of soft matter systems.\nI don’t know exactly where I got the mechanical equilibrium confusion. I may have read some very specific thing that I generalised, but yes, dissipation ought to happen - one of the most important aspects of active matter is the requirement of supplying each autonomous agent with a steady energy supply which they steadily (or perhaps not so steadily in more complex models) use up.\n\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\n\nI forgot that ‘natural world’ in English tends to refer more to general physical processes rather than specifically living organisms; I’ll try to be more specific regarding what active matter models help with understanding.\nFrom the brief look I managed to take at the literature, it seems that discussion of human behaviour in terms of physical systems is quite contentious. In hindsight, I should spend more than a sentence explaining this: the ‘cognitive heuristics’ argument for holism refers to the way humans deal with other humans in immediate crises. Many models will have an individual agent deal with other (in some way) adjacent agents individually; that is to say, it defines its relationship to each agent in turn, and then computes its behaviour. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds (still) as a collective, and take actions in relation to the collective itself. At the time of writing this, it is unclear to me whether there are any active matter models that apply this ‘holistic’ method; the writers I cited, I believe, were criticising the models that do not attempt to do so. This is the case with the basic models I have engaged with so far (such as ABPs). It’s hard to imagine (though not impossible) how such a model can be implemented, but I don’t doubt that newer human-tracking physical models might work in this direction.\nEither way, I’ll look into Andrea Cavagna’s work. I’m interested in exploring this point more in detail.\n\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\n\nI take the point that ‘state’ is the wrong word; another loss in translation. I did mean a more general thermodynamic framework; thermodynamic ‘state’ implies thermal equilibrium, which is exactly what active matter does not have!\nI do get a bit long-winded here; I’ll try to rephrase this paragraph a bit and make sentences more readable\n\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\nYes, I’ll look into examples of pattern formation, as those tend to be quite demonstrative of what active matter study can do.\n\n#5. Fleshing out Motivational Report Weeks 5-6\nActive matter is, broadly, a subcategory of condensed matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a medium. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the world of living organisms, where energy is constantly dissipated in order to perform various biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2].\nThis biological emulation through physical models is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3]. Note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics (the autonomous individual behaviour of a human) under duress might deal holistically, rather than individually, with other human agents[4]. The issue is that most active matter systems only form individual relationships between agents, and do not account for the way an agent interacts with the group as a whole - the resulting individual behaviour is merely a summation of the agent’s response to each other agent around it. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds as a collective, and take actions in relation to the collective itself. This objection rests on the assumption that this holistic heuristic does not emerge from individual relations, of course (in which case mapping relationships strictly between individuals is unproblematic).\nThese insights lead to the exploration of various models. For flocks of birds, individual cogntive heuristics tend to suffice - self-propelled particles with adaptive movement patterns based on neighbours can accurately reproduce some migrational patterns [5]. Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[6]. Bacteria lends a great example for exploring the intertwining of phenomena to be emulated by active matter. Some strains (such as Bacillus subtilis) can be modelled using both direct physical interaction (between individuals) and long-distance biochemical signalling (within the collective), with complexity and clustering developing in response to harsh external conditions [7]. The latter interaction is called quorum sensing, the adaptation of the individual to local population density; this has developed into its own active matter branch of individual-to-collective behaviour [8]. Using such models, it is possible to recover the aforementioned human holistic cognitive heuristics [9].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic framework. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states. Exploring how these generalisations would hold as departure from equilibrium through various means is increased is then paramount[10]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[11].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[12]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n756 words.\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nNovel Type of Phase Transition in a System of Self-Driven Particles, Vicsek et al. (1995)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nFormation of complex bacterial colonies via self-generated vortices, Czirok et al. (1996)\nSelf-organization of active particles by quorum sensing rules, Bäuerle et al. (2018)\nQuorum sensing as a mechanism to harness the wisdom of the crowds, Moreno-Gámez et al. (2023)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/repo.html",
    "href": "shared/repo.html",
    "title": "Repository information",
    "section": "",
    "text": "Forking\nLocal development\nManaging branches and commits\nInitialise a Quarto project\nBuilding pages\nPush the changes\nAccessing the live website\nA brief guide on how to setup this website elsewhere and develop the wiki."
  },
  {
    "objectID": "shared/repo.html#forking",
    "href": "shared/repo.html#forking",
    "title": "Repository information",
    "section": "Forking",
    "text": "Forking\n\nIf using a Codeberg account, fork this repo.\nIf using a GitHub account, fork this repo."
  },
  {
    "objectID": "shared/repo.html#local-development",
    "href": "shared/repo.html#local-development",
    "title": "Repository information",
    "section": "Local development",
    "text": "Local development\nIt’s recommended to use SSH and SSH keys when working with a remote git.\n\nKey generating instructions are found here (this is git-server agnostic).\nFor GitHub, follow this to add your key.\nFor Codeberg, it’s similar, navigate to this link when logged in, and then Add key under the Manage SSH Keys section.\n\nClone the repo locally:\n\nFor Codeberg:\n\ngit clone git@codeberg.org:&lt;user&gt;/msci-wiki.git\n\nFor GitHub:\n\ngit clone git@github.com:&lt;user&gt;/msci-wiki.git"
  },
  {
    "objectID": "shared/repo.html#managing-branches-and-commits",
    "href": "shared/repo.html#managing-branches-and-commits",
    "title": "Repository information",
    "section": "Managing branches and commits",
    "text": "Managing branches and commits\nMake a new branch to build your own website from with\ngit checkout -b &lt;make_your_own_unique_name&gt;\nThis branch will contain individual work + shared stuff. It should be the source of truth.\nFrom now on, we will refer to this branch as my_branch.\nWhen contributing to shared knowledge, switch to shared branch with git checkout shared. In this branch, there should be only one directory called shared. This is the root folder of all shared information. Any further directory structuring occurs within this, not outside of it.\n\nNo individual work goes into the shared branch to avoid conflicts with filenames!\n\nWhen merging new commits containing shared information from shared to my_branch, switch to my_branch with git checkout my_branch, and then perform a merge with\ngit merge shared\n\nDon’t merge from my_branch to shared!\n\nIn the case there are problematic commits in shared, future merge can be done with cherry-pick. Say this is shared:\nSHA0 (HEAD): I want this commit\nSHA1: I don't want this commit\nSHA2: I want this commit\nThen, in my_branch:\ngit cherry-pick SHA2\ngit cherry-pick SHA0"
  },
  {
    "objectID": "shared/repo.html#initialise-a-quarto-project",
    "href": "shared/repo.html#initialise-a-quarto-project",
    "title": "Repository information",
    "section": "Initialise a Quarto project",
    "text": "Initialise a Quarto project\nIn my_branch, make a new Quarto project with the website template:\nquarto create project website\nTake a look at _quarto.yml from branch np for an example of how to configure it."
  },
  {
    "objectID": "shared/repo.html#building-pages",
    "href": "shared/repo.html#building-pages",
    "title": "Repository information",
    "section": "Building pages",
    "text": "Building pages\nQuarto by default publishes to the branch gh-pages.\nTo build the pages, first, run the following:\nquarto publish gh-pages\nThis will build and push to the gh-pages branch.\n\nIf using GitHub, that’s all you need to do.\nIf using Codeberg, you can view it by going to https://username.codeberg.page/@gh-pages"
  },
  {
    "objectID": "shared/repo.html#push-the-changes",
    "href": "shared/repo.html#push-the-changes",
    "title": "Repository information",
    "section": "Push the changes",
    "text": "Push the changes\nPushing changes to remote servers make rolling back very difficult, make sure everything looks correct first. Use git log and git status to ensure everything has been committed. To push everything to a mirror:\ngit push &lt;remote&gt; &lt;branch&gt;\nTry to keep all branches up-to-date on all mirrors!"
  },
  {
    "objectID": "shared/repo.html#accessing-the-live-website",
    "href": "shared/repo.html#accessing-the-live-website",
    "title": "Repository information",
    "section": "Accessing the live website",
    "text": "Accessing the live website\n\nFor Codeberg, it’s https://username.codeberg.page/msci-wiki/@gh-pages (more information).\nFor GitHub, it’s https://username.github.io/msci-wiki (more information)."
  },
  {
    "objectID": "shared/ftnotes/label.html",
    "href": "shared/ftnotes/label.html",
    "title": "Minimal cluster analysis",
    "section": "",
    "text": "from scipy import ndimage\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nI create a random binary image (through thresholding).\n\nimage = np.random.uniform(0, 1, (128, 128))\nim = (image &gt; 0.8).astype(int)\n\nplt.matshow(im)\n\n&lt;matplotlib.image.AxesImage at 0x1272e1130&gt;\n\n\n\n\n\nI can then label connected regions, by specifying the structuring element (kernel).\n\nkernel = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\nlabelled, nlabels = ndimage.label(im, structure=kernel)\n\n\nplt.imshow(labelled, cmap=plt.cm.rainbow)\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x12736cc70&gt;\n\n\n\n\n\nI count the number of pixels with a certain label (ignoring label 0 because it is the background)\n\ncluster_sizes = np.bincount(labelled.flatten())[1:]\n\nAnd plot the probability distribution with logarithmically spaced bins\n\nminimum = cluster_sizes.min()\nmaximum = cluster_sizes.max()\nbin_edges = np.logspace(np.log2(minimum), np.log2(maximum), 32, base=2)\nhist, edges = np.histogram(cluster_sizes, bins=bin_edges, density=True)\nplt.plot(2 ** bin_edges[:-1], hist, \"o\")\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.xlabel(\"cluster sizes\")\nplt.ylabel(\"pdf\")\n\nText(0, 0.5, 'pdf')\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/cluster.html",
    "href": "shared/cluster.html",
    "title": "Cluster information",
    "section": "",
    "text": "Project code: PHYS030564\n\n\n\n Back to top"
  }
]