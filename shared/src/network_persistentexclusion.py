# -*- coding: utf-8 -*-
"""Network-persistentExclusion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0vXEyb2GG4Mjc5dbd45LIrfcQbReFCr
"""

import numpy as np
from tensorflow.keras import Sequential
from tensorflow import keras
import h5py
import glob
import re

def extract_floats(string):
  return re.findall(r"[-+]?\d*\.\d+|\d+", string)

def data_load():
  files = glob.glob("/content/drive/My Drive/data/dataset*")
  print (files)
  inputs,outputs = [],[]
  for f in files:
    tumble = float(extract_floats(f)[0])

    with h5py.File(f, "r") as fin:
      count = 0
      for key in fin.keys():
          img = fin[key][:]#.astype(np.float32)
          # img /= img.max()
          img = img.reshape((img.shape[0], img.shape[1],1))
          shape = img.shape
          inputs.append(img)
          outputs.append(tumble)
          count+=1
          if count>1000:
            break

  order = np.arange(len(outputs)).astype(int)
  order=np.random.permutation(order)
  return np.array(inputs)[order],np.array(outputs)[order],shape


x,y,shape = data_load()

import tensorflow as tf
tf.test.gpu_device_name()

last = 1000
x_train, y_train = x[:-last], y[:-last]
x_val,y_val = x[-last:],y[-last:]

model = keras.models.Sequential([
    keras.layers.Conv2D(filters=3, kernel_size=(5,5), strides=(3,3), activation='relu', input_shape=shape, name="conv2d"),
    # keras.layers.BatchNormalization(),
    # keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)),
    # keras.layers.Conv2D(filters=6, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),

    # map all neurons out
    keras.layers.Flatten(),
    # link them together to perform regression (hence the linear activation)
    keras.layers.Dense(1, activation='linear')


    # other layers could be sandwiched earlier: beqare of their danger of overfitting

      # keras.layers.BatchNormalization(),
      # keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
      # keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      # keras.layers.BatchNormalization(),
      # keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      # keras.layers.BatchNormalization(),
      # keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      # keras.layers.BatchNormalization(),
      # keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
      #
      # keras.layers.Dense(4096, activation='relu'),
      # keras.layers.Dropout(0.5),
      # keras.layers.Dense(8, activation='relu'),

      # keras.layers.Dense(4, activation='relu'),
      # keras.layers.Dropout(0.5),
])

model.compile(loss='mean_absolute_error', optimizer='adam')

model.fit(x_train, y_train, #initial_epoch=3,
    epochs=5, verbose=True, batch_size=128,validation_data=(x_val, y_val))


print("Evaluate on test data")
results = model.evaluate(x_val, y_val, batch_size=64)
print("test loss, test acc:", results)


num = 100
prediction = model.predict(
        x_val[:num]
        )
print("Prediction", prediction.shape,y_val.shape)
# for p,v in zip(prediction,y_val):
    # print(p[0],v)
import seaborn as sb
import matplotlib.pyplot as plt

# sb.regplot(y_val[:num], np.concatenate(prediction))
plt.plot(y_val[:num], np.concatenate(prediction), 'o')
plt.plot(y_val[:num],y_val[:num], 'o')
from scipy.stats import pearsonr
pearsonr(y_val[:num], np.concatenate(prediction))
plt.xlabel("input turning rate")
plt.ylabel("predicted turning rate")
plt.axis("equal")

model.summary()

import matplotlib.pyplot as plt
filters, biases = model.layers[0].get_weights()

print (filters.shape[-1])
for k in range(filters.shape[-1]):
  f = filters[:, :, :, k]
  plt.matshow(f.squeeze())
  plt.colorbar()


model.save("mymodel")

model.summary()

