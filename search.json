[
  {
    "objectID": "shared/deadlines.html",
    "href": "shared/deadlines.html",
    "title": "Deadline information",
    "section": "",
    "text": "TB1: Thursday 11:00\nTB2: to be decided"
  },
  {
    "objectID": "shared/deadlines.html#weekly-meeting",
    "href": "shared/deadlines.html#weekly-meeting",
    "title": "Deadline information",
    "section": "",
    "text": "TB1: Thursday 11:00\nTB2: to be decided"
  },
  {
    "objectID": "shared/deadlines.html#deadlines",
    "href": "shared/deadlines.html#deadlines",
    "title": "Deadline information",
    "section": "Deadlines",
    "text": "Deadlines\n\nWeek 9: Interim report submitted\nWeek 11/12: Feedback on progress so far\nWeek 18: Practical work finished\nWeek 19/20: Analysis of results finished. Start of report write up. Results presented\nWeek 22: Final reports submitted\nWeek 24/25: Final interviews"
  },
  {
    "objectID": "shared/ftnotes/interim-report-structure.html",
    "href": "shared/ftnotes/interim-report-structure.html",
    "title": "Dissipation Learning in Active Matter",
    "section": "",
    "text": "Generalities [700 words]\nModelling of active matter [300 words]\n\nparticle-based (active Brownian particles or active OrnsteinUhlenbeck particles)\nfield theories (Model B+ , Tjhung et al 2023)\n\nLattice models ( Soto, Telo de Gama) [200 words]\nFocus on Persistent Exclusion Process (Background and main results) [200 words]\nAnticipate the overall goal (linking structure to activity), mention the precedent of Rassolov et al. 2022 [200 words]\nPreparatory work up to now [500 words]\n\nbrief reference to the in-house code\nvalidation of the code\ntests in different conditions\nfirst measurements (orientation, cluster sizes, cluster distributions)\ncompare with Soto?\n\nlook back critically: discussion [200 words]\nPlan for the future [200 words]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/ftnotes/Colab.html",
    "href": "shared/ftnotes/Colab.html",
    "title": "Link to colab notebook",
    "section": "",
    "text": "Link to colab notebook\nThis is an experimental notebook for the training of the convolutional neural network onto the data from the Persistence Exclusion Process\nhttps://colab.research.google.com/drive/1B0vXEyb2GG4Mjc5dbd45LIrfcQbReFCr?usp=sharing\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/gitinfo.html",
    "href": "shared/gitinfo.html",
    "title": "Useful Commands and Information for Shared Github Activity",
    "section": "",
    "text": "Useful Commands and Information for Shared Github Activity\n\n\nTable of Contents\n\nTable of Contents\nIntroduction\nCloning\nBasic Git Commands\nBasic Quarto Commands\n\n\n\n1. Introduction\nThis page is an attempt to take commands that pop up in the course of sharing our activity (whether it’s passing it collectively or publishing it to an easily accessible site). Everything here is subject to change and addition as we discover more commands that need remembering. This page should be as standardised and explanatory as possible; when deemed necessary, examples should be given alongside the basic command structure.\n\n\n2. Cloning\nWhen cloning a repository, use:\ngit clone git@github.com:&lt;user&gt;/&lt;repo&gt;\nIf you didn’t use ssh url while cloning, change remote before pushing:\ngit remote set-url origin git@github.com:&lt;user&gt;/&lt;repo&gt;\nFor instance, cloning the persistent exclusion process example script:\ngit remote set-url origin git@github:dlactivematter/persistent-exclusion-process.git\nIf in doubt, use the following to show all the available remotes:\ngit remote -v\n\n\n3. Basic Git Commands\nCheck which branch you are on (and also other available branches):\ngit status\nAdd new file to current branch\ngit add &lt;filename&gt;\nCommit new file to current branch (prompts adding a commit comment)\ngit commit &lt;filename&gt; \nPush all changes to branch\ngit push origin &lt;branch&gt;\n\n\n4. Basic Quarto Commands\nNote: only works on .qmd files\nPreview .qmd document in browser\nquarto preview &lt;filename&gt;\nRender all .qmd files into html (usually not necessary)\nquarto render\nRender all .qmd files into html and publish them to the gh-pages branch\nquarto publish gh-pages\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/cp_network.html",
    "href": "notebooks/cp_network.html",
    "title": "Preliminary CNN Training and Analysis",
    "section": "",
    "text": "This is a brief example of the methodology used throughout the CNN training and analysis part of this project."
  },
  {
    "objectID": "notebooks/cp_network.html#setting-up-the-models-architecture",
    "href": "notebooks/cp_network.html#setting-up-the-models-architecture",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Setting up the model’s architecture",
    "text": "Setting up the model’s architecture\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', strides=(3,3), activation='relu', input_shape=shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=3, kernel_size=(3,3), padding='same', input_shape=shape))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\n#model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=6, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\nmodel.add(Dense(units=128, activation='relu'))\n\nwith options({\"layout_optimizer\": False}):\n    model.add(Dropout(0.2))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='linear'))\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 43, 43, 3)         30        \n                                                                 \n batch_normalization (Batch  (None, 43, 43, 3)         12        \n Normalization)                                                  \n                                                                 \n conv2d_1 (Conv2D)           (None, 43, 43, 3)         84        \n                                                                 \n batch_normalization_1 (Bat  (None, 43, 43, 3)         12        \n chNormalization)                                                \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 3)         0         \n D)                                                              \n                                                                 \n conv2d_2 (Conv2D)           (None, 14, 14, 6)         168       \n                                                                 \n batch_normalization_2 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n conv2d_3 (Conv2D)           (None, 14, 14, 6)         330       \n                                                                 \n batch_normalization_3 (Bat  (None, 14, 14, 6)         24        \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 4, 4, 6)           0         \n g2D)                                                            \n                                                                 \n dense (Dense)               (None, 4, 4, 128)         896       \n                                                                 \n dropout (Dropout)           (None, 4, 4, 128)         0         \n                                                                 \n dense_1 (Dense)             (None, 4, 4, 10)          1290      \n                                                                 \n flatten (Flatten)           (None, 160)               0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 161       \n                                                                 \n=================================================================\nTotal params: 3031 (11.84 KB)\nTrainable params: 2995 (11.70 KB)\nNon-trainable params: 36 (144.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "notebooks/cp_network.html#optimizer",
    "href": "notebooks/cp_network.html#optimizer",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Optimizer",
    "text": "Optimizer\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])"
  },
  {
    "objectID": "notebooks/cp_network.html#training-and-evaluation",
    "href": "notebooks/cp_network.html#training-and-evaluation",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Training and evaluation",
    "text": "Training and evaluation\nBefore training, these are the “predictions”:\n\nprediction = model.predict(x_val, batch_size=64)\nprint(\"Shape of prediction : \", np.shape(prediction))\n\nplt.plot(y_val, prediction.T[0], 'o', c='k', alpha=0.25)\nplt.plot(y_val, y_val, 'o', color='r')\n\nprint(\"Pearson's correlation coeff: \", pearsonr(y_val, prediction.T[0]).statistic)\nplt.xlabel(\"Input turning rate\")\nplt.ylabel(\"Predicted turning rate\")\nplt.axis(\"equal\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n2024-02-14 14:12:29.587687: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n313/313 [==============================] - 8s 25ms/step\nShape of prediction :  (20000, 1)\nPearson's correlation coeff:  -0.04497726280558032\n\n\n\n\n\n\ndemo_idx = 100\nplt.matshow(x_val[demo_idx])\nprint(\"Actual: \", y_val[demo_idx])\nprint(\"Predicted: \", prediction.T[0][demo_idx])\n\nActual:  0.034\nPredicted:  0.07371252\n\n\n\n\n\nWe can play with the architecture and see how the untrained predictions can change too."
  },
  {
    "objectID": "notebooks/cp_network.html#run-the-training",
    "href": "notebooks/cp_network.html#run-the-training",
    "title": "Preliminary CNN Training and Analysis",
    "section": "Run the training",
    "text": "Run the training\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs=10,\n    verbose=True,\n    batch_size=64,\n    validation_data=(x_val, y_val)\n)\n\nEpoch 1/10\n781/782 [============================&gt;.] - ETA: 0s - loss: 0.0456 - accuracy: 0.0000e+00782/782 [==============================] - 54s 66ms/step - loss: 0.0456 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0000e+00\nEpoch 2/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - val_loss: 0.0202 - val_accuracy: 0.0000e+00\nEpoch 3/10\n782/782 [==============================] - 50s 64ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - val_loss: 0.0207 - val_accuracy: 0.0000e+00\nEpoch 4/10\n782/782 [==============================] - 37s 48ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\nEpoch 5/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\nEpoch 6/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0200 - val_accuracy: 0.0000e+00\nEpoch 7/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\nEpoch 8/10\n782/782 [==============================] - 43s 55ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - val_loss: 0.0197 - val_accuracy: 0.0000e+00\nEpoch 9/10\n782/782 [==============================] - 49s 63ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - val_loss: 0.0145 - val_accuracy: 0.0000e+00\nEpoch 10/10\n782/782 [==============================] - 69s 88ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n\n\n2024-02-14 14:13:58.918844: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3276800000 exceeds 10% of free system memory.\n2024-02-14 14:14:55.119286: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory.\n\n\n\nprint(\"Evaluate on test data:\")\nresults = model.evaluate(x_val, y_val, batch_size=64, verbose=0)\nprint(\"Test loss:\", results[0])\nprint(\"Test accuracy:\", results[1])\n\nEvaluate on test data:\nTest loss: 0.01768036000430584\nTest accuracy: 0.0\n\n\n2024-02-14 14:22:51.483660: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1310720000 exceeds 10% of free system memory."
  },
  {
    "objectID": "activity_log/week9-12.html",
    "href": "activity_log/week9-12.html",
    "title": "Weeks 10-13",
    "section": "",
    "text": "Weeks 10-13\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nQuanitfying Percolation\nCluster Convergence\n\n\n\n1. Introduction\n\n\n2. Quantifying Percolation\nWe can begin to quantify percolation by applying the cluster analysis outlined in Weeks 5-6 (specifically subsection 5). By obtaining the size of the biggest cluster, we can see how it varies for different densities - percolation occurs at higher sizes relative to the entire system size (which as of now is still 128x128 lattice sites). The figure below shows such an analysis for different tumbling rates \\(P_t\\). \nWe can see that the highest possible cluster stagnates around zero for lower densities in all graphs. This means that, while clustering does begin to occur, there are many unconnected and small clusters rather than one big cluster reaching across the entire system. Take the clustering analysis picture from Weeks 5-6 below, for instance; we notice obvious non-zero clusters, but they are all individually relatively small to the entire system. This is an evident non-percolation regime.\n\n\n\ntest\n\n\nWhere percolation does start being called into question is when the relative percentage of the cluster becomes comparable to the set density. Sticking with the \\(P_t=0.034\\) case, we can see that by \\(\\rho=0.8\\), the gradient of the plotted curve becomes distinctly linear. This is precisely because the biggest cluster in the system covers the exact density thereof.\nNote of course that the ratio need not exactly cover the density - there is an ambiguous regime where percolation feasibly occurs with most clustering being enmeshed within the same connected cluster, but simply with a lot of agents in constant movement. We can characterise this as the zone around the ‘inflection point’ within the above graphs.\nNote, as well, the inflection region, which is more spread out for a smaller tumbling rate, and conversely is sharper for higher tumbling probabilities.\nTo further examine percolation,\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week9.html",
    "href": "activity_log/week9.html",
    "title": "Dissipation Learning in Active Matter",
    "section": "",
    "text": "MIPS\n2D: DiGregorio et al PRL 2018 3D: Turci & Wilding PRL 2021a\n\\[P_e=\\frac{\\sigma}{D_r}=\\frac{1}{\\alpha}\\]\nMarkov chains\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week8.html",
    "href": "activity_log/week8.html",
    "title": "Week 8",
    "section": "",
    "text": "Week 8\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nProject Plan\nTumbling Rate Clarifications\nDissipation and Structure\nFurther Information\nInterim Report\n\n\n\n1. Introduction\nThe main aim of this week is to finish the Interim Report. As this is not summative for us, the deadline has proven to not be strict - nonetheless I aim to finish it in time.\nThe secondary aim is to clarify various things that have come up throughout the past weeks, first and foremost the general plan of this project. The project plan is outlined below; we are currently still in the Characterisation phase. Percolation and pair-correlations are high priority in order to understand energy dissipation and to then transition into CNN construction.\n\n\n2. Project Plan\nThese are some notes taken during open discussion between us and our supervisor.\n\nCharacterisation - set the landscape of physical properties by analysing the given Persistent Exclusion Process model\n\n\ntimescales\n\\(\\rho\\)-\\(P_{tumble}\\) diagram\ncluster distibution\npercolation\npair-corelations -&gt; link to dissipation\n\n\nConstruction - make a minimal Convoluted Neural Network (CNN) model; retrieve physical properties from data analysis\n\n\ninput details\n\nignore orientations (experiment-like): only use particle positions\ninclude everything: particle orientations and positions\n\npull information from steady-state systems\nvalidate the architecture\n\nhow do we choose architecture options?\n\nmap size\nlayer number\n\nhow much data? can we minimise it?\nhow does it extrapolate?\n\nexplainability\n\ncompare with feature maps\ncheck with explicit ways of measuring dissipation\n\n\n\nExtension\n\n\nuse CNN on non-steady state systems\n\nchart applicability on transition into steady state\n\nextend CNN use to off-lattice models\n\n\n\n3. Tumbling Rate Clarifications\nIn Week 7, I mentioned some potential issues with \\(P_{tumble}\\). This was due to a misunderstanding of the role this variable plays. To reiterate, \\(P_{tumble}\\) is the probability that a particle will tumble in any direction, including the one it is already going into. This means that \\(P_{tumble}\\) is related, but not synonimous, to the rate of changing orientation, because the former has a 25% chance to effectively ‘tumble in place’. This is not an oversight of the model, it is a characteristic of analogising to random thermal fluctuations. Departure from equilibrium is caused as \\(P_{tumble}\\) departs from \\(P_{tumble}^{max}=1\\), irrespective of the probability of actually changing direction.\nThis clarification is useful to keep in mind as we begin talking about energy dissipation.\n\n\n4. Dissipation and Structure\nThere is a prominent link between the structure of an active matter system and energy dissipation. Here energy dissipation is understood as the breaking of detailed balance; we can follow the argument below to trace expression of energy dissipation in terms of structural characteristic \\(P_{tumble}\\):\n\\[P_{tumble}\\sim \\frac{1}{\\tau} \\sim \\frac{1}{P_e} \\sim \\frac{1}{v_{active}}\\] where \\(\\tau\\) is the persistence time, \\(P_e\\) is the Peclet number and \\(v_{active}\\) is active velocity; the latter can be rephrased as a force using a friction coefficient, thereby giving us a ‘negative friction’. Thus the link between dissipation and the tumbling rate is established: a decrease in tumbling rate is an increase in detailed balance breaking, which is an increase in dissipation.\nThere are specific relations we can use to infer dissipation; these will be analysed in subsequent weeks.\n\n\n6. Bin Count Grid\nUsing the cluster analysis shown in Weeks 5-6, we can obtain cluster distribution relations as we vary the tumbling rate and the density. For the following grid of particle distributions:\n\n\n\nCould not load PDF..\n\n\n\nwe get the following cluster grid:\n\n\n\nCould not load PDF..\n\n\n\n\n\n7. Interim Report\nThis is an almost finished version of the interim report, after applying some preliminary supervisor feedback. The finalised version will be done next week. There are some points that should be elaborated regarding CNN functionality, MIPS, percolation, pair correlation function as well as slightly revising the Discussion.  \n\nCould not load PDF..\n\n &lt;/embed&gt;\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week16.html",
    "href": "activity_log/week16.html",
    "title": "Week 16",
    "section": "",
    "text": "Week 16\n\n\n0. Table of Contents\n\n\n1. Introduction\nNow that we have set up the machine learning system and begun familiarising ourselves with its architecture, the aim of the next few weeks (this one included) is to begin experimenting on different training and validation data, as well as tuning the architecture.\n\n\n2. Checklist\nOur evolving checklist of things to explore regarding CNNs can be found here (Sheet 1). The highlight of what has been currently done and what is currently aimed at has been transcribed below.\n\nData augmentation (rolled): noticeable improvement\nMore samples: noticeable improvement\nDifferent alphas (training): noticeable improvement\nGaps in alphas (training)\nDifferent densities\nData augmentation (rotate)\nTune learning rate/optimiser\nTune batch size\n\n\n\n3. Model List and Storage\nIn order to keep track of our many models, involving different layers and different data training sets, we opted to create a standardised table detailing the different parameters going into the system, such as training data snapshots, ratio of training to prediction data, etc. Below is a quick (preliminary) snapshot of the first few lines of the table, for reference. Note that a lot of columns have been skipped, which detail the utilised densities, tumbling rates and system sizes used for training and validation.\n\nThe growing model list can be found here, on the same link as the checklist above (Sheet 2). Below are transcribed our current sought metrics (coloured in green within the table above).\n\nMaximum STD: 0.02\n\nThis is the maximum standard deviation a particular prediction spread can have. For example, given a model, the maximum STD determines the biggest spread in the output predictions (which are generated for some specific tumbling rates). Ideally the spreads would be similar across, but we can consider the learning to have failed if any individual prediction has too big a spread.\n\nAverage STD: 0.01\n\nThis is the average standard deviation a particular prediction spread can have. Taking the same example as above, this averages over all predicted tumbling rates to obtain an average spread deviation. This naturally has to be small as well in order for the model to succeed.\n\nMean Average Error: 0.01\nPearson’s Coefficient: 0.975\nOverlap Ratio: 1\n\nThis ratio indicates the amount of tumbling rate prediction spreads which overlap at all with the expected tumbling rates.\n\n\nCombining a ubiquitous overlap ratio with a very small average and maximum standard deviation yields results which are both very accurate and very precise.\n\n\n4. Shorthand Model List\nWhile each model is referred to by the aforementioned standardisation, it is also useful to communicate architecture more quickly. Here’s the current list of different architecturess being used, all indexed by a model number (MN). The latter predictions generated in Weeks 13-15 are all done with MN_1, for example.\n\nMN_1\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,strides=(3,3),activation=“relu”, input_shape=shape)\nBN\nCONV (filters=3,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nFC (units=128,activation=‘relu’)\nDO (0.2) (without layout optimiser)\nFC (units=10,activation=‘softmax’)\nFLATTEN\nFC (units=1,activation=‘linear’)\n\n\n\nMN_2\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=6,kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nAVGPOOL\nDO (0.2) (without layout optimiser)\nFC (units=64,activation=‘relu’)\nDO (0.2) (without layout optimiser)\nFC (units=10,activation=‘softmax’)\nFLATTEN\nFC (units=1,activation=‘linear’)\n\nAs more architectures are employed, the full list will be updated here.\n\n\n\n3. New Architecture\nUsing the architecture of MN_2, we can try to examine a similar case to which we applied MN_1. Below is the prediction of model atreus1273, applied to a sample size of 70000 iterations spread equally among 5 turning rates (50000 training, 20000 validation), for density 0.15.\n\nThe fitting overall seems to be worse, but the top middle values are almost perfectly fitted. My lab partner has gotten better results with this model, so I will continue exploring it in parallel with MN_1 for now.\n\n\n4. More Tumbling Rates\nThere is an obvious extension in increasing the amount of tumbling rates utilised. This may help the CNN pick up on the clustering/dissipation pattern better by providing more scenarios in which it can be examined.\nBelow is an example reprouction of MN_1 trained on the same values as the last example in Weeks 13-15, model median4431. It has now been standardised to be easliy referred to in our table.\n\nNote that this is using rolled data, so it is bound to be more accurate than the unrolled variants we will consider below.\n\n\n4. Bigger N Values\nHere is a brief attempt to train the CNN on broader N values. This will be investigated more in depth at the end of the project. Model michigan2241 is trained on \\(N_x=N_y=256\\); an example screenshot can be found below.\n\nRunning MN_2 architecture, the training yields the following predictions:\n\nWe can see the fitting does not match\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 256, 256, 3)       30        \n                                                                 \n max_pooling2d (MaxPooling2  (None, 128, 128, 3)       0         \n D)                                                              \n                                                                 \n re_lu (ReLU)                (None, 128, 128, 3)       0         \n                                                                 \n batch_normalization (Batch  (None, 128, 128, 3)       12        \n Normalization)                                                  \n                                                                 \n conv2d_1 (Conv2D)           (None, 128, 128, 6)       456       \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 64, 64, 6)         0         \n g2D)                                                            \n                                                                 \n re_lu_1 (ReLU)              (None, 64, 64, 6)         0         \n                                                                 \n batch_normalization_1 (Bat  (None, 64, 64, 6)         24        \n chNormalization)                                                \n                                                                 \n global_average_pooling2d (  (None, 6)                 0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dropout (Dropout)           (None, 6)                 0         \n                                                                 \n dense (Dense)               (None, 64)                448       \n                                                                 \n dropout_1 (Dropout)         (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n flatten (Flatten)           (None, 10)                0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 1631 (6.37 KB)\nTrainable params: 1613 (6.30 KB)\nNon-trainable params: 18 (72.00 Byte)\n_________________________________________________________________\n\n\n5. Model Summaries\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week13-15.html",
    "href": "activity_log/week13-15.html",
    "title": "Weeks 13-15",
    "section": "",
    "text": "Weeks 13-15\n\n\n0. Table of Contents\n\nIntroduction\nCluster Convergence\nMachine Learning\nConvolutional Neural Networks (CNNs)\nDataset Rolling\nPreliminary CNN Experiment\nCNN Layers\nPreliminary CNN Tests on Rolled Data\nTraining and Predicting on Mixed Densities\nCurrent Problems\nMisc Notes\n\n\n\n1. Introduction\nThe main purpose of this week is to begin machine learning tests. The secondary purpose is to flesh out some details a\n\n\n2. Cluster Convergence\nOur cluster analysis code currently outputs various clusters organised by their sizes. In practice, we have analysed them by plotting their distributions within a snapshot using histograms (see, for example, Weeks 5-6 Cluster Analysis).\nThe initial thought was to maintain the bin distributions and chart how each individual bin varies over time. This would essentially involve a ‘rolling average’, where the bin values of iteration \\(n\\) would be compared with the average of the previous \\(k\\) iterations for every bin. This would mean a constant account of standard deviations for moving averages, and setting an acceptable threshold beyond which the system can be considered to stabilise. This is obviously complicated - firstly, the histogram bins will not always be constant; there are bins which will have zero clusters, or which will fluctuate even within the steady state regime by virtue of clusters slightly evaporating (past a bin threshold) and then growing back. This is also quite difficult to visualise.\nInstead, the simpler method is to plot the mean of these cluster sizes (weighted by virtue of their categorisation into clusters) and chart how this mean evolves in time. The principle of monitoring stabilisation is the same, but it can be done much simply and visually through a two-dimensional image.\n\n\n3. Machine Learning\nWith tools of analysis developed over the past 12 weeks (and the last few able to be fleshed out in the following weeks), we can move into the machine learning part of the project. This section has some general details about machine learning as a whole and the CNN we are using.\nThe principle of layered machine learning is that of receiving an input vector of datapoints (a ‘layer’ of ‘neurons’) which is fitted with a weight matrix and callibrated with a bias in order to generate successive learning layers; the whole process is given an ‘activation function’, which alters the results after a desired rule (which will be explained below). These weights are trained on the input data in order to correspond to some desired output data, with the intent of generalising this process to work on extended environments and cases. A useful way to express this in linear algebraic notation is:\n\\[\n\\overrightarrow{a}^{(n+1)}=\\sigma(\\textbf{W}\\overrightarrow{a}^{(n)}+\\overrightarrow{b})\n\\] where we take \\(\\overrightarrow{a}^{(n)}\\) to be the n layer of neurons, \\(\\textbf{W}\\) to be the weight matrix, \\(\\overrightarrow{b}\\) to the associated bias vector, and \\(\\sigma\\) to be the activation function. The weight matrix \\(\\textbf{W}\\) applies different weights to every neuron of the previous layer for each neuron of the current layer.\nThe activation function \\(\\sigma\\) is a function which is meant to bind the neuron values within certain restrictions. A common function, for instance, is the sigmoid function, which binds neurons between 0 and 1, flattening very high values in both negative and positive directions towards the former and the latter, respectively. It is illustrated below:\n\\[\n\\sigma_{sigmoid}=\\frac{1}{1+e^{-a}}\n\\]\nThe reasoning behind the sigmoid function is to treat neurons on a continuous gradient between fully activated (1) and fully deactivated (0), in loose analogy with biological neurological systems. The problem with the sigmoid function is precisely the flattening behaviour it displays at high values - a big part of the machine learning process (precisely the ‘learning’ aspect) is the adjustment of weights with the purposes of minimising the cost of deviation from the expected values (which will also be explained below). High weight values will not produce meaningful values which can then be fine-tuned by the system, so the fluctuations in learning improve much less in practice, especially with a lot of inputs and layers involved.\nWhat is often opted for instead, which is the activation function we are using in this project as well, is the Rectified Linear Unit (ReLU) function \\(\\sigma_{ReLU}\\); this function sets any negative number to 0, and any positive number to its own value. Under this function, varying the weights will always yield some feedback, i.e. learning, on how the algorithm can improve to better match expectations.\nHow does a machine learning algorithm figure out how to alter its weights? The brief answer is through an analysis of deviation, or cost. The machine is fed training data alongside the expected values, and it generates predicted values in turn based on the final layer of neurons. The algorithm obtains a cost function by summing the squares of differences between the expected output value of a neuron and its actual value. Not only does this function convey how well an algorithm performs on its training data (the bigger the value of the cost function is, the worse the algorithm is), but its principle can be used to compute a cost gradient - essentially, the algorithm can compute a cost function vector across all its weights and biases (which constitute its ‘dimensions’, in a calculus sense) and then take the gradient to determine how much each weight/bias parameter ought to change.\n\n\n4. Convolutional Neural Networks (CNNs)\nFor our purposes, running a machine learning algorithm for 128x128 lattice sites will get very resource-intensive very quickly. The input layer alone requires more than 16000 neurons, and subsequent layers will require an extremely high number of weights to compile into new neurons, as, per the equation above, each new neuron \\(a_k^{(n)}\\) will depend on all previous neurons \\(\\overrightarrow{a}^{(n-1)}\\). This results in poor scaling with increases in image size.\nFor this reason, alongside considerations of accuracy, this project employs a convolutional neural network. From hereon I will be denoting the number of a layer with \\(l\\). The main difference in principle is that the main successive layer within the network is a convolutional layer, wherein each neuron at layer \\(l\\) is only connected to a given neighbourhood of neurons in layer \\(l-1\\) through a kernel/filter (the entity analogous to the aforementioned weight matrix \\(\\textbf{W}\\)). This kernel ‘moves’ across the input map in strides, generating one neuron value by adding the sum of its respective weights.\nIn broad strokes, the weight function of a standard machine learning algorithm applies holistically to the entire previous layer map, whereas the kernel of a convolutional neural network applies locally to a region of the previous layer map. Both are subsequently subjected to an activation function before yielding a neuronal value.\nThe consequence of a moving kernel is that the (output) feature map of a convolutional layer will be smaller than the input map it analyses. Using many convolutional layers would therefore mean that the edges of the input map would be factored considerably less in the learning algorithm compared to the centre. The feature maps would get smaller and smaller - crucially, the more layers there are, the more focus is shifted towards the central part of the initial map.\nOne approach is to introduce padding. Essentially, padding constitutes inserting values at the edges of the output feature map in order to increase its overall size. In most cases, the aim is to preserve the size of the input map across convolution, and the edges are padded with zeroes (“zero-padding”). Note that the amount of padding \\(P\\) must align with a relational formula of the different sizes of the system in order to be able to construct a valid convolutional layer:\n\\[\n\\frac{V_{i} - F + 2P}{S}-1 \\in \\mathbb{Z}\n\\] where \\(V_i\\) is the input volume size (for square images), F is the receptive field size, and S is the stride.\nThe receptive field size, in turn, can be calculated generally using the following formula (taken from Araujo et al, 2019):\n\\[\nF_0 = \\sum_{l=1}^{L}((k_l-1)\\prod_{i=1}^{l-1}s_i)+1\n\\] where \\(k_l\\) is the kernel size of layer \\(l\\), \\(s_l\\) is the stride size of layer \\(l\\), and \\(L\\) is the final layer of the system.\nOr recursively using the following formula:\n\\[\nF_{l-1} = S_lF_l + (k_l-s_l)\n\\]\nFinally, the amount of kernels determines the depth of a feature map. Essentially, we can picture each kernel as generating a two-dimensional grid, which can be stacked along the z-axis (the depth-axis, in other words). This is important in order to preserve the depth of the original input map, pertinently stored in our case in colour, through an RGB depth of 3. Therefore, for all CNN algorithms which make use of particle orientation in our PEP system, we will need three filters in our convolutional layers.\n\n\n5. Dataset Rolling\nOne key trait of the in-house code has been a ‘rolling’ of the dataset. Essentially, once the iteration of a dataset is generated by the sampler.py function, it is stored under the header conf_{iteration}. This iteration is then ‘rolled’ (using the np.roll function) 12 times along the x-axis, essentially creating new datasets from the same generative process. So far, we have not been making use of this in-house code property in our data at all - this was intended specifically for the machine learning part of the project, so it was not brought up in our analysis (which was strictly focused on the non-rolled datasets). For reference, the rolled datasets are stored within the h5py file as conf_{iteration}_{roll}.\nBut this brings up an interesting point now that we are moving into the machine learning parts. The CNN will be training on all available datasets in order to draw its inferences. To what extent is introducing rolling data favourable to our predictions? On one hand, training the model on recycled data may give it a weighing bias towards the specific conditions of the datasets we present it with. There is the worry of overfitting, or overspecialisation to strictly the circumstances (\\(P_t\\), \\(\\rho\\) and \\((N_x,N_y)\\) combinations) it receives as input. On the other hand, the rolling provides more samples on which the CNN can train, with a much smaller computational cost. The end of this experiment involves large scale data manipulation and generation, and if rolling proves to be a reliable tool to train a model on, it will aid immensely in the generation aspect.\nOn this note, I have done some quick analysis on the data. As a rule of thumb, datasets with rolling take approximately 10 extra seconds to generate in comparison to the strictly unrolled datasets. This has been tested on my personal computer - it is possible the time difference is much smaller once a supercomputer is employed.\nThe file size contrast is considerable. Datasets which have surplus rolled iterations have approximately 284MB each. Datasets which do not only have about 66MB. Since all the files have been stored on my computer, this is considerable insofar as so far around 215MB per dataset have gone unused (though they will now be employed by the CNN). For example, to obtain the biggest cluster size distribution in Weeks 9-12 required 100 different datasets, resulting in about 19GB of data that was not called.\nData analysis aside, we will run the CNN using both options, and contrast the results for different scenarios. I have rewritten sampler.py to accommodate the requirements of unrolled dataset files (as sampler_no_roll.py). This code stores its datasets in a separate folder (data/no-roll).\n\n\n6. Preliminary CNN Experiment\nWe can start with a simple case to illustrate the principle. Taking a single density value of \\(\\rho=0.15\\), we can train the model on a few probability distributions. Running for 5 epochs and using only one convolutional layer, with 3 5x5 kernels with 3x3 strides, we get the following predictions:\n\nwhere the orange data points are the input turning rates, and the blue data points are the model predicitons.\nThe spreads are quite big on most probability values, and for the highest one the model is clearly not fitting properly at all. Nontheless, this is a good start for how simple the layer diagram is:\nINPUT =&gt; CONV =&gt; FLATTEN =&gt; FC (optimiser: adam ; 5 epochs)\nAfter some experimenting, below are the predictions made on the unrolled data, running for 10 epochs:\n\nWith the following layer diagram:\nINPUT =&gt; CONV =&gt; BN =&gt; CONV =&gt; BN =&gt; MAXPOOL =&gt; CONV =&gt; BN =&gt; CONV =&gt; BN =&gt; MAXPOOL =&gt; FC =&gt; DO =&gt; FC =&gt; FLATTEN =&gt; FC (optimiser: adam ; 10 epochs)\nThe training size is also important; this model is trained on one dataset for each probability distribution (so 5 datasets total). Each of these datasets has 1000 snapshots, leaving a total of 5000 data. We trained the model on 3000 snapshots, and then applied it to the last 2000.\nThe natural question is how the predictions change if we train on longer evolutions. Below is the graphical result of this, with the same (\\(\\rho\\),\\(P_t\\),(\\(N_x\\),\\(N_y\\))) configuration, but with 10 times the generated evolution steps (and therefore snapshots). This means the mode is now trained on 30000 snapshots, and then applied to the last 20000.\n\nWe can see the fitting drastically improve for the higher tumbling rates, though it’s clearly still lacking at the extremes of our turning rates.\nTo understand the general principle and methodology of our preliminary analysis, see the uploaded Preliminary Analysis Notebook example.\n\n\n7. CNN Layers\nQuick definitional list for the most commonly used CNN layers.\n\nConvolutional Layer (CONV): Standard convolution layer, mapping the input map through kernels in strides, and generating a feature map of lower dimensionality as a result.\nActivation Function Layer (RELU): Applies the activation function to the emergent neurons. Often left implicit after a CONV or FC layer.\nBatch Normalisation Layer (BN): Normalises layer inputs through re-centring and re-scaling.\nPooling Layer (POOL): Separates input into patches; replaces each patch in input with single value in output, which is often either…\n\nMAXPOOL: the maximum value within the pool\nAVGPOOL: the average value within the pool\n\nDense/Fully Connected Layer (FC): Standard neural network layer; maps each neuron of the feature map to every neuron of the input map.\nDropout Layer (DO): Nullifies contribution of some neurons towards next layer while keeping the rest intact.\nFlattening Layer (FLATTEN): Flattens feature map into a one-dimensional column.\n\n\n\n8. Preliminary CNN Tests on Rolled Data\nSo far we have trained the model strictly on non-rolled datasets. Below is an example of the same 1000 iteration per dataset scenario, but used on rolled datasets to facilitate more samples for the model to train on. As a result, there are 70000 samples in the dataset, 50000 of which are training data, and 20000 of which are validation data.\n\n\n\nPearson’s correlation coeff: 0.98276\n\n\nWe can see that the fitting is even better in the low iteration rolling case than in the high iteration no-rolling case. However, the \\(P_t=0.34\\) values are consistently predicted incorrectly across these two examples, which might suggest an underlying problem with the system.\n\n\n9. Training and Predicting on Mixed Densities\nThe next evident question is how the model holds up when we apply the above principles to a broad range of densities. In the figure below, the CNN runs the same framework on 100 different densities. Unfortunately, I have not been able to construct predictions for such a model yet, as the CUDA memory storage overwhelms my current machine and crashes. As a result, this will be done once the supercomputer system is fully set up.\n\n\n10. Current Problems\nWe are currently training on every snapshot of a lattice’s evolution. This means that our model trains on both steady state and non-steady state segments of a system. In the following weeks it will be worth separating these segments for the purposes of analysing how training the CNN system only on steady state postively or negatively affects its ability to derive tumbling rates.\n\n\n11. Misc Notes\nJ. Chodera - Equilibration in Monte Carlo and molecular dynamics\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week2.html",
    "href": "activity_log/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Week 2\n\n\n0. Table of Contents\n\n0. Table of Contents\n\n\n1. Introduction\n\n\n2. Deadline Schedule\n\n\n3. Text Information\n\n\n4. Motivation Report\n\n\n\n1. Introduction\nThe goal of week 2 is to further familiarise ourselves with the literature. We are to write a quick 500-word referenced report detailing motivations for studying active matter. Notes for this week will therefore be slightly sparser, primarily sifting through the goals of various papers (their detailed processes are to be examined at a later date). Some of these tidbits of information will be taken from the texts explored in week 1; as such, overlap with week 1 information is not only possible, but very likely. Repetition was preferred over disorganised and uncatalogued information, as these notes serve both as an activity log (which must document repeats occasionally) and as a catalogue of information as it is found.\nAnother essential goal of weeks 1 and 2 was to figure out the general examination schedule of the project. This in turn would influence planning the project out more properly, in anticipation of the deadline for the interrim report (other influences, naturally, are personal academic schedules; I personally anticipate being much busier during the first term, and therefore expect higher activity during the latter half of the project). This schedule was not given out to the 30cp version of the project page until very recently, and has therefore been appended in the activity log for this week.\nAbove is a table of contents. Below is a list of a few of the utilised texts, with some key information extracted. The 500-word referenced essay is given afterwards. At the very bottom is a very brief introduction to the Langevin equation. On an informational level, this is meant as a way to ease in into Brownian motion theory; on a technical level, this also gives a brief occasion to experiment with LaTex in markdown.\n\n\n2. Deadline Schedule\n\nWeek 1: Start of project\nWeek 9: Optional interim report submitted. Formative only. Deadline is Thursday 23rd November @ 12.30pm.\nWeek 19: Practical work (experimental or computational/theoretical) must finish by the end of the week. Friday 8th March.\nWeeks 20/21: Analysis of results obtained. Start write up of report. Results to be presented to supervisor and analysis discussed.\nWeek 22: Final Report submission. Deadline is Thursday 18th April @ 12.30pm.\nWeeks 23/24: Final interviews with Assessors and supervisors. Assessment: The final assessment is worth 100% of the total marks available.\n\n(This schedule will be added on the website as a separate tab as well.)\n\n\n3. Text information\n\nThe Mechanics and Statistics of Active Matter\nThis is a 2010 review of (at the time) recent progress within the field. Its main role for my current purposes is to obtain references to older papers (and therefore to their motivations), while also providing a starting definition for active matter.\n\nactive matter can be considered as a type of material\ntake active matter as condensed matter in a nonequilibrium regime\n\neach (autonomous/active) constituent takes direct energetic input\n\nenergy input is therefore homogenously distributed in system\ncompare to fluid motion, for instance: energy is not supplied to each individual particle, but rather is applied (e.g. kinetically) at the boundaries: this then causes particles to push others forward, but they don’t all have direct access to energy\nslightly related, Ramaswamy argues in one of his lectures that this is the key distinction of active matter, phrased as direct access to energy (in the fluid example above, the bulk particles have indirect access to energy)\n\nforce-free: forces exerted between particle and fluid cancel\n(self-propelled) motion is set by particle, not external fields\n\n\n\n\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton, Kistler et al.\nThis is a 2004 paper experimentally confirming a solution to the convection-diffusion relation applied to rod-shaped particles (with Pt and Au segments) moving autonomously in hydrogen peroxide solutions.\n\none of the big nanotechnology challenges is the conversion of stored chemical technology to motion\n\nthis is precisely what many biological active matter systems do\nstudying this would yield useful artifficial active matter systems\n\n\n\n\nActive matter: quantifying the departure from equilibrium, Flenner & Szamel\nThis is a 2020 paper examining active matter systems as they are moved further away from thermodynamic equilibrium.\n\nthe main motivational point here is that quantifying departure from equilibrium helps understand the difference between active matter and equilibium systems, and thus can help chart generalised models of how they both work\n\n\n\n\n4. Motivation Report\nActive matter is, broadly, a subcategory of matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel)[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4]). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n\n499 words. \n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n5. Langevin Equation\nThe Langevin equation represents the partly-random particle movement of a particle within a fluid:\n\\[m \\frac{dv}{dt}=-\\lambda \\mathbf{v} +\\boldsymbol{\\eta} (t)\\] where \\(\\textbf{v}\\) is particle velocity, \\(\\lambda\\) is the damping coefficient, m is the particle mass, \\(\\boldsymbol{\\eta}\\) is the noise term.\nThe noise term indicates collisions of the given particle with other molecules within the fluid. It is determined by a Gaussian distribution, and for the Boltzmann constant \\(k_{B}\\), temperature \\(T\\) and \\(\\eta_{i}\\) being the i-th component of vector \\(\\boldsymbol{\\eta}(t)\\), it is described by the following correlation function:\n\\[\\langle \\eta_{i} (t) \\eta_{j} (t')\\rangle = 2 \\lambda k_{B} T \\delta_{i,j} \\delta(t-t')\\]\nThis approximates that any given force (at time \\(t\\)) is uncorrelated with a force at any other time. The collision time with other molecules indicates this is not the case. However, within great collective motion this is broadly the case. The appearance of the damping coefficient \\(\\lambda\\) in the correlation function is, within an equilibrium system, an expression of the Einstein relation.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week18.html",
    "href": "activity_log/week18.html",
    "title": "Week 18",
    "section": "",
    "text": "Week 18\n\n\n0. Table of Contents\n\nIntroduction\nRefining CNN Architecture\nDegrees of Freedom Discussion\nDensity Discussion in Big Tumbling Rate Spread\nGaps in Lower Tumbling Rates for Higher Densities\nGaps in Higher Tumbling Rates for Higher Densities\nOmitting Highest Tumbling Rate\nDifferent Density Comparison (Omitting Highest Tumbling Rate)\nMultiple Nearby Densities\nEpoch Numbers\nMonochrome Interpolation (Low Tumbling Rates)\nMonochrome Interpolation (High Tumbling Rates)\nMonochrome Extrapolation\n\n\n\n1. Introduction\nThe main purpose of this week is to further explore gaps in tumbling rates for our CNN predictions, alongside discussions of densities and clusters. The auxiliary purpose of this week is to flesh out the discussion around degrees of freedom.\n\n\n2. Refining CNN Architecture\nThe intermediary convolutional layer in MN_3 was running a (4,4) kernel size; this is uncentered, and therefore slightly hinders the model. We have swapped it out for a (5,5) kernel size.\nWe could do ‘contour plots’ of individual clusters to map out how their orientation\n\n\n3. Degrees of Freedom Discussion\nWe have identified three different changes we could make to the system images before experimenting with the CNN that could pose interesting results.\n\nOrientation case\nMonochrome case\nConfusion case: This case consists of a random scrambling orientations. The orientation case keeps its orientation categories, but we alter the image such that these categories do not mean anything. Our expectations are that:\n\nTraining on Confusion case will yield the same result whether validated on Confusion or Orientation case.\nTraining on Orientation case will yield better results when validated on Orientation case, rather than when validated on Confusion case. We expect this because the Orientation case training should prime the model to detect an intrinsic feature of the system, which is then completely scrambled by the Confusion case. If our hypothesis is incorrect, and validating on Orientation in fact yields similar results, it would mean that the Orientation case does not pick up on this degree of freedom in its analysis.\nThe Orientation case should overall produce better results than the Confusion case, unless our hypothesis in the last bullet point is false.\n\nMisinformation case: This case consists of misattributing a random percentage of particle orientations in an Orientation case image. This has physical parallels to misidentifying the orientation of active matter particles from a two-dimensional perception (as they are three-dimensional swimmers).\nNoise case: This case consists of giving a random (float) noise distribution in a Monochrome case image. The reasoning behind this is granting the CNN the ability to distinguish\n\n\n\n4. Density Discussion in Big Tumbling Rate Spread\nA natural question that arises out of the density analysis done in Week 17 is why the prediction distributions get skewed by the upper probability values. That is to say, why does adding bigger tumbling rates significantly decrease the prediction accuracy and precision?\nOur current theory is that higher tumbling rates do not exhibit the clustering behaviour which the CNN is tracking in order to assert its predictions. Once the tumbling rate reaches a certain amount for our \\(\\rho=0.15\\) case, it is more difficult for the CNN to draw comparisons, due to the feature landscape dramatically changing. This essentially causes the CNN to misrecognise these different (clustering and non-clustering) ranges of data, across both cases having too small a training sample size to effectively predict the probabilities.\nThe natural fix for such a problem is more data, but there is some other analysis that can be done to further explore the situation. As stated above, we have so far been working on a density of \\(\\rho=0.15\\). Provided our theory is correct, we might notice changes to predictions by increasing the density, thus allowing for clustering at higher tumbling rates.\nBelow is an example of how the landscape looks for \\(\\rho=0.25\\). This is a randomly selected image from the pool of utilised probabilities, so its tumbling rate is unknown; nonetheless this image gives a visual idea of the amount of particles on the screen.\n\nAnd below is a side by side comparison of the last screenshots of an evolution using \\(P_{tumble}=0.34\\), for \\(\\rho=0.35\\) (left), \\(\\rho=0.25\\) (center) and \\(\\rho=0.15\\) (right). We can see that in the center case the density is hitgh enough for clusters to begin forming (though only barely), whereas the left case already has more noticeable clusters.\n\n\n\n\n\n\n\n\n\n\\(\\rho=0.35\\), \\(P_{tumble}=0.34\\)\n\\(\\rho=0.25\\), \\(P_{tumble}=0.34\\)\n\\(\\rho=0.15\\), \\(P_{tumble}=0.34\\)\n\n\n\n\n\n\n\n\n\n\n\nreverb3164: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340,0.500 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\n\nMAE: 0.0177948232740164\nMin STD: 0.001830725\nAvg STD: 0.011430472\nMax STD: 0.030022161\nOverlap Ratio: 0.7 (acc 1e-3)\nPearson Coefficient: 0.990001865469512\n\nWe can also visually compare reverb3164 (the \\(\\rho=0.25\\) case) with stage4124 (the analogous \\(\\rho=0.15\\) case):\n\n\n\n\n\n\n\nreverb3164: \\(\\rho=0.25\\)\nstage4124: \\(\\rho=0.15\\)\n\n\n\n\n\n\n\n\n\nGiven that the scales are the same, we can qualitatively notice a decrease in spread (i.e. an increase in accuracy) while jumping from a smaller density to a larger density. This is reflected in our quantitative results:\n\n\n\nParameter\nreverb3164\nstage4124\n\n\n\n\nMAE\n0.017795\n0.000895\n\n\nAvg STD\n0.011430472\n0.019767912\n\n\nMax STD\n0.030022161\n0.04721327\n\n\nOverlap Ratio\n0.7\n1.0\n\n\n\nWe can see that the standard deviations are lower for a bigger density, both on average and regarding maxima. We have, however, included two parameters which are in fact worse in the higher density case: the overlap ratio and the mean absolute error. Regarding the overlap ratio, this is directly tied to the lowering of spread: if we look to the lower density case, we can see that the lower values which are ‘hit’ there (and which are bareky missed in the higher density case) do not hit so with the centre of their distribution, but rather only with the spread perifery. There is also the issue of the accuracy we’ve been employing so far for our overlap ratio: 1e-3 is simply too small to account for distributions which do not have a big spread, but are nonetheless within the vicinity of the guessed distribution. We later decided to increase the accuracy to 5e-3; there is an argument to be made that it should be increased even further; in reality the contents of these probability distributions matter much more: are they Gaussian? We will see once we introduce violin plots and absolute deviation considerations that this is indeed the case. Furthermore, how does the mean of the distribution relate to the expected value? Within how many standard deviations are they from each other? This will also start to be factored in in the analysis below.\n\n\n\n5. Gaps in Lower Tumbling Rates for Higher Densities\nWe begin by mirroring the cases we explored with \\(\\rho = 0.15\\). The thought is that we can show the prior results (again, see Week 17) are somewhat general by doing so, while also exploring how allowing more clusters to be picked up on across the tumbling rate distributions slightly improves our data.\n\nsalad8110: \\(\\rho = 0.25\\), \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\n\nMAE: 0.004445969592779875\nMin STD: 1.8626451e-09\nAvg STD: 0.0035765618\nMax STD: 0.0058416952\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.9865499725921921\n\nWe can now see the prediction distributions that the CNN makes, and get confirmation that they are broadly Gaussian in form. This suggests that a mean analysis of our predictions would accurately reflect the predicttion dynamics at play. Note that the violin plots show specifically the absolute difference between the expected and predicted values, and so values are better the closer they are to zero. We can also see what was previously intuited from the original distribution graphs: the spread does get larger with increased tumbling rates.\nWe can also see exactly how precise the predictions of the CNN are regarding the lowest value. Rather than a Gaussian, the \\(P_{tumble}=0.016\\) case appears at our scope to be a constant distribution (it is, in actuality, still a Gaussian distribution with an extremely narrow standard deviatiom; the CNN prediction naturally never hits the exact same real number twice).\nBeyond that, we can see that the above parameters are still very good. We can once again visually and quantitatively compare with the \\(\\rho = 0.15\\) case:\n\n\n\n\n\n\n\nsalad8110: \\(\\rho=0.25\\)\nbalteus3123: \\(\\rho=0.15\\)\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nsalad8110\nbalteus3123\n\n\n\n\nMAE\n0.004446\n0.006792\n\n\nAvg STD\n0.0035765618\n0.00386919\n\n\nMax STD\n0.0058416952\n0.006857624\n\n\nOverlap Ratio\n1.0\n1.0\n\n\n\nAnd see that once again, the higher density case yields better results over all. Note, of course, that we are discussing differences of magnitude \\(10^{-3}\\) (for MAE and Max STD) and \\(10^{-4}\\) (for Avg STD).\n\n\ncrab8432: \\(\\rho = 0.25, P_{tumble} \\in \\\\{0.016,0.034,0.050,0.107 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\n\nMAE: 0.003553365357220173\nMin STD: 0.00015766203\nAvg STD: 0.0037065577\nMax STD: 0.005531624\nOverlap Ratio: 1.0 (acc 5e-3)\nPearson Coefficient: 0.9910517414919411\n\n\n\n\n\n\n\n\ncrab8432: \\(\\rho=0.25\\)\ngoose4421: \\(\\rho=0.15\\)\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\ncrab8432\ngoose4421\n\n\n\n\nMAE\n0.003553\n0.004174\n\n\nAvg STD\n0.0037065577\n0.0038618112\n\n\nMax STD\n0.005531624\n0.006754256\n\n\nOverlap Ratio\n1.0\n1.0\n\n\n\n(Note: the overlap ratio for goose4421 was adapted to the new criterion of accuracy 5e-3)\n\n\nsummer6911 \\(\\rho = 0.25\\), \\(P_{tumble} \\in \\\\{0.016, 0.073, 0.107, 0.157 \\\\}\\)\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\n\nMAE: 0.008954823948442936\nMin STD: 0.004150052\nAvg STD: 0.0066630687\nMax STD: 0.013605628\nOverlap Ratio: 1.0 (acc 5e-3)\nPearson Coefficient: 0.9869872375647769\n\n\n\n\n\n\n\n\nsummer6911: \\(\\rho=0.25\\)\nbook1634: \\(\\rho=0.15\\)\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\ncrab8432\nbook1634\n\n\n\n\nMAE\n0.008955\n0.008299\n\n\nAvg STD\n0.0066630687\n0.007414392\n\n\nMax STD\n0.013605628\n0.013894851\n\n\nOverlap Ratio\n1.0\n0.75\n\n\n\n(Note: the overlap ratio just barely misses book1634 in the \\(P_{tumble}=0.016\\) case even with the 5e-3 extension)\n\n\n\n6. Gaps in Higher Tumbling Rates for Higher Densities\n\nsalmon9100: \\(\\rho = 0.25\\), \\(P_{tumble} \\in \\\\{0.073,0.107,0.157,0.231,0.34,0.5 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.073: Average = 0.07637 +- 0.00064; Expected value within 5.309 stdevs of mean\nActual value 0.107: Average = 0.11958 +- 0.01252; Expected value within 1.005 stdevs of mean\nActual value 0.157: Average = 0.16357 +- 0.00663; Expected value within 0.990 stdevs of mean\nActual value 0.231: Average = 0.24277 +- 0.02668; Expected value within 0.441 stdevs of mean\nActual value 0.34: Average = 0.34620 +- 0.03178; Expected value within 0.195 stdevs of mean\nActual value 0.5: Average = 0.44783 +- 0.03338; Expected value within 1.563 stdevs of mean\n\n\nMAE: 0.021534917876124382\nMin STD: 0.0006356345\nAvg STD: 0.01860332\nMax STD: 0.033375088\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.9793251812649026\n\n\n\ntread4399: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.073,0.157,0.5 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.073: Average = 0.08449 +- 0.00540; Expected value within 2.128 stdevs of mean\nActual value 0.157: Average = 0.15972 +- 0.00279; Expected value within 0.973 stdevs of mean\nActual value 0.5: Average = 0.49756 +- 0.00513; Expected value within 0.475 stdevs of mean\n\nMAE: 0.00639208871871233\nMin STD: 0.0027909318\nAvg STD: 0.004439787\nMax STD: 0.0054007815\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.999563884360183\n\n\n\nrevolve8117: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.073,0.231,0.340,0.500 \\\\}\\), 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.073: Average = 0.09487 +- 0.00837; Expected value within 2.613 stdevs of mean\nActual value 0.231: Average = 0.24794 +- 0.01834; Expected value within 0.924 stdevs of mean\nActual value 0.34: Average = 0.37743 +- 0.05689; Expected value within 0.658 stdevs of mean\nActual value 0.5: Average = 0.49567 +- 0.02125; Expected value within 0.204 stdevs of mean\n\nMAE: 0.0256530288606882\nMin STD: 0.008370637\nAvg STD: 0.026211156\nMax STD: 0.05688635\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.974904538431155\n\n\n\n\n7. Omitting Highest Tumbling Rate\n\nflag1899: \\(\\rho=0.25\\), $$P_{tumble} \\{0.034,0.050,0.073,0.157,0.231 \\}, 30 epochs\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.034: Average = 0.03666 +- 0.00000; Expected value within inf stdevs of mean\nActual value 0.05: Average = 0.04100 +- 0.00806; Expected value within 1.116 stdevs of mean\nActual value 0.073: Average = 0.07631 +- 0.00551; Expected value within 0.600 stdevs of mean\nActual value 0.107: Average = 0.11042 +- 0.00746; Expected value within 0.458 stdevs of mean\nActual value 0.157: Average = 0.16183 +- 0.01717; Expected value within 0.282 stdevs of mean\nActual value 0.231: Average = 0.22189 +- 0.01270; Expected value within 0.718 stdevs of mean\n\nMAE: 0.00829365104436874\nMin STD: 0.0 !!\nAvg STD: 0.008482912\nMax STD: 0.017168295\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.984593225567436\n\n\n\ncandy8131: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340 \\\\}\\), 30 epochs, 32000 (0.2) snapshots\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.023: Average = 0.03053 +- 0.00393; Expected value within 1.917 stdevs of mean\nActual value 0.034: Average = 0.04008 +- 0.00437; Expected value within 1.393 stdevs of mean\nActual value 0.05: Average = 0.05421 +- 0.00473; Expected value within 0.891 stdevs of mean\nActual value 0.073: Average = 0.07196 +- 0.00521; Expected value within 0.199 stdevs of mean\nActual value 0.107: Average = 0.10870 +- 0.00724; Expected value within 0.235 stdevs of mean\nActual value 0.157: Average = 0.15856 +- 0.01793; Expected value within 0.087 stdevs of mean\nActual value 0.231: Average = 0.23771 +- 0.02513; Expected value within 0.267 stdevs of mean\nActual value 0.34: Average = 0.30826 +- 0.02299; Expected value within 1.381 stdevs of mean\n\nMAE: 0.0119977109134197\nMin STD: 0.0039281165\nAvg STD: 0.011441505\nMax STD: 0.025132488\nOverlap Ratio: 1.0 (acc 5e-3)\nPearson Coefficient: 0.985809870908463\n\n\n\nbriar9222: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340 \\\\}\\), 30 epochs, 36000 (0.2) snapshots\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.016: Average = 0.01687 +- 0.00000; Expected value within 469738.408 stdevs of mean\nActual value 0.023: Average = 0.01806 +- 0.00219; Expected value within 2.259 stdevs of mean\nActual value 0.034: Average = 0.03374 +- 0.00421; Expected value within 0.061 stdevs of mean\nActual value 0.05: Average = 0.05006 +- 0.00412; Expected value within 0.015 stdevs of mean\nActual value 0.073: Average = 0.06890 +- 0.00514; Expected value within 0.798 stdevs of mean\nActual value 0.107: Average = 0.10614 +- 0.00696; Expected value within 0.124 stdevs of mean\nActual value 0.157: Average = 0.15741 +- 0.01823; Expected value within 0.022 stdevs of mean\nActual value 0.231: Average = 0.22961 +- 0.02319; Expected value within 0.060 stdevs of mean\nActual value 0.34: Average = 0.30098 +- 0.02491; Expected value within 1.566 stdevs of mean\n\n\nMAE: 0.0109049286693335\nMin STD: 0.0000000018626451\nAvg STD: 0.009882737\nMax STD: 0.024909819\nOverlap Ratio: 1.0 (acc 5e-3)\nPearson Coefficient: 0.986715154338279\n\n\n\n\n8. Different Density Comparison (Omitting Highest Tumbling Rate)\nOmitting the regime which exhibits non-clustering behaviour for both \\(\\rho=0.15\\) and \\(\\rho=0.25\\), we can even better highlight the difference between the two densities in generating predictions. We can better yet contrast both of them to the \\(\\rho=0.35\\) case, as was briefly done near the beginning of this Week.\n\nstamp5111: \\(\\rho=0.15\\), \\(P_{tumble}\\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340 \\\\}\\), 30 epochs, 36000 (0.2) snapshots\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.016: Average = 0.02170 +- 0.00062; Expected value within 9.130 stdevs of mean\nActual value 0.023: Average = 0.02707 +- 0.00378; Expected value within 1.077 stdevs of mean\nActual value 0.034: Average = 0.04148 +- 0.00404; Expected value within 1.850 stdevs of mean\nActual value 0.05: Average = 0.05424 +- 0.00314; Expected value within 1.349 stdevs of mean\nActual value 0.073: Average = 0.08061 +- 0.00730; Expected value within 1.041 stdevs of mean\nActual value 0.107: Average = 0.11660 +- 0.01217; Expected value within 0.789 stdevs of mean\nActual value 0.157: Average = 0.17568 +- 0.02403; Expected value within 0.777 stdevs of mean\nActual value 0.231: Average = 0.24407 +- 0.02617; Expected value within 0.499 stdevs of mean\nActual value 0.34: Average = 0.30026 +- 0.02976; Expected value within 1.336 stdevs of mean\n\nMAE: 0.0144910635426641\nMin STD: 0.000623846\nAvg STD: 0.012336487\nMax STD: 0.029758396\nOverlap Ratio: 0.89 (acc 5e-3)\nPearson Coefficient: 0.976878137376062\n\n\n\nripple9010: \\(\\rho=0.35\\), \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340 \\\\}\\), 30 epochs, 36000 (0.2) snapshots\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.016: Average = 0.01950 +- 0.00000; Expected value within 1879028.408 stdevs of mean\nActual value 0.023: Average = 0.02014 +- 0.00208; Expected value within 1.375 stdevs of mean\nActual value 0.034: Average = 0.03159 +- 0.00694; Expected value within 0.347 stdevs of mean\nActual value 0.05: Average = 0.05213 +- 0.00506; Expected value within 0.420 stdevs of mean\nActual value 0.073: Average = 0.07047 +- 0.00630; Expected value within 0.401 stdevs of mean\nActual value 0.107: Average = 0.10682 +- 0.00768; Expected value within 0.024 stdevs of mean\nActual value 0.157: Average = 0.15306 +- 0.01171; Expected value within 0.336 stdevs of mean\nActual value 0.231: Average = 0.22579 +- 0.01955; Expected value within 0.266 stdevs of mean\nActual value 0.34: Average = 0.30243 +- 0.01907; Expected value within 1.971 stdevs of mean\n\nMAE: 0.010349047370255\nMin STD: 0.0000000018626451\nAvg STD: 0.008708556\nMax STD: 0.019550083\nOverlap Ratio: 1.0 (acc 5e-3)\nPearson Coefficient: 0.991194498813799\n\n\n\nComparison\n\n\n\n\n\n\n\n\n\nParameter\nripple9010 \\((\\rho=0.35)\\)\nbriar9222 \\((\\rho=0.25)\\)\nstamp5111 \\((\\rho=0.15)\\)\n\n\n\n\nMAE\n0.010349\n0.010905\n0.014491\n\n\nAvg STD\n0.008708556\n0.009882737\n0.012336487\n\n\nMax STD\n0.019550083\n0.024909819\n0.029758396\n\n\nOverlap Ratio\n1.0\n1.0\n0.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Values\n0.016\n0.023\n0.034\n0.050\n0.073\n0.107\n0.157\n0.231\n0.34\n\n\n\n\n\\(\\rho=0.35\\)\n\\(0.01950 \\pm 0.00000\\)\n\\(0.02014 \\pm 0.00208\\)\n\\(0.03159 \\pm 0.00694\\)\n\\(0.05213 \\pm 0.00506\\)\n\\(0.07047 \\pm 0.00630\\)\n\\(0.10682 \\pm 0.00768\\)\n\\(0.15306 \\pm 0.01171\\)\n\\(0.22579 \\pm 0.01955\\)\n\\(0.30243 \\pm 0.01907\\)\n\n\n\\(\\rho=0.25\\)\n\\(0.01687 \\pm 0.00000\\)\n\\(0.01806 \\pm 0.00219\\)\n\\(0.03374 \\pm 0.00421\\)\n\\(0.05006 \\pm 0.00412\\)\n\\(0.06890 \\pm 0.00514\\)\n\\(0.10614 \\pm 0.00696\\)\n\\(0.15741 \\pm 0.01823\\)\n\\(0.22961 \\pm 0.02319\\)\n\\(0.30098 \\pm 0.02491\\)\n\n\n\\(\\rho=0.15\\)\n\\(0.02170\\pm 0.00062\\)\n\\(0.02707 \\pm 0.00378\\)\n\\(0.04148 \\pm 0.00404\\)\n\\(0.05424 \\pm 0.00314\\)\n\\(0.08061 \\pm 0.00730\\)\n\\(0.11660 \\pm 0.01217\\)\n\\(0.17568 \\pm 0.02403\\)\n\\(0.24407 \\pm 0.02617\\)\n\\(0.30026 \\pm 0.02976\\)\n\n\n\n\n\n\n9. Multiple Nearby Densities\n\nketer3955:\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations.\nActual value 0.016: Average = 0.02119 +- 0.00000; Expected value within 1392564.204 stdevs of mean\nActual value 0.023: Average = 0.02253 +- 0.00321; Expected value within 0.147 stdevs of mean\nActual value 0.034: Average = 0.03882 +- 0.00464; Expected value within 1.038 stdevs of mean\nActual value 0.05: Average = 0.05392 +- 0.00443; Expected value within 0.886 stdevs of mean\nActual value 0.073: Average = 0.07471 +- 0.00426; Expected value within 0.403 stdevs of mean\nActual value 0.107: Average = 0.11074 +- 0.00665; Expected value within 0.563 stdevs of mean\nActual value 0.157: Average = 0.15046 +- 0.01607; Expected value within 0.407 stdevs of mean\nActual value 0.231: Average = 0.21991 +- 0.02594; Expected value within 0.427 stdevs of mean\nActual value 0.34: Average = 0.29158 +- 0.02782; Expected value within 1.740 stdevs of mean\n\nMAE: 0.0126360701397061\nMin STD: 0.0000000037252903\nAvg STD: 0.010336722\nMax STD: 0.027823886\nOverlap Ratio: 0.89 (acc 5e-3)\nPearson Coefficient: 0.984687332725716\n\n\n\n\n10. Epoch Numbers\nWe have been mostly running 30 epochs for each CNN model. We can see a downward shift in all the MAE evolutions above, with a potential indication that more epochs might decrease it further and thus yield even better results. Below is a model ran for 40 epochs, as it compares to reverb3164, outlined above at the beginning.\n\nremnant3992: \\(\\rho=0.25\\), \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340,0.5000 \\\\}\\), 40 epochs, 40000 (0.2) snapshots\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\n\nMAE: 0.0164859797805548\nMin STD: 0.00021988283\nAvg STD: 0.013953483\nMax STD: 0.0383877\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.987502201574785\n\n\n\nComparison\n\n\n\nParameter\nreverb3164 (30 epochs)\nremnant3992 (40 epochs)\n\n\n\n\nMAE\n0.017795\n0.016486\n\n\nAvg STD\n0.011430472\n0.013953483\n\n\nMax STD\n0.030022161\n0.0383877\n\n\nOverlap Ratio\n0.7\n1.0\n\n\n\n\n\n\n11. Monochrome Interpolation (Low Tumbling Rates)\n\nderive1278 (crab8432) \\(\\rho=0.25\\) \\(P_{val} \\in \\\\{ 0.023, 0.073\\\\}\\), \\(P_{train} \\in \\\\{0.016,0.034,0.050,0.107 \\\\}\\)\n\n\n\n\n\n\n\nderive1278\ncrab8432\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for derive1278.\nActual value 0.023: Average = 0.01815 +- 0.00382; Expected value within 1.270 stdevs of mean\nActual value 0.073: Average = 0.07531 +- 0.00863; Expected value within 0.268 stdevs of mean\n\nMAE: -\nMin STD: 0.0038166833\nAvg STD: 0.0062226485\nMax STD: 0.008628614\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.973814798645944\n\n\n\n\nParameter\nderive1278\ncrab8432\n\n\n\n\nAvg STD\n0.0062226485\n0.0037065577\n\n\nMax STD\n0.008628614\n0.005531624\n\n\nOverlap Ratio\n1.0\n1.0\n\n\nPearson Coefficient\n0.973815\n0.991052\n\n\n\n\n\nstory4919 (summer6911): \\(\\rho=0.25\\), \\(P_{val} \\in \\\\{ 0.023,0.034,0.050 \\\\}\\), \\(P_{train} \\in \\\\{0.016,0.073,0.107,0.157 \\\\}\\)\n\n\n\n\n\n\n\nstory4919\nsummer6911\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for story4919.\nActual value 0.023: Average = 0.03101 +- 0.00552; Expected value within 1.452 stdevs of mean\nActual value 0.034: Average = 0.04259 +- 0.00784; Expected value within 1.096 stdevs of mean\nActual value 0.05: Average = 0.06339 +- 0.00805; Expected value within 1.663 stdevs of mean\n\nMAE: -\nMin STD: 0.005518279\nAvg STD: 0.0071368576\nMax STD: 0.00805434\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.878673132485899\n\n\n\n\nParameter\nstory4919\nsummer6911\n\n\n\n\nAvg STD\n0.0071368576\n0.0066630687\n\n\nMax STD\n0.00805434\n0.013605628\n\n\nOverlap Ratio\n1.0\n1.0\n\n\nPearson Coefficient\n0.878673\n0.986987\n\n\n\n\n\n\n12. Monochrome Interpolation (High Tumbling Rates)\n\nrabbit0196 (tread4399): \\(\\rho=0.25\\), \\(P_{val} \\in \\\\{0.107,0.231,0.340 \\\\}\\), \\(P_{train} \\in \\\\{0.073,0.157,0.5 \\\\}\\)\n\n\n\n\n\n\n\nrabbit0196\ntread4399\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for rabbit0196.\nActual value 0.107: Average = 0.13162 +- 0.01054; Expected value within 2.336 stdevs of mean\nActual value 0.231: Average = 0.29793 +- 0.06501; Expected value within 1.030 stdevs of mean\nActual value 0.34: Average = 0.47849 +- 0.01906; Expected value within 7.268 stdevs of mean\n\nPrediction means and standard deviations for tread4399.\nActual value 0.073: Average = 0.08449 +- 0.00540; Expected value within 2.128 stdevs of mean\nActual value 0.157: Average = 0.15972 +- 0.00279; Expected value within 0.973 stdevs of mean\nActual value 0.5: Average = 0.49756 +- 0.00513; Expected value within 0.475 stdevs of mean\n\nMAE: -\nMin STD: 0.010536376\nAvg STD: 0.031534202\n\nMax STD: 0.06501088\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.961320568504786\n\n\n\n\nParameter\nrabbit0196\ntread4399\n\n\n\n\nAvg STD\n0.031534202\n0.004439787\n\n\nMax STD\n0.06501088\n0.0054007815\n\n\nOverlap Ratio\n1.0\n1.0\n\n\nPearson Coefficient\n0.961321\n0.999564\n\n\n\n\n\ndoor1333 (revolve8117): \\(\\rho=0.25\\), \\(P_{val} \\in \\\\{0.107,0.157 \\\\}\\), \\(P_{train} \\in \\\\{0.073,0.231,0.340,0.500\\\\}\\)\n\n\n\n\n\n\n\ndoor1333\nrevolve8117\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for door1333.\nActual value 0.107: Average = 0.14482 +- 0.01286; Expected value within 2.942 stdevs of mean\nActual value 0.157: Average = 0.20067 +- 0.01161; Expected value within 3.761 stdevs of mean\n\nPrediction means and standard deviations for revolve8117.\nActual value 0.073: Average = 0.09487 +- 0.00837; Expected value within 2.613 stdevs of mean\nActual value 0.231: Average = 0.24794 +- 0.01834; Expected value within 0.924 stdevs of mean\nActual value 0.34: Average = 0.37743 +- 0.05689; Expected value within 0.658 stdevs of mean\nActual value 0.5: Average = 0.49567 +- 0.02125; Expected value within 0.204 stdevs of mean\n\nMAE: -\nMin STD: 0.0116104465\nAvg STD: 0.0122337025\nMax STD: 0.012856959\nOverlap Ratio: 1 (acc 5e-3)\nPearson Coefficient: 0.915762512590914\n\n\n\n\nParameter\ndoor1333\nrevolve8117\n\n\n\n\nAvg STD\n0.0122337025\n0.026211156\n\n\nMax STD\n0.012856959\n0.05688635\n\n\nOverlap Ratio\n1.0\n1.0\n\n\nPearson Coefficient\n0.915763\n0.974905\n\n\n\n\n\n\n13. Monochrome Extrapolation\nsalmon9100: extrapolate downwards\n\ntomato6633 (salad8110): \\(\\rho=0.25\\), \\(P_{val} \\in \\\\{ 0.157,0.231,0.340,0.500\\\\}\\), \\(P_{train} \\in \\\\{0.016,0.023,0.034,0.05,0.073,0.107 \\\\}\\)\nExtrapolating upwards.\n\n\n\n\n\n\n\ntomato6633\nsalad8110\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for tomato6633.\nActual value 0.157: Average = 0.12189 +- 0.00509; Expected value within 6.904 stdevs of mean\nActual value 0.231: Average = 0.13669 +- 0.00462; Expected value within 20.414 stdevs of mean\nActual value 0.34: Average = 0.14682 +- 0.00438; Expected value within 44.096 stdevs of mean\nActual value 0.5: Average = 0.15471 +- 0.00434; Expected value within 79.595 stdevs of mean\n\nMAE: -\nMin STD: 0.004338048\nAvg STD: 0.0046061105\nMax STD: 0.0050855814\nOverlap Ratio: 0 (acc 5e-3)\nPearson Coefficient: 0.891692673465758\n\n\n\n\nParameter\ntomato6633\nsalad8110\n\n\n\n\nAvg STD\n0.0046061105\n0.0035765618\n\n\nMax STD\n0.0050855814\n0.0058416952\n\n\nOverlap Ratio\n0.0\n1.0\n\n\nPearson Coefficient\n0.891693\n0.986550\n\n\n\n\n\nbear3250 (salmon9100): \\(\\rho=0.25\\), $P_{val} \\{ \\}\nExtrapolating downwards.\n\n\n\n\n\n\n\nbear3250\nsalmon9100\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for bear3250.\nActual value 0.016: Average = 0.07631 +- 0.00000; Expected value within 8094958.352 stdevs of mean\nActual value 0.023: Average = 0.07631 +- 0.00000; Expected value within 7155434.256 stdevs of mean\nActual value 0.034: Average = 0.07631 +- 0.00000; Expected value within 5679039.248 stdevs of mean\nActual value 0.05: Average = 0.07631 +- 0.00000; Expected value within 3531555.600 stdevs of mean\n\nPrediction means and standard deviations for salmon9100.\nActual value 0.073: Average = 0.07637 +- 0.00064; Expected value within 5.309 stdevs of mean\nActual value 0.107: Average = 0.11958 +- 0.01252; Expected value within 1.005 stdevs of mean\nActual value 0.157: Average = 0.16357 +- 0.00663; Expected value within 0.990 stdevs of mean\nActual value 0.231: Average = 0.24277 +- 0.02668; Expected value within 0.441 stdevs of mean\nActual value 0.34: Average = 0.34620 +- 0.03178; Expected value within 0.195 stdevs of mean\nActual value 0.5: Average = 0.44783 +- 0.03338; Expected value within 1.563 stdevs of mean\n\nMAE: -\nMin STD: 0.000000007450581\nAvg STD: 0.000000007450581\n\nMax STD: 0.000000007450581\nOverlap Ratio: 0 (acc 5e-3)\nPearson Coefficient: nan\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week1.html",
    "href": "activity_log/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Week 1\n\n\n0. Table of Contents\n\nTable of Contents\nGoals\nCommunication/Recording Method\nInformation Gathering\n\n\n\n1. Goals\nThe main goal of week 1 is to get an introductory grasp of the literature regarding active matter.\nThe secondary goal of week 1 is to set up a communication and recording method for sharing files and information easily throughout this project.\n\n\n2. Communication/Recording Method\nCommunication is done through a github repository. Both myself and my lab partner have personal branches: cp and np respectively. I will try to stick to the convention of referencing my contributions in github commits and code comments as “cp”. We also have a shared branch, where we upload collective work. Per the demands of this assignment, all progress will be documented individually, despite the collaborative nature of the project. There may therefore be repeats between the activity log and various files in the shared branch; wherever possible, the shared folder will be referenced rather than copied (such as the storing of programs and data).\nThere is also the individual gh-pages branch, which hosts a github website which should contain both shared and individual work. This may be the cleanest method of accessing the activity log, though presumably an exported pdf will be required for examination. Nonetheless, I link (embedded) all the relevant pages below:\n\ngithub repository (note that this is private, so it requires an access invite)\nwebsite landing page\n\npersonal activity log\n\n\nThere is also the matter of convention. We have set up weekly supervisor meetings on Thursday. As such, the way weeks are kept track of is slightly unconventional, purely for pragmatic reasons. Week 1, for instance, began on the first Thursday of the university year, and ended on the second Thursday of it. It might therefore seem strange to see major edits in the github repository for, say, the Week 2 activity log on Wednesday night in what would normally be called week 3. I will think of ways to make this easier to keep track of, but the system works for now. There will (hopefully) be an addendum to this paragraph if/when I clarify the record-keeping system.\n\n\n3. Information Gathering\nTaking information from the following articles:\nI. The 2020 motile active roadmap\n\nIntroduction\nActive Brownian particles: from collective phenomena to fundamental physics\n\n\nRun-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers\n\n\nI. The 2020 motile active roadmap, Gompper et al.\n\na. Introduction, Gompper & Roland\nActive matter is a class of nonequilibrium systems composed of a large number of autonomous agents\n\npersistently out of equilibrium (constituents continuously consume energy)\n\nabsence of equilibrium concepts\n\ndetailed balance\nGibbs ensemble & free energy\ntime-reversal symmetry\n\n\ntherefore theories must be constructed on:\n\nsymmetries:\n\npolar shape (regarding polarity of molecules)\nnematic shape (regarding molecules that are aligned loosely parallel)\ninteractions of agents\n\nconservation laws\ndynamic rules\n\nexamples are agent-based standard models\n\nactive Brownian particles\nsquirmers (complemented by continuum field theory)\n\naim is creation of artificial active matter (synthetic micro/nanomachines)\n\nlook to biological active matter\n\npropulsion mechanisms (rotation, translation and periodic altering of shape)\n\ncilia\nflagella\n\nnavigation strategies\n\nchemotaxis: movement/orientation along chemical concentration gradient (toward or away from stimulus)\nphototaxis: movement/orientation towards or away from light\n\n\n(what is the scale lower limit of such behaviour?)\nas such, suggested synthetic micro/nanomachines can utilise:\n\nphoresis\n\ndiffusiophoresis: motion of species A in response to concentration gradient in colloidal species B\nthermophoresis: motion in mixture of particles along temperature gradient (tendency of light molecules to hot and heavy particles to cold)\n\n\n\nswarming: spontaneous self-organisation of active agents in large numbers -&gt; emergent coordinated collective motion on various length scales\n\ndetermined by\n\nagent shape\nsteric interactions\nsensing\nfluctuations\nenvironmentally-mediated interactions\n\nnovel phenomena:\n\nmotility-induced phase separation\nactive turbulence\n\n\n\n\n\nb. Active Brownian particles: from collective phenomena to fundamental physics, Speck\nActive matter makes use of “persistence of motion”; locally broken symmetry, rather than a global preferred direction.\nSynthetic active matter employs particle shape, ultrasound, etc. to facilitate movement.\nJanus particles as important experimental strategy of locomotion: two hemispheres with different surface properties. Example: colloidal particles\n\nsolvent containing H2O2\ncoat one hemisphere in catalyst for H2O2\nresulting local concentration leads to individual particle propulsion along symmetry axis\n\naxis undergoes rotational diffusion due to fluctuation\n\nsingle-particle trajectories with a persistence length analogous to polymers(?)\n\n\n\nActive Brownian Particles (ABPs)\n\npersistent motion\nparticle interactions\n\nshort range\ntypically repulsive\n\nparticles aggregate into clusters (even without cohesive forces)\n\ndynamic feedback between speed and density: motility-induced phase separation (MIPS)\n\n\n\n\n\nII. Run-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers, Soto & Golestanian\nBacteria biofilms constitute development into multicellular communities through aggregation; exhibit novel properties:\n\ndifferentiation\ndelegation of function\n\nFree bacteria, upon interacting with surfaces, adapt to the new conditions by adopting different motility modes\n\nby contrast, biofilms can nucleate when a number of bacteria settle down near a surface and become completely localised\n\nnucleation: first step in formation of a new thermodynamic phase or structure via self-assembly/self-organisation within a substance or mixture\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the home page (or landing spot) for the Dissipative Learning in Active Matter project! This page links to a few useful pages for the project as a whole. For weekly lab notes, see the activity log, also available on the sidebar.\nSome generally useful links for day to day sharing and work:\n\nDeadlines : A quick list of deadlines for this project.\nRepository information : Info on how to set up the repository.\nCommands Information : Useful commands for navigating the repository and publishing pages."
  },
  {
    "objectID": "index.html#information",
    "href": "index.html#information",
    "title": "Home",
    "section": "",
    "text": "Welcome to the home page (or landing spot) for the Dissipative Learning in Active Matter project! This page links to a few useful pages for the project as a whole. For weekly lab notes, see the activity log, also available on the sidebar.\nSome generally useful links for day to day sharing and work:\n\nDeadlines : A quick list of deadlines for this project.\nRepository information : Info on how to set up the repository.\nCommands Information : Useful commands for navigating the repository and publishing pages."
  },
  {
    "objectID": "structured_information/landing.html",
    "href": "structured_information/landing.html",
    "title": "Information Glossary",
    "section": "",
    "text": "This is the landing page for a standardised collection of information regarding active matter. A lot of it will consist of information taken from the activity log (which should still serve as the main documentation of progress) and restructuring it in a more accessible way for ease of use."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "CNN Models",
    "section": "",
    "text": "This page hosts all of the main model architectures utilised in our project, indexed by model number (MN). ### MN_0\n\nMN_1\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,strides=(3,3),activation=“relu”, input_shape=shape)\nBN\nCONV (filters=3,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nDENSE (units=128,activation=‘relu’)\nDO (0.2) (without layout optimiser)\nDENSE (units=10,activation=‘softmax’)\nFLATTEN\nDENSE (units=1,activation=‘linear’)\n\n\n\nMN_1.5\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,strides=(3,3),activation=‘relu’,input_shape=shape)\nBN\nCONV (filters=3,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nCONV (filters=6,kernel_size=(3,3),padding=‘same’)\nBN\nMAXPOOL (pool_size=(3,3))\nFC (units=128,activation=‘relu’)\nDO (0.2) (without layout optimiser)\nFC (units=10,activation=‘softmax’)\nFLATTEN\nFC (units=1,activation=‘linear’)\n\n\n\nMN_2\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=6,kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nAVGPOOL\nDO (0.2) (without layout optimiser)\nDENSE (units=64,activation=‘relu’)\nDO (0.2) (without layout optimiser)\nDENSE (units=10,activation=‘softmax’)\nFLATTEN\nDENSE (units=1,activation=‘linear’)\n\n\n\nMN_3\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=4,kernel_size=(4,4),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=6, kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nAVGPOOL\nDO (0.1) (without layout optimiser)\nFC (units=128,activation=‘relu’)\nDO (0.1) (without layout optimiser)\n\n\n\nMN_3*\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=4,kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=6, kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nAVGPOOL\nDO (0.1) (without layout optimiser)\nFC (units=128,activation=‘relu’)\nDO (0.1) (without layout optimiser)\nFC (units=3,activation=‘relu’)\nFLATTEN\nFC (units=1,activation=‘linear’)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week17.html",
    "href": "activity_log/week17.html",
    "title": "Week 17",
    "section": "",
    "text": "Week 17\n\n\n0. Table of Contents\n\nIntroduction\nStory\nDiscussing Similar Research\nMN_3 Discussion\nExample MN_3 Application\nGaps in \\(P_{tumble}\\) (monochrome)\n\n\n\n1. Introduction\nThe main purpose of this week is to bring out a concrete direction in the numerical research done so far, as well as continue carrying it out.\n\n\n2. Story\nThe aim of the second part of our project was to generate neural network automatised recovery of intrinsic properties within our persistent exclusion process system by evaluating experimental data. The motivation is twofold: pragmatically, the aim is to provide a proof of concept engineering of a tool which can evaluate and identify active matter systems. To this end, generalisation of phenomena is key; such a neural network should be able to adapt to a vast variation of parameters, namely tumbling rates, densities and sample sizes. Furthermore, there is a theoretical component to the utility of this research - on one hand, a machine learning algorithm reproducing the inherent tumbling quantity of a system can serve as external validation of active matter phenomena; with the caveat, of course, that this external validation (by which we mean externalised, removed from human judgement) is nonetheless trained and verified with human-set metrics and understanding.** On the other hand, obtaining a generalised and automated tool for evaluating active matter systems can point human focus to the interesting regions of behaviour, thus supplementing analytical examination.**\nThe algorithm returns various predictions for every tumbling-density-size system it is applied to; these predictions have a spread, which we intend to lower and center around the real tumbling rate, but nonetheless it is important to note that it does not provide a discrete singular prediction. One goal is to qualitatively evaluate the distributions of predictions, with the hopes of a gaussian-like organisation centred on the actual tumbling rate. In this sense the algorithm exhibits human-like prediction. The crux of the issue is determining satisfactory metrics for evaluating the systems. The goal, however, is not to perfectly fine-tune the system as much as to make it applicable enough in order to highlight different qualitative distinctions and how they influence the algorithm’s output - with the explicit intent to link to qualitative phenomena which we have examined in the first part of our project.\n\n1. Degrees of freedom\nOne of the important explorations is examining how changing the input degrees of freedom alters the machine output. By degrees of freedom, we specifically mean positions and orientations - the CNN can be fed either ‘experiment-like’ images, which present only black and white displays of our active matter system, and thus only allows the network to extrapolate on positions, or it can be fed afferent orientations as well, as colours for each particle, and henceforth weight its parameters with this extra difference. We can then explore how training the CNN on one degree of freedom setup influences its predictions on the other. Our expectations going in are that:\n\ntraining the CNN on strictly positions will have similar predictions when validating it on positions+orientations; this is because it will/might simply approach the coloured landscape the same way it approaches the monochrome landscape, by contrasting existing particles with the black background.\ntraining the CNN on positions and orientations will have worse preditions when validating it just on positions; this is because, having been trained to recognise colour and incorporate it in predictions, the system might struggle to ‘explain’ the images which now lack this degree of freedom\n\nIf these expectations prove to be correct, training the CNN on positions+orientations might be a worse prospect, due to most real life applications/evaluations of active matter not being able to access the orientation of agents through still images. Nonetheless, training on positions+orientations may give the CNN a better insight into the clustering phenomenon as a whole, due to the role particle orientations play in cluster formation and evaporation.\nOnce we’ve explored how degrees of freedom alter the CNN predictions, it is useful to explore some modifications to the orientation+position setup. An interesting extension is exploring how misinformation affects the neural network. Some active matter imaging, of, say, Janus particles rolling in 2D, can trace the orientation of a particle. But this tracing is imperfect, and there are some ambiguous cases where orientation may be misread. As such, we could engineer a dataset which ‘misreads’ the orientation of a certain ratio of the total particles, recolouring them.\n\n\n2. Predicting untrained tumbling rates (interpolation, extrapolation)\nAnother important exploration is how training on a certain set of tumbling rates can then cause the CNN to be able to accurately predict tumbling rates in different regions. Our preliminary experiments have not been able to replicate such results - attempting to predict tumbling rates spaced out inbetween the training tumbling rates has caused the system to attempt to fit its predictions to the neighbouring training rates, and therefore given a very large spread to the data. This may however be strictly due to the low training data we have given the CNN so far - both low amount of original evolutions to give the system more depth of experience, as well as low amount of distinct tumbling rates. It ma\nHere are our expectations regarding this part of the project:\n\nUsing interspersed sets of training/validation will yield accurate predictions (on the validation samples) provided the spacing between values is low enough.\nUsing vastly separated sets is going to fare poorly in prediction, due to different clustering behaviour which the CNN is not adequately accustomed to.\n\n\n\n3. Predicting untrained densities\nTraining on a certain set of densities might also present interesting behaviour when applying the algorithm on a different set of densities.\nA noteworthy extension of our research is to then apply this neural network to real active matter systems by ‘translating’ physical scenarios into the visual language our tool is familiar with.\nWe have experimented with various architecture models (a summary of the different architectures can be found here). From now on we will be mostly running the MN_3 (discussed in a section below). Slight variations, if existent, will be noted explicitly, as well as reversals to other architectures.\n\n\n4. Gaps in data\nTraining the CNN on an equally (and slightly) spaced set of tumbling rates might cause it to develop a certain logarithmic bias in establishing a tumbling rate. It is therefore useful to test how the CNN develops when trained on more and/or unevenly spaced tumbling rates, in order to examine any potential misdirections in data.\n\n\n5. System size dependence\nTo gauge how the CNN can trace the underlying behaviour of clustering and detailed balance breaking as it relates to the tumbling rate, it may be useful to examine how a CNN trained on a specific system size will fare in predicting tumbling rates for different system sizes.\n\n\n\n3. Discussing Similar Research\nDeep learning probability flows and entropy production rates in active matter, Boffi and Vanden-Eijnden, 2023.\n\n\n4. MN_3 Discussion\nBelow is the CNN architecture we have settled on. As a reminder, all the architectures used in this project can be found here.\n\nCONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=4,kernel_size=(4,4),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nCONV (filters=6, kernel_size=(5,5),padding=‘same’)\nMAXPOOL (pool_size=(2,2),padding=‘same’)\nReLU\nBN\nAVGPOOL\nDO (0.1) (without layout optimiser)\nFC (units=128,activation=‘relu’)\nDO (0.1) (without layout optimiser)\nFC (units=3,activation=‘relu’)\nFLATTEN\nFC (units=1,activation=‘linear’)\n\nWe have already briefly gone over what each architecture function type does in Weeks 13-15. However, a more general justification is outlined here. The activation function is kept as ReLU althroughout (with the exception of the last function); it is overall both better at accommodating high weights in the kernel functions, as well as much less resource intensive to generate when compared to the sigma function.\nConvolutional layers are our main method of extrapolating features from input data. It is generally good practice to structure architecture such that the first stages employ a small kernel, which is gradually increased with subsequent convolutions. This approach is meant to leverage a fundamental prospect of neural networks: they begin by combing and extrapolating low-level features, which in subsequent layers are meant to form into more general, ‘zoomed out’ features. A good analogy would be drawn number recognition (borrowed from here: the first few stages of a neural network may identify pixels or curved lines, while the latter stages might pick up on entire shapes and ultimately numbers themselves. In a similar vein, our CNN may identify individual paricles and small clustering before it identifies large clusters, and subsequently picks up on underlying tumbling rate. Therefore, the kernel sizes for our CONV layers go from (3,3) to (5,5) in increments of (1,1). The CONV layer depth (its number of filters) is similarly increasing; the first layers pick up on low-level features, which then are extrapolated to form more complex patterns in latter layers.\nEach convolutional layer is followed by a maximum pooling layer. These reduce dimensionality and keep the maximal value in the subregions binned. This is a method of down-sampling, increasing the generality of the feature map in order to circumvent very slight variations significantly altering the neural network training; the pool size is almost always (2,2) in literature, due to further increases resulting in too steep a reduction of output neurons. This is then followed by explicit ReLU activations, and finally we use batch normalisation layers in order to ‘standardise’ the output of previous layers to be used as the input for subsequent layers. Such layers involve variance scaling and mean centering in order to circumvent co-variate shifts caused by the normalised input becoming much smaller or bigger throughout the layers. Batch normalisation layers also help prevent overfitting.\nFinally, after three cycles of CONV-&gt;MAXPOOL-&gt;ReLU-&gt;BN, an average pooling layer further downsizes the system. We use two dropout layers to prevent overfitting. note their placement after pooling layers. Dropout layers aid in getting rid of high dependency on small sets of features (by nullifying a proportion of neurons), but a pooling layer negates some of this by synthesising areas of neurons and converging them into less neurons. Maximum pooling is much more harmful, due to it effectively invalidating any nulled neurons it ‘catches’ in its pool and simply picking the highest value, but average pooling also risks minimising the impact of null neurons by taking the average of its pool.\nThese dropout layers are interspersed with fully connected (dense) layers, which leverage all input neurons as they decrease output neurons (note the sharp decrease of ‘units’). We are essentially attempting to arrive at a single tumbling rate value at the end of the architecture, and therefore will require a single unit for the final FC layer. This is, again, a movement into deeper features by increasing the generality of our layers. the flattening layer that precedes the final FC is there to convert out feature maps to one dimension for final output.\nWe have used ‘same’ padding due to the borders of the image being overall important in our case.\nThe general ballpark for hyperparameters, a broad perspective for architecture motivations, as well as some explanations has been taken from brief surveys of CNN literature. See, for example: 1, 2, 3, 4.\nFor our standard case of \\(N_x=N_y=128\\), the number of tuneable parameters in MN_3 is 2171.\n\n\n5. Example MN_3 Application\nBelow is the model stage4124. It is trained on our full logspace of \\(P_{tumble}\\) values, and only on density \\(\\rho=0.15\\), with 40000 total snapshots and a validation ratio of 0.2 (which is to say, 0.2*40000=8000 is the validation pool). Training was done for 30 epochs.\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nSome parameters are within acceptable margins as outlined in Week 16. Intuitively from the graph, the deviations from our margins relate specifically to the maximum spread (as can be seen in the higher tumbling rate values) and the Pearson’s coefficient.\nThis seems to be a persisting problem for the neural network. As we will see in the section below, lower values of the turning rate tend to have less spread, whereas higher values of the turning rate tend to have more spread.\n\n\n6. Gaps in \\(P_{tumble}\\) (monochrome)\nAs mentioned at the beginning of this week’s log, an exploration of gaps in tumbling rate is essential for understanding the potential shortcomings of architecture MN_3. All models below are trained on 4x rolling, with 1000 snapshots per \\((P_t,\\rho)\\) combination and only \\(\\rho=0.15\\). All models are trained for 30 epochs.\n\nbalteus3123: \\(P_t \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107\\\\}\\)\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nAll parameters are within margins.\n\nMAE: 0.00679200328886509\nMin STD: 0.0000000018626451\nAvg STD: 0.00386919\nMax STD: 0.006857624\nOverlap Ratio: 1.0\nPearson Coefficient: 0.98338868291357\n\n\n\ngoose4421: \\(P_t \\in \\\\{0.016,0.034,0.050,0.107 \\\\}\\)\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nMost parameters are within margins, but the prediction misses the first point.\n\nMAE: 0.00417405366897583\nMin STD: 0.00031636294\nAvg STD:0.0038618112\nMax STD: 0.006754256\nOverlap ratio: 0.75\nPearson Coefficient: 0.988395863033326\n\n\n\nlyrical2734: \\(P_t \\in \\\\{ 0.016\\\\}\\)\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nAll parameters are within margins. It’s hard to gleam this from the graph, since all the prediction points seem to undershoot, but they undershoot by very little. Zoomed out to the magnitudes we usually see spread in (\\(10^{-1}\\) or \\(10^{-2}\\)) they would look perfectly centred. Note that the Pearson coefficient is not available, since we only have one data set.\n\nMAE: 0.000894570257514715\nMin STD: 0.00021818299\nAvg STD: 0.00021818299\nMax STD: 0.00021818299\nOverlap ratio: 1.0\nPearson Coefficient: nan\n\n\n\nbook1634: \\(P_t \\in \\\\{ 0.016,0.073,0.107,0.157 \\\\}\\)\n\n\n\n\n\n\n\nPredictions\nLoss Evolution\n\n\n\n\n\n\n\n\n\nParameters\n\nMAE: 0.00829896610230207\nMin STD: 0.0000000037252903\nAvg STD: 0.007414392\nMax STD: 0.013894851\nOverlap ratio: 0.75\nPearson Coefficient: 0.978325479793127\n\n\n\n\n7. Preliminary Tumbling Rate Extrapolation\nModel balteus3123 yielded promising data when validated on the same parameters it was trained in. By also validating it on the rest of the tumbling rate logspace, we may be able to obtain some useful information about how the neural network learns from data. We expect it to have relatively close tumbling rate predictions for those closest to the logarithmic scale it was given (so, namely, on 0.157, as it is the next value after the trained ones), but that it will break up fairly quickly when trying to predict bigger turning rates.\n\nParameters\n\n[] MAE:\n[] Min STD:\n[] Avg STD:\n[] Max STD:\n[] Overlap ratio:\n[] Pearson Coefficient:\n\nMONOCHROME if img &gt; 0 img = 1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week5-6.html",
    "href": "activity_log/week5-6.html",
    "title": "Weeks 5-6",
    "section": "",
    "text": "Weeks 5-6\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nPersistent Exclusion Process Visualisation\nUtility Functions\nTotal Orientation Against Time\nCluster Analysis\nUpdated Motivational Report\n\n\n\n1. Introduction\nThese two weeks broadly overlapped with reading week in the arts department; I used a significant portion of this time to catch up on other projects. As such, I have elected to combine these two weeks. This specific log is nonetheless expected to be shorter than the previous ones.\nThe goals for this week are as follows:\n\nobtaining a working graph of varying tumble probability and orientation (by arranging screnshots with various values next to each other)\n\nmention a few issues in the code\n\nbegin cluster analysis\ncontinue work on the motivational report\n\n\n\n2. Persistent Exclusion Process Visualisation\nIt became apparent that the images obtained in Week 4 using the sampler.py script to generate static images and the view.py script to visualise certain of these static images showed less clustering when compared to the animation in Week 3. Upon closer analysis, the reasoning is due to the way orientation is set up. lattice.py (the script that sets up the lattice in which particles move) assigns orientation ‘0’ to the background, in order to assign it a color (black). It then assigns orientations to the moving particles, depending on orientation. However, the orientations assigned to particles are ‘0’, ‘1’, ‘2’, ‘3’. What this effectively does is give a quarter of the particles the same orientation as the background, thus rendering them invisible. This is a significant information loss.\nAnother issue is the way I have visualised static lattices in the past. In Week 4 I have opted for downloadable .pdf files, which comes in handy for quick downloading in an unchangeable format (with no potential information loss due to poor quality), but is slightly harder to format. As such the titles are slightly cut off (something I did not notice until much later). I will try to fix it, or otherwise I will switch pack to .png formats.\nBelow is a general graph (before having fixed the orientation problem) showing how clustering varies with varying tumble probability.\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nFor contrast, this is how a similar graph looks like after having fixed the orientation issues:\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nThese are different samples generated for the same values of \\(P_{tumble}\\) and \\(\\rho\\). The contrast given by recovering the last 25% of the particles is very stark - some graphs that formerly displayed individual clusters now prove to have them be connected.\nSome qualitative analysis:\n\nlow densities do not display clustering for any \\(P_{tumble}\\), as there are not enough particles in the environment to actually cluster\nonce density is increased, clustering does occur for certain tumble probability values, as long as density is not too large\nwhen density \\(\\rho\\) gets too large, clustering cannot be said to occur because particles occupy most of the simulation area (and in a sense, they form one big cluster which is trivial for our considerations; we’re exploring how autonomous behaviour creates clustering, whereas this clustering is given simply by sheer quantity)\n\nthis can be counterbalanced by increasing \\(P_{tumble}\\) up to a point - for the selected \\(\\rho\\) values this is (mostly) effective, but for a large enough \\(\\rho\\) no amount of tumbling will preserve discrete and separate clustering\n\n\nThere is still the task of making a 10x10 grid which shows a more elaborate change in clustering across a wider range of values. This can get quite cumbersome and needs to be rendered in a large enough size for the quality to be preserved.\n\n\n3. Utility Functions\nBelow is the script utils.py written by my lab partner, which features two utility functions which will come up occasionally in code. They unpack the h5py files in which the persistent exclusion process data is stored, and obtain unique iteration numbers or mean orientations.\n\"\"\"\nUtility functions\n\"\"\"\nimport re\nimport h5py\nimport numpy as np\n\n\ndef get_ds_iters(key_list: list) -&gt; list:\n    \"\"\"\n    Get all the unique iteration numbers\n\n    :param key_list: a list of all the possible dataset keys/names\n\n    :returns: a list of unique iterations\n    \"\"\"\n    iter_n = []\n    for val in key_list:\n        if re.search(\"^conf_\\d+$\", val):\n            iter_n.append(int(val[5:]))\n    return sorted(iter_n)\n\n\ndef get_mean_orientation(file) -&gt; list:\n    \"\"\"\n    Get the mean orientation at each iteration\n\n    :param file: the h5 file to open [str]\n    :returns: mean orientation [list] of length 1000\n\n    Go through all iteration\n    \"\"\"\n    hf = h5py.File(file, \"r\")\n    key_list = list(hf.keys())\n    iter_n = get_ds_iters(key_list)\n    ori = []\n    ori_acm = []\n    for idx, val in enumerate(iter_n):\n        sshot = np.array(hf[f\"conf_{val}\"]).flatten()\n        avg_ori = np.average(sshot[np.where(sshot != 0)[0]] - 1)\n        ori.append(avg_ori)\n        ori_acm.append(np.mean(ori))\n    return ori_acm\n\n\n4. Total Orientation Against Time\nThe next thing to do is to show how the total orientation of the system differs with time. As a quick preliminary demonstration, we can make use of the video.py function and the total orientation it displays for every frame. Below are two demonstrations for the same particle density, \\(\\rho=0.3\\)\n\n\\(\\rho=0.3; P_{tumble}=0.05\\)\n\n\n\n\n\n\n\nReal-Space Animation\nOrientation Frame Analysis\n\n\n\n\n\n\n\n\n\n\n\n\\(\\rho=0.3; P_{tumble}=0.2\\)\n\n\n\n\n\n\n\nReal-Space Animation\nOrientation Frame Analysis\n\n\n\n\n\n\n\n\n\nThe animations only run over 50 frames - this may be too small a sample size to gauge what’s actually going on. Nonetheless, some basic traits can be inferred:\n\nfor the \\(P_{tumble}=0.05\\) case, the persistent length is long enough that the system does not fundamentally change; fluctuations in total orientation are quite small, of the order \\(10^{-4}\\)\nfor the \\(P_{tumble}=0.2\\) case, the persistent length is shorter - this means that the initial state of the system is heavily chaotic, and begins changing rapidly as clusters form. We can observe a stabilisation effect, which slowly brings fluctuations to about the same order of magnitude as the one above\n\nnote that this is only qualitative analysis so far\n\n\n\n\n\n5. Cluster Analysis\nThe process consists of splitting the particles using neighbour analysis. This is done with scipy.ndimage, using a kernel that only engages with vertical and horizontal neighbours (the ‘1’ values) and disregards diagonal neighbours (the ‘0’ values). The justification for this is that particles only have freedom of movement along vertical and horizontal directions; clustering as such emerges when inner particles are prohibited from moving by outer particles. If a particle is present on the diagonal, it cannot be said to contribute to the cluster; as such, it is disregarded.\nThe code bellow constitutes the main analysis done in this section. It leverages h5py file manipulation and uses a locally-created function (utils.get_ds_iters) for obtaining individual iterations from within it. This code is heavily borrowed from my lab partner’s version (external link), which in turn makes use of our supervisor’s code.\nfrom scipy import ndimage\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport utils\nimport cmcrameri #for different cmaps\n\nPt=0.157 #tumble probability\nrho=0.3 #particle density\nfile = (\"../data/dataset_tumble_{}_{}.h5\".format(Pt,rho)) #change this to analyse different file\n\ncmap1 = plt.get_cmap(name=\"gnuplot\",lut=5) #cmap for first picture\ncmap2 = plt.get_cmap(name=\"cmc.tokyoS\") #cmap for second picture\nhfile = h5py.File(file,\"r\")\n\nfig, (regplot, clusterplot, clusterhistogram) = plt.subplots (1,3,figsize=(9,3),width_ratios=(1,1,1.3),constrained_layout=True)\n\niters = utils.get_ds_iters(hfile.keys())\nfig.suptitle(\"Cluster Analysis (P={}; rho={})\".format(Pt,rho))\n\n#plot regular graph\nimage = hfile[f\"conf_{iters[-1]}\"]\nregplot.matshow(image,cmap=cmap1)\n\n#plot cluster separation graph\nkernel = [[0,1,0],\n          [1,1,1],\n          [0,1,0]]\nlabelled, nlabels = ndimage.label(image,structure=kernel)\nclusterplot.matshow(labelled,cmap=cmap2)\n\n#plot histogram of obtained clusters\ncluster_sizes = np.bincount(labelled.flatten())[1:]\nbin_edges = np.linspace(cluster_sizes.min(),cluster_sizes.max(),100)\ncounts, _ = np.histogram(cluster_sizes,bins=bin_edges,density=True)\nclusterhistogram.grid(alpha=.4)\nclusterhistogram.set_axisbelow(True)\nclusterhistogram.scatter(bin_edges[:-1],counts,edgecolor=(0,0,0,1),facecolor=(0,0,0,.5))\nclusterhistogram.set_yscale(\"log\"), clusterhistogram.set_xscale(\"log\")\nclusterhistogram.set_xlabel(\"Cluster Size\")\n\nfig.colorbar(plt.cm.ScalarMappable(cmap=cmap1),ax=regplot)\nfig.colorbar(plt.cm.ScalarMappable(cmap=cmap2),ax=clusterplot)\nplt.show()\nBelow are a few results with varying $P_{tumble} and a fixed \\(\\rho=0.3\\)\n\n\n\n\n\n6. Updated Motivational Report\n\nFor full evolution history of the report, see the Motivational Report page.\nActive matter is, broadly, a subcategory of condensed matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a medium. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the world of living organisms, where energy is constantly dissipated in order to perform various biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2].\nThis biological emulation through physical models is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3]. Note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics (the autonomous individual behaviour of a human) under duress might deal holistically, rather than individually, with other human agents[4]. The issue is that most active matter systems only form individual relationships between agents, and do not account for the way an agent interacts with the group as a whole - the resulting individual behaviour is merely a summation of the agent’s response to each other agent around it. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds as a collective, and take actions in relation to the collective itself. This objection rests on the assumption that this holistic heuristic does not emerge from individual relations, of course (in which case mapping relationships strictly between individuals is unproblematic).\nThese insights lead to the exploration of various models. For flocks of birds, individual cogntive heuristics tend to suffice - self-propelled particles with adaptive movement patterns based on neighbours can accurately reproduce some migrational patterns [5]. Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[6]. Bacteria lends a great example for exploring the intertwining of phenomena to be emulated by active matter. Some strains (such as Bacillus subtilis) can be modelled using both direct physical interaction (between individuals) and long-distance biochemical signalling (within the collective), with complexity and clustering developing in response to harsh external conditions [7]. The latter interaction is called quorum sensing, the adaptation of the individual to local population density; this has developed into its own active matter branch of individual-to-collective behaviour [8]. Using such models, it is possible to recover the aforementioned human holistic cognitive heuristics [9].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic framework. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states. Exploring how these generalisations would hold as departure from equilibrium through various means is increased is then paramount[10]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[11].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[12]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n756 words.\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nNovel Type of Phase Transition in a System of Self-Driven Particles, Vicsek et al. (1995)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nFormation of complex bacterial colonies via self-generated vortices, Czirok et al. (1996)\nSelf-organization of active particles by quorum sensing rules, Bäuerle et al. (2018)\nQuorum sensing as a mechanism to harness the wisdom of the crowds, Moreno-Gámez et al. (2023)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week4.html",
    "href": "activity_log/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Week 4\n\n\nTable of Contents\n\nTable of Contents\nIntroduction\nMotivational Report Feedback\nVarying Sample Size and Tumble Probability\nMiscellaneous and Unsorted Notes\n\n\n\n1. Introduction\nThe aim of this week is to consolidate the work of previous weeks and to build up qualitative understanding from it. The tasks for the following few weeks are:\n\nLook over supervisor feedback for the motivational report from last week\nMake a 5x10 configuration list of persistent exclusion process particle system from Week 3, varying tumble probability and particle number.\nCombine results with lab partner into a 10x10 grid (qualitatively showing how clustering changes as particle density and tumble probability are altered)\nPlot total orientation against time for various cases\nLook into clustering analysis\n\nOnly the first two points are expected to be done this week.\n\n\n2. Motivation Report Feedback\nI have created a separate website for monitoring development of the Motivational Report here, also accessible through the sidebar. I will address every piece of feedback below.\nMy original context is presented, with the supervisor comments hyperlinked and prefaced by “FT”. Underneath I add my own comments in the form of bullet points. Only the commented parts of the report are shown.\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation\n\nHere I was looking for a broad category to place active matter into; matter systems is indeed too vague. I would have been better off calling it a subcategory of soft matter systems.\nI don’t know exactly where I got the mechanical equilibrium confusion. I may have read some very specific thing that I generalised, but yes, dissipation ought to happen - one of the most important aspects of active matter is the requirement of supplying each autonomous agent with a steady energy supply which they steadily (or perhaps not so steadily in more complex models) use up.\n\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\n\nI forgot that ‘natural world’ in English tends to refer more to general physical processes rather than specifically living organisms; I’ll try to be more specific regarding what active matter models help with understanding.\nFrom the brief look I managed to take at the literature, it seems that discussion of human behaviour in terms of physical systems is quite contentious. In hindsight, I should spend more than a sentence explaining this: the ‘cognitive heuristics’ argument for holism refers to the way humans deal with other humans in immediate crises. Many models will have an individual agent deal with other (in some way) adjacent agents individually; that is to say, it defines its relationship to each agent in turn, and then computes its behaviour. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds (still) as a collective, and take actions in relation to the collective itself. At the time of writing this, it is unclear to me whether there are any active matter models that apply this ‘holistic’ method; the writers I cited, I believe, were criticising the models that do not attempt to do so. This is the case with the basic models I have engaged with so far (such as ABPs). It’s hard to imagine (though not impossible) how such a model can be implemented, but I don’t doubt that newer human-tracking physical models might work in this direction.\nEither way, I’ll look into Andrea Cavagna’s work. I’m interested in exploring this point more in detail.\n\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\n\nI take the point that ‘state’ is the wrong word; another loss in translation. I did mean a more general thermodynamic framework; thermodynamic ‘state’ implies thermal equilibrium, which is exactly what active matter does not have!\nI do get a bit long-winded here; I’ll try to rephrase this paragraph a bit and make sentences more readable\n\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\nYes, I’ll look into examples of pattern formation, as those tend to be quite demonstrative of what active matter study can do.\n\n\n\n3. Varying Sample Size and Tumble Probability\nThere are two independent ways of generating graphical results. The first is to generate frames and animate them into a GIF file, which is done through the script video.py (see Week 3). The other way is what is explored this week: generate a data set with sampler.py and then use view.py to obtain snapshots.\nThe benefits of this are that datasets are stored for reference alongside the images, unlike the gif computation (which only stored the animation). This is useful for record keeping, as well as for understanding the way file storage is organised. The datasets are stored in h5py formats, a way to store huge amounts of data in a compressed manner - the downside is that the understanding process of how data is stored is les straightforward. So far, we have elected to keep a quite unoptimised version of the code for our purposes until we get a firmer grasp of the way the datasets work.\nWe have modified the code slightly to fit our purposes - made a view variables more explicit and standardised some parts. Furthermore, we’ve made the sampler vary with density and tumble probability - our aim is for each of us to generate 50 datasets, with 10 shared density values varied over 5 individual tumble probability values. The end goal, as stated in point 3 in the Introduction, to combine these into a 100 dataset grid.\nEach combination of data is stored in its own .h5 file. They are all indexed by a pandas dataframe, which keeps track of their ascribed particle density, tumble probability, particle speed and iteration count.\nThe particle density \\(\\rho_{p}\\) is defined as a percentage, such that the total number of particles \\(n_{p}\\) is defined by:\n\\[n_{p}=\\rho_{p}n_{x}n_{y}\\] where \\(n_{x}\\) and \\(n_{y}\\) are the dimensions of the lattice sites: the number of lattice sites along the x and y directions, respectively.\nAs defined in Week 3, the tumble probability \\(P_{tumble}\\) is the probability at any given time cycle that a specific particle will change direction. } Below are some preliminary examples of varying the density \\(\\rho_{p} \\in [0.1,0.3,0.5]\\) for a constant tumble probability of \\(P_{tumble} \\cong 0.34\\).\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\nIt’s hard to see exactly how clustering forms at such a high particle density. As such, below are some more examples that vary the density \\(\\rho \\in [0.1,0.2,0.3]\\), under the constant tumble probability of \\(P_{tumble} \\cong 0.073\\). The reasoning is that checking lower particle densities will help avoid noise that gets in the way of clustering, and checking lower tumble probabilities will allow more clustering to occur.\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\n\n\n\nThis browser does not support PDFs..\n\n\n\nClusters can be observed, especially in the \\(\\rho_p = 0.2\\) case. They are arguably also visible in the \\(\\rho_p = 0.3\\) case, although it’s hard to disentangle clusters from one another - this will be very important later, once we start employing cluster analysis.\nSome other things that were not mentioned last week. In all these diagrams and animations, colours indicate orientations. Clustering can therefore be seen by observing the orientation of exterior particles pushing them into an existing chunk, which is then consequently ‘trapped’.\n\n\n4. Miscellaneous and Unsorted Notes\nThere are a few overarching (long-term) goals to pursue right now.\n\n0. Prepare a dataset for CNN\n\n\n1. grid 2D ABP and bin the density\n\n\n2. Explore PDEs\n\\[\\rho_{1}= D_{1}\\nabla \\rho_{1} + f_{1} (\\rho_{1},\\rho_{2},\\rho_{3}) \\] \\[\\rho_{2}= D_{2}\\nabla \\rho_{2} + f_{2} (\\rho_{1},\\rho_{2},\\rho_{3}) \\] \\[\\rho_{3}= D_{3}\\nabla \\rho_{3} + f_{1} (\\rho_{1},\\rho_{2},\\rho_{3}) \\]\nSome other short-term ideas to explore in the following weeks:\n\nSee radial distribution function (probability) - see for Leonard Jones Fluid\nAttempt a reconstruction of features of systems without breaking any symmetries.\nPlot umble rate against predicted tumble rate\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week7.html",
    "href": "activity_log/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Week 7\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nPotential Troubles with Tumbling\nRevisions for Last Week\nInterim Report Research\nInterim Report Progress\n\n\n\n1. Introduction\nThe aim of this week is mostly to work on the upcoming interim report. As such, most of this week’s activity constitutes in gathering data from various papers. A further potential problem in the code was also spotted with regards to how the tumbling rate is established. Beyond that, some minor revisions for the previous week were mentioned.\n\n\n2. Potential Troubles with Tumbling\nExamining the code a bit further, we noticed a peculiarity of the tumbling process - on the offchance that a particle does decide to tumble (the probability of which is \\(P_{tumble}\\), it to picks one of the four orientations (0,1,2,3) with equal chance to switch its orientation to. This is an issue, in that the particle has a 25% chance to “tumble in place”. This means that \\(P_{tumble}\\) has been consistently off by a factor. The real tumbling rate is:\n\\[\nP_{tumble}^{actual}=\\frac{3}{4}P_{tumble}\n\\]\nFor now, we will operate with the flawed \\(P_{tumble}\\). I will mention when this is fixed; for this week, at least, \\(P_{tumble}\\) will refer to the flawed tumbling rate, and \\(P_{tumble}^{actual}\\) will refer to the actual tumbling rate.\n\n\n3. Revisions for Last Week\nI misunderstood the effect of large densities within run-and-tumble models and what it means for clusters: it isn’t that cluster analysis cannot be done due to every particle being joined in the same cluster. Rather, this is where percolation analysis becomes important - will look into it next week further.\nThere is also the matter of units be established: length can be measured in lattice sites, such as by establishing a lattice site convention as \\(a\\).\nThis is technically not a revision, but I’ve updated the cluster analysis grouping graph from last week to also work in log scale. Here is an example, with the histogram bin fitted for log 2:\n\nThis is quite similar with the fitting done by Soto and Golestanian in their 2014 paper “Run-and-tumble dynamics in a crowded environment: Persistent exclusion process for swimmers”. More comparison and research is needed, though.\n\n\n4. Interim Report Research\nI’ve done a lot of research into the Persistent Exclusion Process, some of its underlying principles (such as the simple exclusion interaction, run and tumble dynamics and the zero range process), Active Brownian Particles (ABP), and the like. I would usually list a classificatory list with sorted information; however, due to illness and most of my project time being allotted to working on the interim report itself, I could not make the list this week.\nI will instead make a small list of what I have to look into next week, in anticipation of the deadline:\n\ngeneral convolutional neural network usage\nfurther active matter theories\n\nactive matter field theories\n\nModel B\nModel B+\nModel H, potentially\n\ncollision active matter theories\n\nActive Ornstein Uhlenbeck Particles (AOUP)\n\ncompare and contrast with ABPs\n\n\n\ncorrelation between dissipation and spatial patterns\n\nhow to study entropy through clusters\nlink structure with activity\n\nfurther research percolation\n\n\n\n5. Interim Report Progress\n\n123  124\n\nThis browser does not support PDFs..\n\n125  126\n\nMany things here are subject to change. This is a combination of the Motivational Report from before and the research we have done this week. There are a few places which have to be completed, and the Discussion section is particularly lackluster as of now. The report is currently 1695 words without references.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week20-22.html#import-libraries-1",
    "href": "activity_log/week20-22.html#import-libraries-1",
    "title": "Weeks 20-22",
    "section": "Import Libraries",
    "text": "Import Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('./..')\n\n\n\nimport h5py\nimport glob\nimport re\n\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom cmcrameri import cm\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\nfrom src.utils import get_ds_iters, get_cluster_labels\nfrom src.plot_utils import get_plot_configs"
  },
  {
    "objectID": "activity_log/week20-22.html#get-biggest-cluster-cluster-count-details",
    "href": "activity_log/week20-22.html#get-biggest-cluster-cluster-count-details",
    "title": "Weeks 20-22",
    "section": "Get Biggest Cluster + Cluster Count Details",
    "text": "Get Biggest Cluster + Cluster Count Details\ndensities = [0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\ntumbles = np.logspace(-6, -1, 10, base=2)\nnum = []\nbig = []\nds = []\nts = []\n\nfor idx, d in enumerate(densities):\n    for jdx, t in enumerate(tumbles):\n        file = f\"../data/no-rolling/dataset_tumble_{t:.3f}_density_{d}.h5\"\n        for idx2 in range(500,1000,1):\n            ds.append(d)\n            ts.append(t)\n            labelled, nlabels = get_cluster_labels(file, idx2)\n            lb = labelled.flatten()\n            big.append(np.max(np.bincount(lb)[1:]))\n            num.append(nlabels)"
  },
  {
    "objectID": "activity_log/week20-22.html#create-dataframe-storage",
    "href": "activity_log/week20-22.html#create-dataframe-storage",
    "title": "Weeks 20-22",
    "section": "Create Dataframe Storage",
    "text": "Create Dataframe Storage\ndf = pd.DataFrame()\ndf.insert(0, \"alpha\", ts,300)\ndf.insert(1, \"numclus\", num)\ndf.insert(2, \"bigsize\", big)\ndf.insert(0, \"density\", ds,300)\n\ndf.to_csv(\"cache/cluster_count_size.csv\")"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-results",
    "href": "activity_log/week20-22.html#plot-results",
    "title": "Weeks 20-22",
    "section": "Plot Results",
    "text": "Plot Results\ntumbles = np.logspace(-6, -1, 10, base=2)\ndf = pd.read_csv(\"cache/cluster_count_size.csv\")\ndf[\"density\"] = [\"$%s$\" % x for x in df[\"density\"]] #this is a rough fix for the sns hue not working properly when fed floats (we fix it by forcing the floats into a latex string; a regular string would yield the same problem!)\nsns.set_style(\"ticks\")\nsns.set_style({\"xtick.direction\": \"in\",\"ytick.direction\": \"in\"})\n\nfig, axes = plt.subplots(\n    2,\n    1,\n    figsize=(10, 10),\n    constrained_layout=True,\n    dpi=600\n)\n\nax1 = sns.lineplot(ax=axes[0], data=df, x=\"alpha\", y=\"numclus\", errorbar='sd', marker='o', hue=\"density\", palette=\"cmc.acton\")\nax2 = sns.lineplot(ax=axes[1], data=df, x=\"alpha\", y=\"bigsize\", errorbar='sd', marker='o', hue=\"density\", palette=\"cmc.acton\")\n\nax1.set_ylabel(\"Cluster count\")\nax2.set_ylabel(r\"Biggest cluster volume ($a^2$)\")\n\nfor ax in axes:\n    ax.set_xscale('log')\n    ax.set_xlabel(r\"Tumbling rate ($P_t$)\")\n    ax.get_xaxis().set_major_formatter(ticker.ScalarFormatter())\n    ax.set_xticks(np.round(tumbles,3)[::2])\n\nax2.set_yscale('log')\nsns.move_legend(ax1, \"upper left\", title=r\"Density ($\\rho$)\",ncols=2)\nsns.move_legend(ax2, \"upper right\", title=r\"Density ($\\rho$)\",ncols=3)\nsns.despine()\nplt.show()\n\nfig.savefig(\"../plots/cluster_count_size.pdf\")\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#import-data",
    "href": "activity_log/week20-22.html#import-data",
    "title": "Weeks 20-22",
    "section": "Import Data",
    "text": "Import Data\nimport matplotlib.pyplot as plt\nimport cmcrameri\nimport h5py\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport numpy as np\nfrom scipy import ndimage\nimport seaborn as sns\nimport pandas as pd\nfrom cmcrameri import cm\n\nimport sys\nsys.path.append('./..')\n\nfrom src.utils import get_cluster_labels, get_ds_iters\nfrom src.waffle_plot import waffle_plot\n\n#from src.plot_utils import get_plot_configs\n\n#plot_configs = get_plot_configs()\n#plt.rcParams.update(plot_configs)"
  },
  {
    "objectID": "activity_log/week20-22.html#define-functions",
    "href": "activity_log/week20-22.html#define-functions",
    "title": "Weeks 20-22",
    "section": "Define Functions",
    "text": "Define Functions\ndef plot_labelled_cluster(axis, file, sshot_idx):\n    cmap_label = plt.get_cmap(name=\"gnuplot\")\n    #cmap_label = 'cmc.acton'\n    labelled, _ = get_cluster_labels(file, sshot_idx)\n    axis.matshow(labelled, cmap=cmap_label)\n    return axis\n\n\ndef get_biggest_cluster(img):\n    kernel = [[0, 1, 0], [1, 1, 1], [0, 1, 0]]\n    labelled, _ = ndimage.label(img, structure=kernel)\n    lb = labelled.flatten()\n    cluster_sizes = np.bincount(lb)[1:]\n    biggest_cluster_id = np.argmax(cluster_sizes)\n    loc = ndimage.find_objects(labelled)[biggest_cluster_id]\n    labelled_crop = labelled[loc]\n    img_crop = img[loc]\n    labelled_crop[labelled_crop != biggest_cluster_id+1] = 0\n    labelled_crop[labelled_crop == biggest_cluster_id+1] = 1\n    img_crop *= labelled_crop\n    return img_crop, ndimage.center_of_mass(labelled_crop)\n\ndef get_edges(img,axis):\n    img_threshold = np.zeros_like(img)\n    img_threshold[img &gt; 0] = 1\n    edges = ndimage.sobel(img_threshold, axis=axis)\n    #edges[edges &gt; -2] = 0\n    #edges[edges != 0] = 1\n    edges *= img\n    return edges\n\ndef map_ori(ori):\n    ori_mapped = np.zeros_like(ori, dtype=np.float_)\n    ori_mapped[ori == 1] = np.pi\n    ori_mapped[ori == 2] = np.pi/2\n    ori_mapped[ori == 3] = 0\n    ori_mapped[ori == 4] = -np.pi/2\n    return ori_mapped\n\n\ndef map_ori_human(ori):\n    ori_mapped = np.zeros_like(ori, dtype=np.dtype('U100'))\n    ori_mapped[ori == 3] = \"Down\"\n    ori_mapped[ori == 2] = \"Right\"\n    ori_mapped[ori == 1] = \"Up\"\n    ori_mapped[ori == 4] = \"Left\"\n    return ori_mapped\n\n\n# 0: y, 1: x\ndef get_ori_and_loc(edges,com):\n    positions = edges.nonzero()\n    edges_ori = map_ori(edges[positions[0],positions[1]])\n    edges_loc = np.arctan2((com[0]-positions[0]),(positions[1]-com[1]))\n\n    # CHECK CODE\n    # colors = ListedColormap([\"k\", \"r\", \"yellow\", \"g\", \"b\"])\n    # edges[edges == 0] = -4\n    # edges[positions[0],positions[1]] = edges_loc\n    plt.matshow(edges)\n    plt.colorbar()\n    plt.axvline(com[1], c='w')\n    plt.axhline(com[0], c='w')\n\n    return edges_ori, edges_loc"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-cluster-map",
    "href": "activity_log/week20-22.html#plot-cluster-map",
    "title": "Weeks 20-22",
    "section": "Plot Cluster Map",
    "text": "Plot Cluster Map\nimport matplotlib as mpl\nmpl.rcParams.update(mpl.rcParamsDefault)\nsshot_idx = -1\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3), constrained_layout=True)\nax1 = plot_labelled_cluster(ax1, \"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", sshot_idx)\nax1.text(\n    y=-0.1,\n    x=1,\n    transform=ax1.transAxes,\n    ha=\"right\",\n    s=r\"$\\alpha = 0.016$\",\n)\nax2 = plot_labelled_cluster(ax2, \"../data/no-rolling/dataset_tumble_0.340_density_0.4.h5\", sshot_idx)\nax2.text(\n    y=-0.1,\n    x=1,\n    transform=ax2.transAxes,\n    ha=\"right\",\n    s=r\"$\\alpha = 0.340$\",\n)\nplt.show()\nfig.savefig(\"../plots/cluster_orientation_analysis/cluster_map.svg\")\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#get-frequency-of-up-down-edge-orientations",
    "href": "activity_log/week20-22.html#get-frequency-of-up-down-edge-orientations",
    "title": "Weeks 20-22",
    "section": "Get Frequency of Up-Down Edge Orientations",
    "text": "Get Frequency of Up-Down Edge Orientations\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\n\nimg2 = get_edges(img, axis=0)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\n\nlocs_neg = np.where(img2 == -1)\nlocs_mid = np.where(img2 == 0)\nlocs_pos = np.where(img2 == 1)\n\nneg = img[locs_neg[0][:],locs_neg[1][:]]\nmid = img[locs_mid[0][:],locs_mid[1][:]]\npos = img[locs_pos[0][:],locs_pos[1][:]]\n\ncounts = np.column_stack((np.bincount(neg),np.bincount(pos)))[1:]\n\ndf = pd.DataFrame({\n    'Grad': ([\"Negative\"]+[\"Positive\"])*4,\n    'Orientation': pd.Categorical(\n        [\"Up\", \"Up\", \"Right\", \"Right\", \"Down\", \"Down\", \"Left\", \"Left\"],\n        categories=[\"Left\", \"Up\", \"Right\", \"Down\"]\n    ),\n    'Frequency': counts.flatten(),\n        })\ndf.to_csv(\"cache/updown_ori_freq.csv\")"
  },
  {
    "objectID": "activity_log/week20-22.html#get-frequency-of-left-right-edge-orientations",
    "href": "activity_log/week20-22.html#get-frequency-of-left-right-edge-orientations",
    "title": "Weeks 20-22",
    "section": "Get Frequency of Left-Right Edge Orientations",
    "text": "Get Frequency of Left-Right Edge Orientations\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\n\nimg2 = get_edges(img, axis=1)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\n\nlocs_neg = np.where(img2 == -1)\nlocs_mid = np.where(img2 == 0)\nlocs_pos = np.where(img2 == 1)\n\nneg = img[locs_neg[0][:],locs_neg[1][:]]\nmid = img[locs_mid[0][:],locs_mid[1][:]]\npos = img[locs_pos[0][:],locs_pos[1][:]]\n\ncounts = np.column_stack((np.bincount(neg),np.bincount(pos)))[1:]\n\ndf = pd.DataFrame({\n    'Grad': ([\"Negative\"]+[\"Positive\"])*4,\n   'Orientation': pd.Categorical(\n       [\"Up\", \"Up\", \"Right\", \"Right\", \"Down\", \"Down\", \"Left\", \"Left\"],\n       categories=[\"Left\", \"Up\", \"Right\", \"Down\"]\n   ),\n    'Frequency': counts.flatten(),\n        })\ndf.to_csv(\"cache/leftright_ori_freq.csv\")"
  },
  {
    "objectID": "activity_log/week20-22.html#get-frequency-of-up-down-scrambled-edge-orientations",
    "href": "activity_log/week20-22.html#get-frequency-of-up-down-scrambled-edge-orientations",
    "title": "Weeks 20-22",
    "section": "Get Frequency of Up-Down Scrambled Edge Orientations",
    "text": "Get Frequency of Up-Down Scrambled Edge Orientations\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\nimg[img &gt; 0] = 1\nimg = img * np.random.randint(1, 5, size=(128, 128))\n\nimg2 = get_edges(img, axis=0)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\n\nlocs_neg = np.where(img2 == -1)\nlocs_mid = np.where(img2 == 0)\nlocs_pos = np.where(img2 == 1)\n\nneg = img[locs_neg[0][:],locs_neg[1][:]]\nmid = img[locs_mid[0][:],locs_mid[1][:]]\npos = img[locs_pos[0][:],locs_pos[1][:]]\n\ncounts = np.column_stack((np.bincount(neg),np.bincount(pos)))[1:]\n\ndf = pd.DataFrame({\n    'Grad': ([\"Negative\"]+[\"Positive\"])*4,\n    'Orientation': pd.Categorical(\n        [\"Up\", \"Up\", \"Right\", \"Right\", \"Down\", \"Down\", \"Left\", \"Left\"],\n        categories=[\"Left\", \"Up\", \"Right\", \"Down\"]\n    ),\n    'Frequency': counts.flatten(),\n        })\ndf.to_csv(\"cache/updown_ori_freq_scrambled.csv\")"
  },
  {
    "objectID": "activity_log/week20-22.html#get-frequency-of-left-right-scrambled-edge-orientations",
    "href": "activity_log/week20-22.html#get-frequency-of-left-right-scrambled-edge-orientations",
    "title": "Weeks 20-22",
    "section": "Get Frequency of Left-Right Scrambled Edge Orientations",
    "text": "Get Frequency of Left-Right Scrambled Edge Orientations\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\nimg[img &gt; 0] = 1\nimg = img * np.random.randint(1, 5, size=(128, 128))\n\nimg2 = get_edges(img, axis=1)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\n\nlocs_neg = np.where(img2 == -1)\nlocs_mid = np.where(img2 == 0)\nlocs_pos = np.where(img2 == 1)\n\nneg = img[locs_neg[0][:],locs_neg[1][:]]\nmid = img[locs_mid[0][:],locs_mid[1][:]]\npos = img[locs_pos[0][:],locs_pos[1][:]]\n\ncounts = np.column_stack((np.bincount(neg),np.bincount(pos)))[1:]\n\ndf = pd.DataFrame({\n    'Grad': ([\"Negative\"]+[\"Positive\"])*4,\n    'Orientation': pd.Categorical(\n        [\"Up\", \"Up\", \"Right\", \"Right\", \"Down\", \"Down\", \"Left\", \"Left\"],\n        categories=[\"Left\", \"Up\", \"Right\", \"Down\"])\n    ,\n    'Frequency': counts.flatten(),\n        })\ndf.to_csv(\"cache/leftright_ori_freq_scrambled.csv\")"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-cluster-edge-orientations",
    "href": "activity_log/week20-22.html#plot-cluster-edge-orientations",
    "title": "Weeks 20-22",
    "section": "Plot Cluster Edge Orientations",
    "text": "Plot Cluster Edge Orientations\n#mpl.rcParams.update(mpl.rcParamsDefault)\n\ndf = pd.read_csv(\"cache/leftright_ori_freq_scrambled.csv\")\n\n#plot_configs = get_plot_configs()\n#sns.set(rc=plot_configs)\nsns.set_style(\"ticks\")\nsns.set_style({\"xtick.direction\": \"in\",\"ytick.direction\": \"in\"})\n#plt.rcParams.update(plot_configs)\nfig = plt.figure(figsize=(5,5), constrained_layout=True,dpi=600)\n#ax0 = fig.add_subplot(\n#    1, 1, 1\n#)\nax4 = fig.add_subplot(\n    2, 2, 1\n)\nax2 = fig.add_subplot(\n    2, 2, 2\n)\nax3 = fig.add_subplot(\n    2, 2, 3\n)\nax1 = fig.add_subplot(\n    2, 2, 4\n)\n\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.016_density_0.4.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\nimg_thres = img\nimg_thres[img_thres&gt;0]=1\n#axins = inset_axes(ax4, width=\"100%\", height=\"100%\", borderpad=1)\n#axins.set_axes_locator(InsetPosition(ax4, [.7, 1.3, 1, 1]))\n#axins.matshow(img_thres, cmap='cmc.oslo')\n#axins.tick_params(\n#    axis = \"both\",\n#    which = \"both\",\n#    length = 0,\n#    labelleft = False,\n#    labeltop = False,\n#)\nimg2 = get_edges(img, axis=0)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\ncbar = ax2.matshow(img2, cmap=plt.get_cmap(cm.bam, lut=3))\ncb = plt.colorbar(cbar, ax=ax2, ticks=np.arange(-1, 1 + 1), values=np.arange(-1, 1 + 1), location='bottom', fraction=0.05)\ncb.set_ticklabels([\"Bottom\\nedges\", \"BG.\", \"Top\\nedges\"])\nax2.tick_params(\n    axis = \"both\",\n    which = \"both\",\n    length = 0,\n    labelleft = False,\n    labeltop = False,\n)\n\nimg2 = get_edges(img, axis=1)\nimg2[img2&lt;0] = -1\nimg2[img2&gt;0] = 1\ncbar = ax4.matshow(img2, cmap=plt.get_cmap(cm.bam, lut=3))\ncb = plt.colorbar(cbar, ax=ax4, ticks=np.arange(-1, 1 + 1), values=np.arange(-1, 1 + 1), location='bottom', fraction=0.05)\ncb.set_ticklabels([\"Right\\nedges\", \"BG.\", \"Left\\nedges\"])\nax4.tick_params(\n    axis = \"both\",\n    which = \"both\",\n    length = 0,\n    labelleft = False,\n    labeltop = False,\n)\n\npalette = {'Negative': '#0C4B00', 'Positive': '#65024B' }\ndf = pd.read_csv(\"cache/updown_ori_freq.csv\")\nsns.barplot(ax=ax1, data=df, x='Orientation',y='Frequency', hue='Grad', palette=palette, legend=False, width=0.7, order=[\"Up\",\"Down\",\"Left\",\"Right\"])\nax1.set(xticklabels=[])\n\n#df = pd.read_csv(\"cache/updown_ori_freq_scrambled.csv\")\n#sns.barplot(ax=ax2, data=df, x='Orientation',y='Frequency', hue='Grad', palette=palette)\n#ax2.set(xlabel=None, xticklabels=[], ylabel=None, yticklabels=[])\n#sns.move_legend(ax2, \"upper right\", bbox_to_anchor=(1, 1.1), frameon=False, title=r\"Gradient\")\n\ndf = pd.read_csv(\"cache/leftright_ori_freq.csv\")\nsns.barplot(ax=ax3, data=df, x='Orientation',y='Frequency', hue='Grad', palette=palette, legend=False, width=0.7, order=[\"Up\",\"Down\",\"Left\",\"Right\"])\n\n#df = pd.read_csv(\"cache/leftright_ori_freq_scrambled.csv\")\n#sns.barplot(ax=ax5, data=df, x='Orientation',y='Frequency', hue='Grad', palette=palette, legend=False)\n#ax5.set(yticklabels=[])\n\n\nfor idx, ax in enumerate((ax1,ax3)):\n    sns.despine(ax=ax)\n    ax.set_ylim(0,1100)\n    ax.set(xlabel=None, ylabel=None)\n    ax.set_xticklabels([\"↑\",\"↓\",\"→\",\"←\"])\n    #ax.set_ylabel('Particle count', fontsize=14)\n    ax.set_xlabel(\"Particle orientation\")\n\nax4.set_title(\"Vertical Filter\")\nax2.set_title(\"Horizontal Filter\")\n\n#fig.text(s=\"(A)\",x=0.17, y=0.95)\n#fig.text(s=\"(B)\",x=0.52, y=0.95, backgroundcolor=(1,1,1,0.95))\n#fig.text(s=\"(C)\",x=0.17, y=0.48)\n#fig.text(s=\"(D)\",x=0.52, y=0.48, backgroundcolor=(1,1,1,0.95))\nfig.savefig(\"../plots/cluster_orientation_analysis/cluster_edge_orientations.png\", bbox_inches='tight')\n/tmp/ipykernel_13085/2311038860.py:93: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"↑\",\"↓\",\"→\",\"←\"])\n/tmp/ipykernel_13085/2311038860.py:93: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels([\"↑\",\"↓\",\"→\",\"←\"])\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-waffle-plot-distribution-for-horizontal-filter",
    "href": "activity_log/week20-22.html#plot-waffle-plot-distribution-for-horizontal-filter",
    "title": "Weeks 20-22",
    "section": "Plot Waffle Plot Distribution For Horizontal Filter",
    "text": "Plot Waffle Plot Distribution For Horizontal Filter\ndf = pd.read_csv(\"cache/updown_ori_freq.csv\")\nsns.barplot(ax=ax1, data=df, x='Orientation',y='Frequency', hue='Grad', palette=palette, legend=False)\nax1.set(xticklabels=[])\n\nwaffle_plot(df[df['Grad'] == \"Positive\"]['Orientation'], df[df['Grad'] == \"Positive\"]['Frequency'], cmap=cm.lipari)\nwaffle_plot(df[df['Grad'] == \"Negative\"]['Orientation'], df[df['Grad'] == \"Negative\"]['Frequency'], cmap=cm.lipari)\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#frequency-comparison",
    "href": "activity_log/week20-22.html#frequency-comparison",
    "title": "Weeks 20-22",
    "section": "Frequency Comparison",
    "text": "Frequency Comparison\ndf = pd.read_csv(\"cache/updown_ori_freq.csv\")\ndf2 = pd.read_csv(\"cache/updown_ori_freq_scrambled.csv\")\n\ndf3 = pd.read_csv(\"cache/leftright_ori_freq.csv\")\ndf4 = pd.read_csv(\"cache/leftright_ori_freq_scrambled.csv\")\n\ndiff1 = (df[df['Grad'] == \"Positive\"]['Frequency'].values - df[df['Grad'] == \"Negative\"]['Frequency'].values)\ndiff1[[1,2]] = diff1[[2,1]]\n\ndiff2 = (df3[df3['Grad'] == \"Positive\"]['Frequency'].values - df3[df3['Grad'] == \"Negative\"]['Frequency'].values)\ndiff2[[1,2]] = diff2[[2,1]]\n\ndiff3 = (df2[df2['Grad'] == \"Positive\"]['Frequency'].values - df2[df2['Grad'] == \"Negative\"]['Frequency'].values)\ndiff3[[1,2]] = diff3[[2,1]]\n\ndiff4 = (df4[df4['Grad'] == \"Positive\"]['Frequency'].values - df4[df4['Grad'] == \"Negative\"]['Frequency'].values)\ndiff4[[1,2]] = diff4[[2,1]]\n\nori = np.array([\"Up\", \"Left\", \"Down\", \"Right\"])\nori[[1,2]] = ori[[2,1]]\n\nlen(np.tile(ori,2))\n\ndiff_v = pd.DataFrame({\n    \"Difference\": np.concatenate((\n        diff1,\n        diff2\n    )),\n    \"Orientation\": np.tile(ori,2),\n    'Context': ([\"Vertical filter\"]*4+[\"Horizontal filter\"]*4)*1,\n})\n\n\ndiff_v2 = pd.DataFrame({\n    \"Difference\": np.concatenate((\n        diff3,\n        diff4,\n    )),\n    \"Orientation\": np.tile(ori,2),\n    'Context': ([\"Vertical filter\"]*4+[\"Horizontal filter\"]*4)*1,\n})\nfig = plt.figure(figsize=(4,5), constrained_layout=True,dpi=600)\n\nax1 = sns.barplot(data=diff_v, x='Orientation',y='Difference', palette='cmc.tokyo', hue='Context')\n\nsns.move_legend(ax1, \"lower center\", bbox_to_anchor=(0.5, 0.1), frameon=False, title=None)\n\nax1.set(ylabel=\"Difference between opposing edges\")\n[Text(0, 0.5, 'Difference between opposing edges')]\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#import-libraries-3",
    "href": "activity_log/week20-22.html#import-libraries-3",
    "title": "Weeks 20-22",
    "section": "Import Libraries",
    "text": "Import Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport h5py\nimport numpy as np\n\nimport tensorflow as tf\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom cmcrameri import cm\nfrom numpy import expand_dims\n\nimport sys\nsys.path.append('./..')\n\nfrom src.utils import get_cluster_labels, get_ds_iters\nfrom src.training_utils import (\n    data_load,\n    split_dataset,\n)\nfrom src.plot_utils import get_plot_configs\n2024-04-08 22:29:35.720088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-04-08 22:29:36.831818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"
  },
  {
    "objectID": "activity_log/week20-22.html#load-datasets",
    "href": "activity_log/week20-22.html#load-datasets",
    "title": "Weeks 20-22",
    "section": "Load Datasets",
    "text": "Load Datasets\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.050_density_0.25.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\nimg = img.reshape((img.shape[0], img.shape[1], 1))\n\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.157_density_0.25.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg2 = hf[f\"conf_{iters[300]}\"]\nimg2 = np.array(img2)\nimg2 = img2.reshape((img2.shape[0], img2.shape[1], 1))"
  },
  {
    "objectID": "activity_log/week20-22.html#example-datamaps-p_t-in-0.0500.157-rho-0.25",
    "href": "activity_log/week20-22.html#example-datamaps-p_t-in-0.0500.157-rho-0.25",
    "title": "Weeks 20-22",
    "section": "Example Datamaps (\\(P_t \\in \\{ 0.050,0.157\\}\\), \\(\\rho = 0.25\\))",
    "text": "Example Datamaps (\\(P_t \\in \\{ 0.050,0.157\\}\\), \\(\\rho = 0.25\\))\nplt.matshow(img, cmap='cmc.lajolla')\nplt.xticks([])\nplt.yticks([])\nplt.savefig('../plots/fmaps/input_0.050.svg', bbox_inches='tight', pad_inches=-0.1)\nplt.matshow(img2, cmap='cmc.lajolla')\nplt.xticks([])\nplt.yticks([])\nplt.savefig('../plots/fmaps/input_0.157.svg', bbox_inches='tight', pad_inches=-0.1)\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#set-up-gpu-and-load-model",
    "href": "activity_log/week20-22.html#set-up-gpu-and-load-model",
    "title": "Weeks 20-22",
    "section": "Set Up GPU and Load Model",
    "text": "Set Up GPU and Load Model\nReminder: the following commands need to be ran in console in order to employ GPU. This is not strictly necessary here, since running on the CPU only affects performance (and we are computing very little data here). This is nonetheless a good habit.\nexport CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\nexport LD_LIBRARY_PATH=${CUDNN_PATH}/lib\nexport PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:$PATH\nmodel = tf.keras.models.load_model('../models/orientation0216.keras')\n2024-04-08 22:29:40.971354: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.033494: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.034172: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.036429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.036836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.037016: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.144116: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.144449: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.144621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-08 22:29:41.144752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4784 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-feature-maps",
    "href": "activity_log/week20-22.html#plot-feature-maps",
    "title": "Weeks 20-22",
    "section": "Plot Feature Maps",
    "text": "Plot Feature Maps\nThe following shows output feature maps from each architecture layer (indicated in titles). Row 0 is low tumbling rate (\\(P_t=0.050\\)), row 1 is high tumbling rate (\\(P_t=0.157\\)). Multiple columns indicates multiple emergent feature maps (decided by number of filters) from however many input feature maps the layer takes (dictated by previous layer output maps).\ndef model_mapper (img,img2,model,layer_number,shift=None,ncols=3,post_avgpool=False,flattened=False,output=False,path=f\"../plots/fmaps/\"):\n    model_mini = Model(inputs=model.inputs, outputs=model.layers[layer_number].output)\n\n    feature_maps1 = model_mini.predict(img, verbose=0)\n    feature_maps2 = model_mini.predict(img2, verbose=0)\n\n    if post_avgpool == False and flattened == False:\n        i = 2\n        j = ncols\n        for idx in range(i):\n            for jdx in range(j):\n                ax = plt.subplot(i, j, idx*j+jdx+1)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                if idx == 0:\n                    plt.imshow(feature_maps1[:shift, :, 0, jdx], cmap='cmc.lajolla')\n                else:\n                    plt.imshow(feature_maps2[:shift, :, 0, jdx], cmap='cmc.lajolla')\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}.svg\",bbox_inches='tight')\n\n    if post_avgpool == True and flattened == False: #plots images which do not output multiple filters\n        plt.matshow(feature_maps1[:, :],cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im1.svg\",bbox_inches='tight')\n        plt.matshow(feature_maps2[:, :],cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im2.svg\",bbox_inches='tight')\n\n\n\n    if post_avgpool == True and flattened == True: #rotates images which do not output multiple filters provided they are flattened (turns vertical to horizontal)\n        plt.matshow(np.rot90(feature_maps1[:, :]),cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im1.svg\",bbox_inches='tight')\n        plt.matshow(np.rot90(feature_maps2[:, :]),cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im2.svg\",bbox_inches='tight')\n\n\ndef kernel_printer(model,layer_number=0,path=f\"../plots/fmaps/\"):\n    filters, biases = model.layers[layer_number].get_weights()\n    print (filters.shape[-1])\n    for k in range(filters.shape[-1]):\n        f = filters[:, :, :, k]\n        plt.matshow(f[:,:,0],cmap=\"cmc.lajolla\")\n        plt.xticks([])\n        plt.yticks([])\n        plt.savefig(path+f\"kernel_{k}_layer_{layer_number}.svg\",bbox_inches='tight')\n\n1. CONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nmodel_mapper (img,img2,model,shift=None,layer_number=0,output=True)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712604581.825443    9317 service.cc:145] XLA service 0x78b0500035e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1712604581.825508    9317 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n2024-04-08 22:29:41.880682: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\nI0000 00:00:1712604582.252356    9317 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n\n\npng\n\n\n\nComputed with the Following Kernels\nkernel_printer(model,layer_number=0,path=f\"../plots/fmaps/\")\n3\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\n2. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=64,layer_number=1,output=True)\n\n\n\npng\n\n\n\n\n3. ReLU\nmodel_mapper (img,img2,model,shift=64,layer_number=2,output=True)\n2024-04-08 22:29:44.564777: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n\n\n\npng\n\n\n\n\n4. BN\nmodel_mapper (img,img2,model,shift=64,layer_number=3,output=False)\n\n\n\npng\n\n\n\n\n5. CONV (filters=4,kernel_size=(5,5),padding=‘same’)\nmodel_mapper (img,img2,model,shift=64,ncols=4,layer_number=4,output=True)\n\n\n\npng\n\n\n\nComputed with the Following Kernels\nkernel_printer(model,layer_number=4,path=f\"../plots/fmaps/\")\n4\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\n6. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=5,output=True)\n\n\n\npng\n\n\n\n\n7. ReLU\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=6,output=True)\n\n\n\npng\n\n\n\n\n8. BN\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=7,output=False)\n\n\n\npng\n\n\n\n\n9. CONV (filters=6, kernel_size=(5,5),padding=‘same’)\nmodel_mapper (img,img2,model,shift=32,ncols=6,layer_number=8,output=True)\n\n\n\npng\n\n\nkernel_printer(model,layer_number=8,path=f\"../plots/fmaps/\")\n6\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n10. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=9,output=True)\n\n\n\npng\n\n\n\n\n11. ReLU\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=10,output=True)\n\n\n\npng\n\n\n\n\n12. BN\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=11,output=False)\n\n\n\npng\n\n\n\n\n13. AVGPOOL\nmodel_mapper (img,img2,model,layer_number=12,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n14. DO (0.1) (without layout optimiser)\nmodel_mapper (img,img2,model,layer_number=13,post_avgpool=True,flattened=True,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n15. FC (units=128,activation=‘relu’)\nmodel_mapper (img,img2,model,layer_number=14,post_avgpool=True,flattened=False,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n16. DO (0.1) (without layout optimiser)\nmodel_mapper (img,img2,model,layer_number=15,post_avgpool=True,flattened=False,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n17. FC (units=3,activation=‘relu’)\nmodel_mapper (img,img2,model,layer_number=16,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n18. FLATTEN\nmodel_mapper (img,img2,model,layer_number=17,post_avgpool=True,flattened=True,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n19. FC (units=1,activation=‘linear’)\nmodel_mapper (img,img2,model,layer_number=18,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#some-notes",
    "href": "activity_log/week20-22.html#some-notes",
    "title": "Weeks 20-22",
    "section": "Some notes",
    "text": "Some notes\nThe batch normalisation layers do not display any change becase they simply renormalise the data. They are included here for completeness.\nThe dropout layers don’t actually activate during model predictions. This is a deliberate feature of Keras which we’ve leveraged; the dropout layer helps with dataset training in order to prevent overfitting, but does not eliminate further data during actual prediction applications. They are also included here for completeness."
  },
  {
    "objectID": "activity_log/week20-22.html#import-libraries-4",
    "href": "activity_log/week20-22.html#import-libraries-4",
    "title": "Weeks 20-22",
    "section": "Import Libraries",
    "text": "Import Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport h5py\nimport numpy as np\n\nimport tensorflow as tf\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom cmcrameri import cm\nfrom numpy import expand_dims\n\nimport sys\nsys.path.append('./..')\n\nfrom src.utils import get_cluster_labels, get_ds_iters\nfrom src.training_utils import (\n    data_load,\n    split_dataset,\n)\nfrom src.plot_utils import get_plot_configs\n2024-04-16 16:15:26.430617: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-04-16 16:15:26.456785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-04-16 16:15:26.925737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"
  },
  {
    "objectID": "activity_log/week20-22.html#load-datasets-1",
    "href": "activity_log/week20-22.html#load-datasets-1",
    "title": "Weeks 20-22",
    "section": "Load Datasets",
    "text": "Load Datasets\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.050_density_0.25.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg = hf[f\"conf_{iters[300]}\"]\nimg = np.array(img)\nimg[img&gt;0]=1\nimg = img.reshape((img.shape[0], img.shape[1], 1))\n\nhf = h5py.File(f\"../data/no-rolling/dataset_tumble_0.157_density_0.25.h5\", \"r\")\niters = get_ds_iters(hf.keys())\nimg2 = hf[f\"conf_{iters[300]}\"]\nimg2 = np.array(img2)\nimg2[img2&gt;0]=1\nimg2 = img2.reshape((img2.shape[0], img2.shape[1], 1))"
  },
  {
    "objectID": "activity_log/week20-22.html#example-datamaps-p_t-in-0.0500.157-rho-0.25-1",
    "href": "activity_log/week20-22.html#example-datamaps-p_t-in-0.0500.157-rho-0.25-1",
    "title": "Weeks 20-22",
    "section": "Example Datamaps (\\(P_t \\in \\{ 0.050,0.157\\}\\), \\(\\rho = 0.25\\))",
    "text": "Example Datamaps (\\(P_t \\in \\{ 0.050,0.157\\}\\), \\(\\rho = 0.25\\))\nplt.matshow(img, cmap='cmc.lajolla')\nplt.xticks([])\nplt.yticks([])\nplt.savefig('../plots/fmaps/input_0.050.svg', bbox_inches='tight', pad_inches=-0.1)\nplt.matshow(img2, cmap='cmc.lajolla')\nplt.xticks([])\nplt.yticks([])\nplt.savefig('../plots/fmaps/input_0.157.svg', bbox_inches='tight', pad_inches=-0.1)\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#set-up-gpu-and-load-model-1",
    "href": "activity_log/week20-22.html#set-up-gpu-and-load-model-1",
    "title": "Weeks 20-22",
    "section": "Set Up GPU and Load Model",
    "text": "Set Up GPU and Load Model\nReminder: the following commands need to be ran in console in order to employ GPU. This is not strictly necessary here, since running on the CPU only affects performance (and we are computing very little data here). This is nonetheless a good habit.\nexport CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\nexport LD_LIBRARY_PATH=${CUDNN_PATH}/lib\nexport PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:$PATH\nmodel = tf.keras.models.load_model('../models/monochrome0216.keras')\n2024-04-16 16:15:29.155891: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:29.866598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:29.866961: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:29.871670: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:29.872067: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:29.872279: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:30.087381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:30.087495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:30.087551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-16 16:15:30.088606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6275 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9"
  },
  {
    "objectID": "activity_log/week20-22.html#plot-feature-maps-1",
    "href": "activity_log/week20-22.html#plot-feature-maps-1",
    "title": "Weeks 20-22",
    "section": "Plot Feature Maps",
    "text": "Plot Feature Maps\nThe following shows output feature maps from each architecture layer (indicated in titles). Row 0 is low tumbling rate (\\(P_t=0.050\\)), row 1 is high tumbling rate (\\(P_t=0.157\\)). Multiple columns indicates multiple emergent feature maps (decided by number of filters) from however many input feature maps the layer takes (dictated by previous layer output maps).\ndef model_mapper (img,img2,model,layer_number,shift=None,ncols=3,post_avgpool=False,flattened=False,output=False,path=f\"../plots/fmaps/\"):\n    model_mini = Model(inputs=model.inputs, outputs=model.layers[layer_number].output)\n\n    feature_maps1 = model_mini.predict(img, verbose=0)\n    feature_maps2 = model_mini.predict(img2, verbose=0)\n\n    if post_avgpool == False and flattened == False:\n        i = 2\n        j = ncols\n        for idx in range(i):\n            for jdx in range(j):\n                ax = plt.subplot(i, j, idx*j+jdx+1)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                if idx == 0:\n                    plt.imshow(feature_maps1[:shift, :, 0, jdx], cmap='cmc.lajolla')\n                else:\n                    plt.imshow(feature_maps2[:shift, :, 0, jdx], cmap='cmc.lajolla')\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}.svg\",bbox_inches='tight')\n\n    if post_avgpool == True and flattened == False: #plots images which do not output multiple filters\n        plt.matshow(feature_maps1[:, :],cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im1.svg\",bbox_inches='tight')\n        plt.matshow(feature_maps2[:, :],cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im2.svg\",bbox_inches='tight')\n\n\n\n    if post_avgpool == True and flattened == True: #rotates images which do not output multiple filters provided they are flattened (turns vertical to horizontal)\n        plt.matshow(np.rot90(feature_maps1[:, :]),cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im1.svg\",bbox_inches='tight')\n        plt.matshow(np.rot90(feature_maps2[:, :]),cmap='cmc.lajolla')\n        plt.xticks([])\n        plt.yticks([])\n        if output == True:\n            plt.savefig(path+f\"layer_{layer_number}_im2.svg\",bbox_inches='tight')\n\n\ndef kernel_printer(model,layer_number=0,path=f\"../plots/fmaps/\"):\n    filters, biases = model.layers[layer_number].get_weights()\n    print (filters.shape[-1])\n    for k in range(filters.shape[-1]):\n        f = filters[:, :, :, k]\n        plt.matshow(f[:,:,0],cmap=\"cmc.lajolla\")\n        plt.xticks([])\n        plt.yticks([])\n        plt.savefig(path+f\"kernel_{k}_layer_{layer_number}.svg\",bbox_inches='tight')\n\n1. CONV (filters=3,kernel_size=(3,3),padding=‘same’,input_shape=shape)\nmodel_mapper (img,img2,model,shift=None,layer_number=0,output=True)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1713280530.466361    8794 service.cc:145] XLA service 0x78dc1c002cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1713280530.466396    8794 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n2024-04-16 16:15:30.560875: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\nI0000 00:00:1713280531.625524    8794 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n\n\npng\n\n\n\nComputed with the Following Kernels\nkernel_printer(model,layer_number=0,path=f\"../plots/fmaps/\")\n3\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\n2. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=64,layer_number=1,output=True)\n\n\n\npng\n\n\n\n\n3. ReLU\nmodel_mapper (img,img2,model,shift=64,layer_number=2,output=True)\n2024-04-16 16:15:32.456222: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n\n\n\npng\n\n\n\n\n4. BN\nmodel_mapper (img,img2,model,shift=64,layer_number=3,output=False)\n\n\n\npng\n\n\n\n\n5. CONV (filters=4,kernel_size=(5,5),padding=‘same’)\nmodel_mapper (img,img2,model,shift=64,ncols=4,layer_number=4,output=True)\n\n\n\npng\n\n\n\nComputed with the Following Kernels\nkernel_printer(model,layer_number=4,path=f\"../plots/fmaps/\")\n4\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\n6. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=5,output=True)\n\n\n\npng\n\n\n\n\n7. ReLU\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=6,output=True)\n\n\n\npng\n\n\n\n\n8. BN\nmodel_mapper (img,img2,model,shift=32,ncols=4,layer_number=7,output=False)\n\n\n\npng\n\n\n\n\n9. CONV (filters=6, kernel_size=(5,5),padding=‘same’)\nmodel_mapper (img,img2,model,shift=32,ncols=6,layer_number=8,output=True)\n\n\n\npng\n\n\nkernel_printer(model,layer_number=8,path=f\"../plots/fmaps/\")\n6\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n10. MAXPOOL (pool_size=(2,2),padding=‘same’)\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=9,output=True)\n\n\n\npng\n\n\n\n\n11. ReLU\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=10,output=True)\n\n\n\npng\n\n\n\n\n12. BN\nmodel_mapper (img,img2,model,shift=16,ncols=6,layer_number=11,output=False)\n\n\n\npng\n\n\n\n\n13. AVGPOOL\nmodel_mapper (img,img2,model,layer_number=12,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n14. DO (0.1) (without layout optimiser)\nmodel_mapper (img,img2,model,layer_number=13,post_avgpool=True,flattened=True,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n15. FC (units=128,activation=‘relu’)\nmodel_mapper (img,img2,model,layer_number=14,post_avgpool=True,flattened=False,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n16. DO (0.1) (without layout optimiser)\nmodel_mapper (img,img2,model,layer_number=15,post_avgpool=True,flattened=False,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n17. FC (units=3,activation=‘relu’)\nmodel_mapper (img,img2,model,layer_number=16,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n18. FLATTEN\nmodel_mapper (img,img2,model,layer_number=17,post_avgpool=True,flattened=True,output=False)\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n19. FC (units=1,activation=‘linear’)\nmodel_mapper (img,img2,model,layer_number=18,post_avgpool=True,flattened=True,output=True)\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#some-notes-1",
    "href": "activity_log/week20-22.html#some-notes-1",
    "title": "Weeks 20-22",
    "section": "Some notes",
    "text": "Some notes\nThe batch normalisation layers do not display any change becase they simply renormalise the data. They are included here for completeness.\nThe dropout layers don’t actually activate during model predictions. This is a deliberate feature of Keras which we’ve leveraged; the dropout layer helps with dataset training in order to prevent overfitting, but does not eliminate further data during actual prediction applications. They are also included here for completeness."
  },
  {
    "objectID": "activity_log/week20-22.html#import-packages",
    "href": "activity_log/week20-22.html#import-packages",
    "title": "Weeks 20-22",
    "section": "Import packages",
    "text": "Import packages\nimport numpy as np\nimport h5py\nimport glob\nimport re\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom scipy.stats import pearsonr\n\nimport sys\nsys.path.append('./..')\nfrom src.training_utils import data_load, extract_floats, split_dataset, predict_multi_by_name, plot_violin_and_statistics,cross_mean_err_calculator\n\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D,BatchNormalization,AveragePooling2D,LeakyReLU,GlobalAveragePooling2D,ReLU\n\nfrom cmcrameri import cm\nimport seaborn as sns\nimport pandas as pd\n\nnp.set_printoptions(precision=3, suppress=True)\n2024-04-15 22:46:20.013183: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-04-15 22:46:20.126745: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-04-15 22:46:22.013569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"
  },
  {
    "objectID": "activity_log/week20-22.html#set-seed-optional",
    "href": "activity_log/week20-22.html#set-seed-optional",
    "title": "Weeks 20-22",
    "section": "Set seed (optional)",
    "text": "Set seed (optional)\nfixed_seed = 216 #choose seed (comment out if not needed)\n\nif 'fixed_seed' in locals():\n    keras.utils.set_random_seed(fixed_seed)\n    print(\"Running program with fixed seed:\",fixed_seed)\nelse:\n    print(\"Running program with random seed.\")\nRunning program with fixed seed: 216"
  },
  {
    "objectID": "activity_log/week20-22.html#setup-gpu",
    "href": "activity_log/week20-22.html#setup-gpu",
    "title": "Weeks 20-22",
    "section": "Setup GPU",
    "text": "Setup GPU\nFirst, follow instructions here, or alternatively run:\nfor a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\nWe do this as a workaround for this error:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)\nprint(tf.config.list_physical_devices('GPU'), tf.test.gpu_device_name())\nprint(\"TF Version:\",tf.__version__)\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] /device:GPU:0\nTF Version: 2.16.1\n\n\n2024-04-15 22:46:24.894395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.097387: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.097511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.100146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.100239: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.100292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.201387: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.201471: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.201526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:25.201574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 6187 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9"
  },
  {
    "objectID": "activity_log/week20-22.html#define-functions-1",
    "href": "activity_log/week20-22.html#define-functions-1",
    "title": "Weeks 20-22",
    "section": "Define Functions",
    "text": "Define Functions\ndef violin_plotter (v,y_val,adjustment,legloc=\"upper left\"):\n    bins = np.logspace(-6,-1,10, base=2)*0.85\n\n    #v = prediction2.T[0]\n\n    colors = cm.batlowS(np.digitize(v, bins))\n    colors_actual = cm.batlowS(np.digitize(np.unique(y_val),bins))\n\n    fig, (ax1,ax2) = plt.subplots(nrows=2,ncols=1,figsize=(9,6),dpi=600)\n\n    df = pd.DataFrame()\n    df.insert(0, \"predicted\", v - y_val)\n    df.insert(1, \"actual\", y_val)\n\n    sns.violinplot(\n        ax=ax1,\n        data=df,\n        x=\"actual\",\n        y=\"predicted\",\n        color=\"w\",\n        alpha=0.7,\n        density_norm=\"width\",\n        linewidth=1,\n        inner=\"box\",\n        inner_kws={\"box_width\": 4, \"color\": \"0.2\"},\n        )\n\n    ax1.set_xlabel(\"Actual turning rate\")\n    ax1.set_ylabel(r\"Prediction Difference $P_{pred}-P_{true}$\")\n\n    std = []\n    means = []\n    overlap = []\n    std_div = []\n    accuracy = 5e-3\n    print (\"Prediction means and standard deviations.\")\n    for val in np.unique(y_val):\n        v_mapped = v[np.where(y_val == val)]\n        stdev = np.std(v_mapped)\n        std.append(stdev)\n        mean = np.mean(v_mapped)\n        overlap.append((val + accuracy &gt;= np.min(v_mapped)) & (val - accuracy &lt;= np.max(v_mapped)))\n        within_std = abs(val-mean)/stdev\n        print (f\"Actual value {val}: Average = {mean:.5f} +- {stdev:.5f}; Expected value within {within_std:.3f} stdevs of mean\")\n        std_div.append(within_std)\n\n    print(f\"With accuracy {accuracy}, overlap ratio:\", np.sum(overlap)/len(overlap))\n    print(\"(Min, Max, Avg) STD:\", np.min(std), np.max(std), np.mean(std))\n    print(\"Pearson's correlation coeff: \", pearsonr(y_val, v).statistic)\n\n\n\n    for val in np.unique(y_val):\n        v_mapped = v[np.where(y_val == val)]\n        means.append(np.mean(v_mapped))\n\n    ax2.errorbar(np.sort(np.unique(y_val)),np.abs(means-np.sort(np.unique(y_val))),yerr=(std),ecolor='black',elinewidth=0.5,capsize=3,color='purple',label=r'$|\\langle P_{pred} \\rangle -P_{true}|$')\n    ax2.plot(np.sort(np.unique(y_val)),np.zeros(np.unique(y_val).shape[0]),color='red',label='True value line',linestyle='dotted',alpha=0.5)\n\n\n    ax2.legend(loc=legloc)\n\n    counter = 0\n    for i in np.sort(np.unique(y_val)):\n        ax2.text(i,adjustment,f\"${std_div[counter]:.3f} \\sigma$\",ha=\"center\")\n        counter = counter + 1\n\n    ax2.set_xscale(\"log\")\n    ax2.get_xaxis().set_major_formatter(ticker.ScalarFormatter())\n    ax2.set_xticks(np.unique(y_val))\n\n    ax2.set_xlabel(\"Actual turning rate\")\n    ax2.set_ylabel(\"Absolute mean prediction difference\")\n\n    fig.tight_layout()"
  },
  {
    "objectID": "activity_log/week20-22.html#import-and-prepare-data",
    "href": "activity_log/week20-22.html#import-and-prepare-data",
    "title": "Weeks 20-22",
    "section": "Import and prepare data",
    "text": "Import and prepare data\nset model1 to have orientation, model2 to be monochrome, model3 to be scrambled\n#all alphas: [0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340,0.500]\n#all densities: [0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95]\nx1,y1,shape1 = data_load(alphas=[0.016,0.023,0.034,0.050,0.073,0.1067,0.157,0.231,0.340,0.500], densities=[0.25],orientation=True,scrambled=False)\nx2,y2,shape2 = data_load(alphas=[0.016,0.023,0.034,0.050,0.073,0.1067,0.157,0.231,0.340,0.500], densities=[0.25],orientation=False,scrambled=False)\nx3,y3,shape3 = data_load(alphas=[0.016,0.023,0.034,0.050,0.073,0.1067,0.157,0.231,0.340,0.500], densities=[0.25],orientation=False,scrambled=True)\nWe have N * number of unique alpha snapshots total, we split them into training set and a validation set with the ratio 80/20:\nprint(\"Orientation model:\")\nx_train1, y_train1, x_val1, y_val1 = split_dataset(x1,y1,last=int(len(x1)*1)) #len(x)*1 means no training, only validation!\nx_train2, y_train2, x_val2, y_val2 = split_dataset(x2,y2,last=int(len(x1)*1)) #len(x)*1 means no training, only validation!\nx_train3, y_train3, x_val3, y_val3 = split_dataset(x3,y3,last=int(len(x1)*1)) #len(x)*1 means no training, only validation!\n\nOrientation model:\nNumber of unique alpha:  10\nShape of x:  (10000, 128, 128, 1)\nShape of y:  (10000,)\nSize of training data:  0\nSize of validation data:  10000\nNumber of unique alpha:  10\nShape of x:  (10000, 128, 128, 1)\nShape of y:  (10000,)\nSize of training data:  0\nSize of validation data:  10000\nNumber of unique alpha:  10\nShape of x:  (10000, 128, 128, 1)\nShape of y:  (10000,)\nSize of training data:  0\nSize of validation data:  10000\nfig, (ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3)\nax1.matshow(x_val1[500],cmap=plt.get_cmap(name=\"gnuplot\",lut=5))\nax2.matshow(x_val2[500],cmap=plt.get_cmap(name=\"gnuplot\",lut=5))\nax3.matshow(x_val3[500],cmap=plt.get_cmap(name=\"gnuplot\",lut=5))\n&lt;matplotlib.image.AxesImage at 0x7f253eff1030&gt;\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#predict-multiple-models",
    "href": "activity_log/week20-22.html#predict-multiple-models",
    "title": "Weeks 20-22",
    "section": "Predict multiple models",
    "text": "Predict multiple models\nmodels_one = ['orientation0216','orientation0226','orientation0236','orientation0246','orientation0256','orientation0266','orientation0276','orientation0286','orientation0296','orientation0306'] #array of model names\nmodels_two = ['monochrome0216','monochrome0226','monochrome0236','monochrome0246','monochrome0256','monochrome0266','monochrome0276','monochrome0286','monochrome0296','monochrome0306'] # array of model names\nmodels_three = ['scrambled0216','scrambled0226','scrambled0236','scrambled0246','scrambled0256','scrambled0266','scrambled0276','scrambled0286','scrambled0296','scrambled0306'] # array of model names\n\none_pred_of_one, one_actuals_of_one = predict_multi_by_name(models_one,x_val1,y_val1)\none_pred_of_two, one_actuals_of_two = predict_multi_by_name(models_one,x_val2,y_val2)\none_pred_of_three, one_actuals_of_three = predict_multi_by_name(models_one,x_val3,y_val3)\n\ntwo_pred_of_two, two_actuals_of_two = predict_multi_by_name(models_two,x_val2,y_val2)\ntwo_pred_of_one, two_actuals_of_one = predict_multi_by_name(models_two,x_val1,y_val1)\ntwo_pred_of_three, two_actuals_of_three = predict_multi_by_name(models_two,x_val3,y_val3)\n\nthree_pred_of_three, three_actuals_of_three = predict_multi_by_name(models_three,x_val3,y_val3)\nthree_pred_of_one, three_actuals_of_one = predict_multi_by_name(models_three,x_val1,y_val1)\nthree_pred_of_two, three_actuals_of_two = predict_multi_by_name(models_three,x_val2,y_val2)\n2024-04-15 22:46:33.673278: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:33.673415: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:33.673470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:33.673546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:33.673596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-04-15 22:46:33.673640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6187 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1713217594.529131   80228 service.cc:145] XLA service 0x7f223c004550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1713217594.529189   80228 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n2024-04-15 22:46:34.535575: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-04-15 22:46:34.563498: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\nI0000 00:00:1713217595.037178   80228 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process."
  },
  {
    "objectID": "activity_log/week20-22.html#combined-plots",
    "href": "activity_log/week20-22.html#combined-plots",
    "title": "Weeks 20-22",
    "section": "Combined plots",
    "text": "Combined plots\n#predictions on own kind\n#means1,std1,means2,std2,means3,std3=cross_mean_err_calculator(one_pred_of_one,one_actuals_of_one,two_pred_of_two,two_actuals_of_two,three_pred_of_three,three_actuals_of_three,three_cases=True)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(11,6),dpi=600)\n\n#NOTE: I've commented out the previous method of plotting due to its inelegance, but I have kept it for posterity as a way of plotting error bands without seaborn.\n\n#ONE ON ONE\n#ax1.errorbar(np.sort(np.unique(orientation_actuals_of_orientation)),np.abs(means1-np.sort(np.unique(orientation_actuals_of_orientation))),yerr=(std1),ecolor='blue',elinewidth=0.5,capsize=3,color='blue',label=\"Trained Orientation Predicting Orientation\")\n#ax1.plot(np.sort(np.unique(one_actuals_of_one)),np.abs(means1-np.sort(np.unique(one_actuals_of_one))),'b-',label=\"Trained Orientation Predicting Orientation\")\n#ax1.fill_between(np.sort(np.unique(one_actuals_of_one)),np.abs(means1-np.sort(np.unique(one_actuals_of_one)))-std1,np.abs(means1-np.sort(np.unique(one_actuals_of_one)))+std1,color='b',alpha=0.25)\n\n#TWO ON TWO\n#ax1.errorbar(np.sort(np.unique(monochrome_actuals_of_monochrome)),np.abs(means2-np.sort(np.unique(monochrome_actuals_of_monochrome))),yerr=(std2),ecolor='red',elinewidth=0.5,capsize=3,color='red',label=\"Trained Monochrome Predicting Monochrome\")\n#ax1.plot(np.sort(np.unique(two_actuals_of_two)),np.abs(means2-np.sort(np.unique(two_actuals_of_two))),'r-',label=\"Trained Monochrome Predicting Monochrome\")\n#ax1.fill_between(np.sort(np.unique(two_actuals_of_two)),np.abs(means2-np.sort(np.unique(two_actuals_of_two)))-std2,np.abs(means2-np.sort(np.unique(two_actuals_of_two)))+std2,color='r',alpha=0.25)\n\n#THREE ON THREE\n#ax1.plot(np.sort(np.unique(three_actuals_of_three)),np.abs(means3-np.sort(np.unique(three_actuals_of_three))),'g-',label=\"Trained Scrambled Predicting Scrambled\")\n#ax1.fill_between(np.sort(np.unique(three_actuals_of_three)),np.abs(means3-np.sort(np.unique(three_actuals_of_three)))-std3,np.abs(means3-np.sort(np.unique(three_actuals_of_three)))+std3,color='g',alpha=0.25)\n\n#ZERO LINE, LEGEND, AX1 PLOT CONFIG\n#ax1.plot(np.sort(np.unique(one_actuals_of_one)),np.zeros(np.unique(one_actuals_of_one).shape[0]),color='black',label='True value line',linestyle='dotted',alpha=0.5)\n#ax1.legend(loc='upper left')\n#ax1.set_xscale(\"log\")\n#ax1.get_xaxis().set_major_formatter(ticker.ScalarFormatter())\n#ax1.set_xticks(np.unique(y_val1))\n#ax1.set_xlabel(\"Actual turning rate\")\n#ax1.set_ylabel(\"Absolute mean prediction difference\")\n\ndf_one = pd.DataFrame()\ndf_two = pd.DataFrame()\ndf_three = pd.DataFrame()\ndf_one.insert(0,\"predicted\",np.abs(one_pred_of_one-one_actuals_of_one))\ndf_one.insert(1,\"actuals\",one_actuals_of_one)\ndf_two.insert(0,\"predicted\",np.abs(two_pred_of_two-two_actuals_of_two))\ndf_two.insert(1,\"actuals\",np.abs(two_actuals_of_two))\ndf_three.insert(0,\"predicted\",np.abs(three_pred_of_three-three_actuals_of_three))\ndf_three.insert(1,\"actuals\",np.abs(three_actuals_of_three))\ndf_one['Data Type']='Orientation'\ndf_two['Data Type']='Monochrome'\ndf_three['Data Type']='Scrambled'\ncdf = pd.concat([df_one,df_two,df_three])\n#print(cdf.head())\n\nsns.lineplot(ax=ax1,\n             x=\"actuals\",\n             y=\"predicted\",\n             hue=\"Data Type\",\n             data=cdf,\n             errorbar=\"sd\",\n             palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nsns.boxplot(ax=ax2,\n            data=cdf,\n            x=\"actuals\",\n            y=\"predicted\",\n            hue=\"Data Type\",\n            fill=False,\n            gap=.4,\n            whis=(0,100),\n            width=.5,\n            palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nax1.set_xlabel(\"Actual turning rate\",fontsize=16)\nax2.set_xlabel(\"Actual turning rate\",fontsize=16)\nax1.set_ylabel(\"Absolute mean prediction difference\",fontsize=16)\nax2.set_ylabel(\"Absolute mean prediction difference\",fontsize=16)\nax2.set_title(\"IQR Comparison of Self-Prediction\",fontsize=16)\nax1.set_title(\"STD Comparison of Self-Prediction\",fontsize=16)\n\nax1.set_xscale(\"log\")\nax1.get_xaxis().set_major_formatter(ticker.ScalarFormatter())\nax1.set_xticks(np.unique(y_val1))\n\n#handles,labels=ax2.get_legend_handles_labels()\n#ax2.legend(handles=handles[1:],labels=labels[1:]) #this should fix hue title appearing in rightmost figure legend\nax1.legend(loc=\"upper left\",fontsize=16)\nax2.legend(fontsize=16)\n\n\n#predictions on other kind\n#means1,std1,means2,std2,means3,std3=cross_mean_err_calculator(one_pred_of_two,one_actuals_of_two,two_pred_of_one,two_actuals_of_one,three_cases=False)\n\n#ONE ON TWO\n#ax2.errorbar(np.sort(np.unique(orientation_actuals_of_monochrome)),np.abs(means1-np.sort(np.unique(orientation_actuals_of_monochrome))),yerr=(std1),ecolor='blue',elinewidth=0.5,capsize=3,color='blue',label=\"Trained Orientation Predicting Monochrome\")\n#ax2.plot(np.sort(np.unique(one_actuals_of_two)),np.abs(means1-np.sort(np.unique(one_actuals_of_two))),'b-',label=\"Trained Orientation Predicting Monochrome\")\n#ax2.fill_between(np.sort(np.unique(one_actuals_of_two)),np.abs(means1-np.sort(np.unique(one_actuals_of_two)))-std1,np.abs(means1-np.sort(np.unique(one_actuals_of_two)))+std1,color='b',alpha=0.25)\n\n#TWO ON ONE\n#ax2.errorbar(np.sort(np.unique(monochrome_actuals_of_orientation)),np.abs(means2-np.sort(np.unique(monochrome_actuals_of_orientation))),yerr=(std2),ecolor='red',elinewidth=0.5,capsize=3,color='red',label=\"Trained Monochrome Predicting Orientation\")\n#ax2.plot(np.sort(np.unique(two_actuals_of_one)),np.abs(means2-np.sort(np.unique(two_actuals_of_one))),'r-',label=\"Trained Monochrome Predicting Orientation\")\n#ax2.fill_between(np.sort(np.unique(two_actuals_of_one)),np.abs(means2-np.sort(np.unique(two_actuals_of_one)))-std2,np.abs(means2-np.sort(np.unique(two_actuals_of_one)))+std2,color='r',alpha=0.25)\n\n#ZERO LINE, LEGEND, AX2 PLOT CONFIG\n#ax2.plot(np.sort(np.unique(one_actuals_of_two)),np.zeros(np.unique(one_actuals_of_two).shape[0]),color='black',label='True value line',linestyle='dotted',alpha=0.5)\n#ax2.legend(loc='upper left')\n#ax2.set_xscale(\"log\")\n#ax2.get_xaxis().set_major_formatter(ticker.ScalarFormatter())\n#ax2.set_xticks(np.unique(y_val1))\n#ax2.set_xlabel(\"Actual turning rate\")\n#ax2.set_ylabel(\"Absolute mean prediction difference\")\n\nfig.tight_layout()\n\n\n\npng\n\n\ndf12=pd.DataFrame()\ndf21=pd.DataFrame()\ndf13=pd.DataFrame()\ndf31=pd.DataFrame()\ndf23=pd.DataFrame()\ndf32=pd.DataFrame()\ndf12.insert(0,\"predicted\",np.abs(one_pred_of_two-one_actuals_of_two))\ndf12.insert(1,\"actuals\",one_actuals_of_two)\ndf21.insert(0,\"predicted\",np.abs(two_pred_of_one-two_actuals_of_one))\ndf21.insert(1,\"actuals\",two_actuals_of_one)\ndf13.insert(0,\"predicted\",np.abs(one_pred_of_three-one_actuals_of_three))\ndf13.insert(1,\"actuals\",one_actuals_of_three)\ndf31.insert(0,\"predicted\",np.abs(three_pred_of_one-three_actuals_of_one))\ndf31.insert(1,\"actuals\",three_actuals_of_one)\ndf23.insert(0,\"predicted\",np.abs(two_pred_of_three-two_actuals_of_three))\ndf23.insert(1,\"actuals\",two_actuals_of_three)\ndf32.insert(0,\"predicted\",np.abs(three_pred_of_two-three_actuals_of_two))\ndf32.insert(1,\"actuals\",three_actuals_of_two)\n\nfig,ax = plt.subplots(nrows=3,ncols=2,figsize=(12,16),dpi=600)\n\n#PREDICTING ONE\n\ndf21['Data Type']='Monochrome'\ndf31['Data Type']='Scrambled'\ncdf = pd.concat([df_one,df21,df31])\n\nsns.lineplot(ax=ax[0][0],\n             x=\"actuals\",\n             y=\"predicted\",\n             hue=\"Data Type\",\n             data=cdf,\n             errorbar=\"sd\",\n             palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nsns.boxplot(ax=ax[0][1],\n            data=cdf,\n            x=\"actuals\",\n            y=\"predicted\",\n            hue=\"Data Type\",\n            fill=False,\n            gap=.4,\n            whis=(0,100),\n            width=.5,\n            palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\n\n\n#PREDICTING TWO\n\ndf12['Data Type']='Orientation'\ndf32['Data Type']='Scrambled'\ncdf = pd.concat([df12,df_two,df32])\n\nsns.lineplot(ax=ax[1][0],\n             x=\"actuals\",\n             y=\"predicted\",\n             hue=\"Data Type\",\n             data=cdf,\n             errorbar=\"sd\",\n             palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nax[0][0].legend(loc='upper left',fontsize=16)\nax[0][1].legend(loc='upper right',fontsize=16)\n\nsns.boxplot(ax=ax[1][1],\n            data=cdf,\n            x=\"actuals\",\n            y=\"predicted\",\n            hue=\"Data Type\",\n            fill=False,\n            gap=.4,\n            whis=(0,100),\n            width=.5,\n            palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nax[1][0].legend(loc='upper left',fontsize=16)\nax[1][1].legend(loc='upper left',fontsize=16)\n\n#PREDICTING THREE\n\ndf13['Data Type']='Orientation'\ndf23['Data Type']='Monochrome'\ncdf = pd.concat([df13,df23,df_three])\n\nsns.lineplot(ax=ax[2][0],\n             x=\"actuals\",\n             y=\"predicted\",\n             hue=\"Data Type\",\n             data=cdf,\n             errorbar=\"sd\",\n             palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nsns.boxplot(ax=ax[2][1],\n            data=cdf,\n            x=\"actuals\",\n            y=\"predicted\",\n            hue=\"Data Type\",\n            fill=False,\n            gap=.4,\n            whis=(0,100),\n            width=.5,\n            palette={\"Orientation\": \"blue\", \"Monochrome\": \"red\", \"Scrambled\": \"green\"})\n\nax[2][1].legend(fontsize=16)\nax[2][0].legend(loc='upper left',fontsize=16)\n\nfor i,examiner in enumerate([\"STD\",\"IQR\"]):\n    for j,examined in enumerate([\"Orientation\",\"Monochrome\",\"Scrambled\"]):\n        ax[j][i].set_xlabel(\"Actual turning rate\",fontsize=16)\n        ax[j][i].set_ylabel(\"Absolute mean prediction difference\",fontsize=16)\n        ax[j][i].set_title(f\"{examiner} Comparison Predicting on {examined}\",fontsize=16)\n        if i == 0:\n            ax[j][i].set_xscale(\"log\")\n            ax[j][i].get_xaxis().set_major_formatter(ticker.ScalarFormatter())\n            ax[j][i].set_xticks(np.unique(y_val1))\n\n\n\nfig.tight_layout()\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#interpolation-predictions-on-same-data-types",
    "href": "activity_log/week20-22.html#interpolation-predictions-on-same-data-types",
    "title": "Weeks 20-22",
    "section": "Interpolation Predictions on Same Data Types",
    "text": "Interpolation Predictions on Same Data Types\n[png](week-20-22-files/more_interpol_std_iqr_comparison_of_self_better.png"
  },
  {
    "objectID": "activity_log/week20-22.html#interpolation-predictions-on-other-data-types",
    "href": "activity_log/week20-22.html#interpolation-predictions-on-other-data-types",
    "title": "Weeks 20-22",
    "section": "Interpolation Predictions on Other Data Types",
    "text": "Interpolation Predictions on Other Data Types\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#low-bound-extrapolation",
    "href": "activity_log/week20-22.html#low-bound-extrapolation",
    "title": "Weeks 20-22",
    "section": "Low-bound Extrapolation",
    "text": "Low-bound Extrapolation\n\nPredictions on Same Data Types\n\n\n\npng\n\n\n\n\nPredictions on Different Data Types\n\n\n\npng"
  },
  {
    "objectID": "activity_log/week20-22.html#high-bound-extrapolation",
    "href": "activity_log/week20-22.html#high-bound-extrapolation",
    "title": "Weeks 20-22",
    "section": "High-bound Extrapolation",
    "text": "High-bound Extrapolation\n\nPredictions on Same Data Types\n\n\n\npng\n\n\n\n\nPredictions on Different Data Types\n\n\n\npng"
  },
  {
    "objectID": "activity_log/landing.html",
    "href": "activity_log/landing.html",
    "title": "Activity log",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\n\nWeeks 13-15\n\n\n\n\n\n\n\nWeek 16\n\n\n\n\n\n\n\nWeek 17\n\n\n\n\n\n\n\nWeek 18\n\n\n\n\n\n\n\nWeek 19\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\nWeeks 20-22\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\n\nWeeks 5-6\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\n\nWeeks 10-13\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "activity_log/week3.html",
    "href": "activity_log/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Week 3\n\n\n0. Table of Contents\n\nTable of Contents\nGoals\nConfiguring BlueCrystal4\nSmall Github Digression\nPersistent Exclusion Process Code\nCode Examination\n\n\n\n1. Goals\nThe main aim of this week is to begin reproducing a simple active matter system in code. To this end, a suitable environment to work in must be found and utilised. For now we are primarily sticking to python.\nWe have gained access to the Bristol supercomputers BlueCrystal4 and BluePebble (I have access to the former, my lab partner has access to the latter). We will attempt to apply the scripts to the supercomputers.\nIn Week 2 it was suggested that I would go more in depth in statistical analysis. This has been put aside for now, in the interest of getting a feel for how ABP simulations work in practice.\n\n\n2. Configuring BlueCrystal4\nI already have access to BlueCrystal4 (BC4) due to one of my other modules. For that particular module, I am programming in C/C++; as such, I have already configured (installed and loaded) modules compatible with it. These modules are generally installed on a user instance, and loaded using the following bash command: module load &lt;path&gt;. For example, for loading the necessary files for Intel’s C/C++ compiler (icc), I use the following: module load languages/intel/2020-u4.\nThe issue that arises is that, at least initially, we have decided to work in Python. While I plan to attempt to translate (and speed up) our later codes in C, it is preferable to agree on a shared language while we get our bearings (and Python is easier, for all its faults). I therefore need use both C and python modules; hopefully this will not cause conflicts as long as they are not loaded simultaneously. The loading can easily be done in the .bashrc script on the user home instance, which will keep the module loaded for any and all processes. This is inadvised in the BC4 user manual, however - like I said, conflicts can occur. It is much healthier and safer to instead load modules in the .sh script that will send a request to the job queue. This will be elaborated on later.\nYou load/add modules with module load or module add. I have used the former for C and the latter for Python, though strictly to differentiate the two better. Below is a quick list of the different module commands I use:\nmodule avail #Lists available modules; can combine with grep to search for a specific one\n\nmodule load languages/intel/2020-u4 #Loads icc for C/C++\nmodule add languages/anaconda3/2022.11-3.9.13-tensorflow-2.11 #Loads anaconda tensor flow package\nmodule add languages/anaconda3/2020−3.8.5 #Loads full anaconda package\ncurl https://pyenv.run | bash #Loads python environment (non-anaconda alternative)\n\n#NOTE: only one of these should be done for any instance!\n\nmodule list #Lists loaded modules\nTested a quick “Hello, World!” program just to make sure that python runs correctly; it does, at least for now.\n\n\n3. Small Github Digression\nWe have added a shared file regarding various commands for Github and Quarto. It can now be found here, as well as in the landing page, currently under the name ‘Commands Information’.\n\n\n4. Persistent Exclusion Process Code\nThe code models particles moving across various lattice sites, with a ‘tumble’ probability. This is the probability that the particle will change its direction at every iteration (or time ‘tick’). This means that the tumble variable is inversely proportional to the persistence length of the environment.\nNote that this is a (relatively) simple algorithm: every particle has the same chance to change direction (in a more accurate model they would more likely follow a probability distribution instead).Nonetheless, it’s a good start for getting a feel for one of the core topics of this project: the way persistence length influences collective behaviour. In this case, the clustering phenomena is heavily affected.\nBelow are some GIF files with varying tumble speeds \\(P_{tumble}\\) applied to the model. Note that the animation runs at 6 frames per second, and computes 50 frames total.\n\n\\(P_{tumble}=0.0005\\)\n\n\n\nptumble_0.0005\n\n\nThe persistence length is so small here that almost all particles form into clusters.\n\n\n\\(P_{tumble}=0.001\\)\n\n\n\nptumble_0.001\n\n\nThe clustering is still noticeable here, but there are many more free particles roaming already (with a persistence length twice as big as the previous one).\n\n\n\\(P_{tumble}=0.01\\)\n\n\n\nptumble_0.01\n\n\nClusters are far less frequent, but still noticeable. This is a tenfold increase in persistence length compared to the previous case.\n\n\n\\(P_{tumble}=0.1\\)\n\n\n\nptumble_0.1\n\n\nBarely any clusters form here, and when they do, it’s only for a few frames.\n\n\n\\(P_{tumble}=0.33\\)\n\n\n\nptumble_0.33\n\n\nThere are no noticeable clusters.\nNote that if \\(P_{tumble}=1\\), there is no autonomous activity within the system. In other words, particles exercise simple inertial movement in frictionless environment.\n\n\n\n5. Code Examination\nWe have started looking at the code. It’s hard to standardise everything we have talked about into notes format; most of it is commented on the repository (formalised as in-line comments and docstrings). As the status of this repository is unknown at this time (whether it will be included in the final project submissions, that is; it is, after all, an example supplied by the supervisor, which we are analysing to get a better understaning of the topic), I will summarise to the best of my ability. (Note that this will be vastly incomplete, as we have only discussed a few scripts, and only partially!).\n\nlattice.py\n\nestablishes a lattice class\n\ncall each instance that fits within a class an object\n\nestablishes various attributes for the class\n\nNsites: the total amount of lattice sites within system\n‘Nparticles’: the total amount of particles moving around the system (with the stipulation that only one particle can fit in a lattice site at a given time)\nconnectivity=4: we are unsure what this is as of yet; operating under the assumption that this is the amount of neighbours a lattice site is connected to (and thus the amount of places a particle occupying a lattice site can jump to, depending on its orientation)\n\nset_square_connectivity subfunction\n\nrequires values of Nx and Ny (number of sites along x axis and number of sites along y axis) to rectangularly fit the number of total sites (Nx*Ny==Nsites)\ncreates a ‘neighbor table’ using 2D numpy arrays, with each row being an individual lattice site and each column being an individual neighbour (along collectivity)\nthis is flattened into a one-dimensional array, ran through construct_square_neighbor_table (discussed below), then the result is obtained in both a one-dimensional (flattened) version and in a restored two-dimensional version\n\n\nconstruct_square_neighbor_table\n\ntakes the neighbor table from lattice.py as outlined above\nthe fastest way to store data in C is through a one dimensional array called with pointers, as it ensures that all the data is contiguously stored in the same memory address area\n\nthe indexing is of note (and may be useful later)\n\ni,j lattice sites\n\ni goes through Nx positions (rows)\nj goes through Ny positions (columns)\n\nindex: i * Ny + j\n\n\n\n\n\n\n\n2D to 1D array sketch example\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "activity_log/week19.html",
    "href": "activity_log/week19.html",
    "title": "Week 19",
    "section": "",
    "text": "Week 19\n\n\n0. Table of Contents\n\nIntroduction\nAnalysis Update\nHigher Number Interpolation\nPreliminary Confusion Code\nImproved Full Tumbling Rate Analysis (more rolling, more epochs)\nRunning Same Parameters Multiple Times\nAuto-Correlation Function\n\n\n\n1. Introduction\nThis week consists of exploring higher range parameters in order to consolidate the results in previous weeks.\n\n\n2. Analysis Update\nWe have finally fixed GPU tensorflow, which greatly accelerates model training.\n\nsteadfast7653: \\(P_{t} \\in \\\\{0.016,0.023,0.034,0.050 \\\\}\\), \\(\\rho = 0.25\\), 30 epochs ; sardine0022: same, same, 60 epochs\n\n\n\n\n\n\n\nsteadfast7653 (30 epochs)\nsardine0022 (60 epochs)\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for steadfast7653.\nActual value 0.016: Average = 0.01838 +- 0.00016; Expected value within 15.328 stdevs of mean\nActual value 0.023: Average = 0.02152 +- 0.00409; Expected value within 0.363 stdevs of mean\nActual value 0.034: Average = 0.03588 +- 0.00384; Expected value within 0.489 stdevs of mean\nActual value 0.05: Average = 0.04996 +- 0.00318; Expected value within 0.012 stdevs of mean\n\nPrediction means and standard deviations for sardine0022.\nActual value 0.016: Average = 0.01427 +- 0.00018; Expected value within 9.728 stdevs of mean\nActual value 0.023: Average = 0.01892 +- 0.00370; Expected value within 1.103 stdevs of mean\nActual value 0.034: Average = 0.03128 +- 0.00310; Expected value within 0.878 stdevs of mean\nActual value 0.05: Average = 0.04516 +- 0.00273; Expected value within 1.776 stdevs of mean\n\n\n\n\n\n\n\nsteadfast7653 (30 epochs)\nsardine0022 (60 epochs)\n\n\n\n\n\n\n\n\n\n\n\n\n3. Higher Number Interpolation\n\noutstretched4188 (antenna4149): \\(P_{val} \\in \\\\{0.023,0.034,0.050 \\\\}\\), \\(P_{train} \\in \\\\{0.016,0.073,0.107,0.157 \\\\}\\), \\(\\rho = 0.25\\), 30 epochs\n\n\n\n\n\n\n\noutstretched4188 (evaluation)\nantenna4149 (training)\n\n\n\n\n\n\n\n\n\nPrediction means and standard deviations for outstreched4188.\nActual value 0.023: Average = 0.02421 +- 0.00409; Expected value within 0.296 stdevs of mean\nActual value 0.034: Average = 0.04339 +- 0.00852; Expected value within 1.102 stdevs of mean\nActual value 0.05: Average = 0.07137 +- 0.00582; Expected value within 3.674 stdevs of mean\n\nPrediction means and standard deviations for antenna4149.\nActual value 0.016: Average = 0.02139 +- 0.00216; Expected value within 2.499 stdevs of mean\nActual value 0.073: Average = 0.07590 +- 0.00150; Expected value within 1.940 stdevs of mean\nActual value 0.107: Average = 0.11229 +- 0.00916; Expected value within 0.577 stdevs of mean\nActual value 0.157: Average = 0.15259 +- 0.00645; Expected value within 0.683 stdevs of mean\nCompare to equivalent case from last week:\n\n\n\n\n\n\n\nstory4919 (evaluation)\nsummer6911 (training)\n\n\n\n\n\n\n\n\n\n\n\n\n4. Preliminary Confusion Code\nimg = fin[key][:]\nfor i in range(128):\n    for j in range(128):\n        if img[i,j] &gt; 0:\n            chance=float(random.randint(1,100))/100\n            if chance &lt;= 0.05:\n                while (True):\n                    x = random.randint(1,4)\n                    if x != img[i,j]:\n                        break\n                img[i,j] = x\nimg = img.astype(float)/4.0\n\nless5622: \\(P_{tumbke} = 0.016\\), \\(\\rho = 0.25\\), 60 epochs, 4000 (0.2) snapshots\n\n\n\n\n5. Improved Full Tumbling Rate Analysis (more rolling, more epochs)\n\nstag0149: \\(P_{tumble} \\in \\\\{0.016,0.023,0.034,0.050,0.073,0.107,0.157,0.231,0.340,0.500 \\\\}\\), \\(\\rho = 0.25\\), 60 epochs, 80000 (0.2) snapshots\n\n\n\n\n\n\n\nstag0149 Predictions\nstag0149 Loss\n\n\n\n\n\n\n\n\n\n\n\n\n6. Running Same Parameters Multiple Times\nstag0149, leaflet5121 and branch3151 are different runs of the same parameters (see Section 5).\n\n\n\n\n\n\n\nstag0149 Predictions\nstag0149 Loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nleaflet5121 Predictions\nleaflet5121 Loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbranch3151 Predictions\nbranch3151 Loss\n\n\n\n\n\n\n\n\n\n# 7. Auto-Correlation Function\ndef overlap(traj,i,j):\n   N = traj[i][traj[i]&gt;0].shape[0]\n   return ((traj[i]&gt;0)*(traj[j]&gt;0)).sum()/N\n\ndef acf_analysis (tumble,density,lags,data):\n   acf =[]\n   for i in range(0,500,10):\n       a = [overlap(data,i,i+lag) for lag in lags]\n       acf.append(a)\n\n   acf = np.asarray(acf).mean(axis=0)\n   acf = acf-acf[-1]\n\n   acf/= acf.ptp()\n   return acf\n\n\\(\\rho = 0.15\\)\n\n\n\n\n\n\n\nFull Image\nZoomed In\n\n\n\n\n\n\n\n\n\n\n\n\\(\\rho = 0.25\\)\n\n\n\nFull Image\nZoomed In\n\n\n\n\n\n\n\n\n\n\n\n\\(\\rho = 0.35\\)\n\n\n\n\n\n\n\nFull Image\nZoomed In\n\n\n\n\n\n\n\n\n\n\n\n\\(\\rho = 0.5\\)\n\n\n\n\n\n\n\nFull Image\nZoomed In\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "acf.html",
    "href": "acf.html",
    "title": "Dissipation Learning in Active Matter",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport h5py\nimport natsort\n\nwith h5py.File('dataset_tumble_0.016_density_0.25.h5') as fin:\n    # data = fin[[fin.keys()]]\n    data= [fin [i][:] for i in natsort.natsorted(fin.keys())]\n\n\ndata = np.array(data)\n\n\ndef overlap(traj,i,j):\n    N = traj[i][traj[i]&gt;0].shape[0]\n    return ((traj[i]&gt;0)*(traj[j]&gt;0)).sum()/N\n\n\nlags = np.arange(0, 100+1,1)\nacf =[]\nfor i in range(0,500,10):\n    a = [overlap(data,i,i+lag) for lag in lags]\n    acf.append(a)\n\nacf = np.asarray(acf).mean(axis=0)\nacf = acf-acf[-1]\n\nacf/= acf.ptp()\n\n\nplt.plot(lags,acf)\nplt.xscale('log')\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "motivational-report/motivational-report.html",
    "href": "motivational-report/motivational-report.html",
    "title": "Motivational Report",
    "section": "",
    "text": "Motivational Report\n\n\n0. Table of Contents\n\nTable of Contents\nIntroduction\nOriginal Motivational Report\nSupervisor Feedback\nResponding to Supervisor Feedback\n\n\n\n1. Introduction\nThis is the landing spot for work on the Motivational Report. It was initially prompted in Week 2 as a way to get familiarised with the literature; it is expected to be a considerable part of the Interim Report, whose deadline is in Week 9.\n\n\n2. Original Motivational Report (Week 2)\n\nPasted from “Motivational Report” chapter in Week 2\nActive matter is, broadly, a subcategory of matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel)[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4]). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n499 words.\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n3. Supervisor Feedback (Week 3)\n\nComments prefaced with FT and hyperlinked, commented initially on week2.md in main repository (pure markdown version of Week2)\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation )[1]. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature. FT very good\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[8]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\n502 words.\n\n\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n4. Responding to Supervisor Feedback Week 4\nMy original context is presented, with the supervisor comments hyperlinked and prefaced by “FT”. Underneath I add my own comments in the form of bullet points. Only the commented parts of the report are shown.\nActive matter is, broadly, a subcategory of matter systems FT “matter systems is unclear distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a force-free medium (for instance, the forces between particles and the fluid they move through cancel FT not exactly. Theres i no mechanical equilibrium. On the contrary, there is dissipation\n\nHere I was looking for a broad category to place active matter into; matter systems is indeed too vague. I would have been better off calling it a subcategory of soft matter systems.\nI don’t know exactly where I got the mechanical equilibrium confusion. I may have read some very specific thing that I generalised, but yes, dissipation ought to happen - one of the most important aspects of active matter is the requirement of supplying each autonomous agent with a steady energy supply which they steadily (or perhaps not so steadily in more complex models) use up.\n\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the natural world FT be more precise: it is the world of living organisms, which constantly dissipate energy to perform their biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2]. This is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. FT: You are onto something here. Physicist Andrea Cavagna likes to say that “Physics gauges the surprise in biology”Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3] (note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics under duress might deal holistically, rather than individually, with other human agents[4] FT not clear to me, please explain ). Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by the medical sciences[5].\n\nI forgot that ‘natural world’ in English tends to refer more to general physical processes rather than specifically living organisms; I’ll try to be more specific regarding what active matter models help with understanding.\nFrom the brief look I managed to take at the literature, it seems that discussion of human behaviour in terms of physical systems is quite contentious. In hindsight, I should spend more than a sentence explaining this: the ‘cognitive heuristics’ argument for holism refers to the way humans deal with other humans in immediate crises. Many models will have an individual agent deal with other (in some way) adjacent agents individually; that is to say, it defines its relationship to each agent in turn, and then computes its behaviour. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds (still) as a collective, and take actions in relation to the collective itself. At the time of writing this, it is unclear to me whether there are any active matter models that apply this ‘holistic’ method; the writers I cited, I believe, were criticising the models that do not attempt to do so. This is the case with the basic models I have engaged with so far (such as ABPs). It’s hard to imagine (though not impossible) how such a model can be implemented, but I don’t doubt that newer human-tracking physical models might work in this direction.\nEither way, I’ll look into Andrea Cavagna’s work. I’m interested in exploring this point more in detail.\n\nOutside of biology, active matter research serves to emulate, or otherwise learn from naturally-occurring behaviours in order to analyse a potentially more general thermodynamic state FT “state” is not a good word. Are you thinking about a more general thermodynamic framework? . Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibrium can be modified and generalised to non-equilibrium states, and how these generalisations hold as departure from equilibrium through various means is increased[6] FT: not easy to read, but the idea is important: we can be just slight off equilibrium, and have a so-called linear-response regime, or we could be beyond linear response . These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[7].\n\nI take the point that ‘state’ is the wrong word; another loss in translation. I did mean a more general thermodynamic framework; thermodynamic ‘state’ implies thermal equilibrium, which is exactly what active matter does not have!\nI do get a bit long-winded here; I’ll try to rephrase this paragraph a bit and make sentences more readable\n\nFT: You could get into more specifics, illustrating some examples of interesting behaviorus such as pattern formation or phase separation\n\nYes, I’ll look into examples of pattern formation, as those tend to be quite demonstrative of what active matter study can do.\n\n#5. Fleshing out Motivational Report Weeks 5-6\nActive matter is, broadly, a subcategory of condensed matter systems distingushed primarily by energy input homogenously distributed across all constituents (agents) of the system, which in turn set their own self-propelled motion across a medium. The agents therefore have direct access to energy, and use it to autonomously propel and direct themselves (with different models and situations allowing for various degrees of freedom). The study of active matter is generally grounded in (but not limited to) observed behaviour of biological agents, as they are the primary (though not only) examples of active matter in nature.\nThe evident motivation in studying active matter is that it helps understand biological behaviours, and therefore the world of living organisms, where energy is constantly dissipated in order to perform various biological functions. Macroscopically, the construction of theoretical models can help explain, and to a limited degree predict, the behaviour of animals (such as locusts) undergoing collectively-emergent swarming behaviours (where each animal can be treated as its own autonomous agent, sharing the same generally stable ‘rules’ of altering speed and orientation while interacting with each other and the environment)[2].\nThis biological emulation through physical models is not limited to what can be termed ‘simple’ behaviour; human behaviour can be partially mapped and understood within physically-indexed accounts of autonomous choices within (overtly or suggestively) constrained collective action. Interesting examples are swarming behaviours identified in traffic, crowd disasters and concerts[3]. Note however that physical models are sometimes challenged in literature due to potential oversimplifications, insofar as, for instance, cognitive heuristics (the autonomous individual behaviour of a human) under duress might deal holistically, rather than individually, with other human agents[4]. The issue is that most active matter systems only form individual relationships between agents, and do not account for the way an agent interacts with the group as a whole - the resulting individual behaviour is merely a summation of the agent’s response to each other agent around it. There are psychological arguments that this is not the case, and that instead humans might under duress conceptualise crowds as a collective, and take actions in relation to the collective itself. This objection rests on the assumption that this holistic heuristic does not emerge from individual relations, of course (in which case mapping relationships strictly between individuals is unproblematic).\nThese insights lead to the exploration of various models. For flocks of birds, individual cogntive heuristics tend to suffice - self-propelled particles with adaptive movement patterns based on neighbours can accurately reproduce some migrational patterns [5]. Microscopically, active matter models offer insight into understanding how hierarchically-organised emergence happens within cell tissues, and how it may be leveraged by medicine[6]. Bacteria lends a great example for exploring the intertwining of phenomena to be emulated by active matter. Some strains (such as Bacillus subtilis) can be modelled using both direct physical interaction (between individuals) and long-distance biochemical signalling (within the collective), with complexity and clustering developing in response to harsh external conditions [7]. The latter interaction is called quorum sensing, the adaptation of the individual to local population density; this has developed into its own active matter branch of individual-to-collective behaviour [8]. Using such models, it is possible to recover the aforementioned human holistic cognitive heuristics [9].\nOutside of biology, active matter research serves to emulate, or otherwise learn from, naturally-occurring behaviours in order to analyse a potentially more general thermodynamic framework. Due to the necessarily dissipative use of energy within self-organised agents, and their internally-induced behaviour, active matter is not described by the statistical mechanics of equilibrium states. The question then arises whether, through quantitative computation and qualitative modelling/theorising, the thermodynamic laws of equilibria can be modified and generalised to non-equilibrium states. Exploring how these generalisations would hold as departure from equilibrium through various means is increased is then paramount[10]. These generalisations would, ideally, collapse into the known statistical thermodynamics states within the equilibrium limit. These insights, in turn, would facilitate the creation of synthetic active matter, whose potential, although speculatory, ranges from the biomedical application of nanomachine targeted drug delivery possibilities to the location-mapping application of nanoscopic/microscopic enivronmental sensing[11].\nThe feature in active matter of converting stored and homogenously available energy, such as chemical potential, into mechanical work is also of great importance to the field: understanding how this can work and how to facilitate, among other things, long-term energy access across the active matter substance is a key pursuit of nanotechnology[12]. Statistical and computational models can lend insight into individual and collective dynamics, and in turn give way to new experimental designs of nano/micromechanical systems.\n756 words.\n\nReferences\n\nActive matter: quantifying the departure from equilibrium. Flenner & Szamel\nFrom Disorder to Order in Marching Locusts. Buhl et al. (2006)\nCollective Motion of Humans in Mosh and Circle Pits at Heavy Metal Concerts. Silverberg et al. (2013)\nHow simple rules determine pedestrian behavior and crowd disasters. Moussaid, Helbing & Theraulaz (2011)\nNovel Type of Phase Transition in a System of Self-Driven Particles, Vicsek et al. (1995)\nActive matter at the interface between materials science and cell biology. Needleman & Zvonimir (2017)\nFormation of complex bacterial colonies via self-generated vortices, Czirok et al. (1996)\nSelf-organization of active particles by quorum sensing rules, Bäuerle et al. (2018)\nQuorum sensing as a mechanism to harness the wisdom of the crowds, Moreno-Gámez et al. (2023)\nPhase Separation and Multibody Effects in Three-Dimensional Active Brownian Particles. Turci & Wilding (2021)\nNano/Micromotors in Active Matte. Lv, Yank & Li (2022)\nCatalytic Nanomotors: Autonomous Movement of Striped Nanorods, Paxton et al. (2004)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/repo.html",
    "href": "shared/repo.html",
    "title": "Repository information",
    "section": "",
    "text": "Forking\nLocal development\nManaging branches and commits\nInitialise a Quarto project\nBuilding pages\nPush the changes\nAccessing the live website\nA brief guide on how to setup this website elsewhere and develop the wiki."
  },
  {
    "objectID": "shared/repo.html#forking",
    "href": "shared/repo.html#forking",
    "title": "Repository information",
    "section": "Forking",
    "text": "Forking\n\nIf using a Codeberg account, fork this repo.\nIf using a GitHub account, fork this repo."
  },
  {
    "objectID": "shared/repo.html#local-development",
    "href": "shared/repo.html#local-development",
    "title": "Repository information",
    "section": "Local development",
    "text": "Local development\nIt’s recommended to use SSH and SSH keys when working with a remote git.\n\nKey generating instructions are found here (this is git-server agnostic).\nFor GitHub, follow this to add your key.\nFor Codeberg, it’s similar, navigate to this link when logged in, and then Add key under the Manage SSH Keys section.\n\nClone the repo locally:\n\nFor Codeberg:\n\ngit clone git@codeberg.org:&lt;user&gt;/msci-wiki.git\n\nFor GitHub:\n\ngit clone git@github.com:&lt;user&gt;/msci-wiki.git"
  },
  {
    "objectID": "shared/repo.html#managing-branches-and-commits",
    "href": "shared/repo.html#managing-branches-and-commits",
    "title": "Repository information",
    "section": "Managing branches and commits",
    "text": "Managing branches and commits\nMake a new branch to build your own website from with\ngit checkout -b &lt;make_your_own_unique_name&gt;\nThis branch will contain individual work + shared stuff. It should be the source of truth.\nFrom now on, we will refer to this branch as my_branch.\nWhen contributing to shared knowledge, switch to shared branch with git checkout shared. In this branch, there should be only one directory called shared. This is the root folder of all shared information. Any further directory structuring occurs within this, not outside of it.\n\nNo individual work goes into the shared branch to avoid conflicts with filenames!\n\nWhen merging new commits containing shared information from shared to my_branch, switch to my_branch with git checkout my_branch, and then perform a merge with\ngit merge shared\n\nDon’t merge from my_branch to shared!\n\nIn the case there are problematic commits in shared, future merge can be done with cherry-pick. Say this is shared:\nSHA0 (HEAD): I want this commit\nSHA1: I don't want this commit\nSHA2: I want this commit\nThen, in my_branch:\ngit cherry-pick SHA2\ngit cherry-pick SHA0"
  },
  {
    "objectID": "shared/repo.html#initialise-a-quarto-project",
    "href": "shared/repo.html#initialise-a-quarto-project",
    "title": "Repository information",
    "section": "Initialise a Quarto project",
    "text": "Initialise a Quarto project\nIn my_branch, make a new Quarto project with the website template:\nquarto create project website\nTake a look at _quarto.yml from branch np for an example of how to configure it."
  },
  {
    "objectID": "shared/repo.html#building-pages",
    "href": "shared/repo.html#building-pages",
    "title": "Repository information",
    "section": "Building pages",
    "text": "Building pages\nQuarto by default publishes to the branch gh-pages.\nTo build the pages, first, run the following:\nquarto publish gh-pages\nThis will build and push to the gh-pages branch.\n\nIf using GitHub, that’s all you need to do.\nIf using Codeberg, you can view it by going to https://username.codeberg.page/@gh-pages"
  },
  {
    "objectID": "shared/repo.html#push-the-changes",
    "href": "shared/repo.html#push-the-changes",
    "title": "Repository information",
    "section": "Push the changes",
    "text": "Push the changes\nPushing changes to remote servers make rolling back very difficult, make sure everything looks correct first. Use git log and git status to ensure everything has been committed. To push everything to a mirror:\ngit push &lt;remote&gt; &lt;branch&gt;\nTry to keep all branches up-to-date on all mirrors!"
  },
  {
    "objectID": "shared/repo.html#accessing-the-live-website",
    "href": "shared/repo.html#accessing-the-live-website",
    "title": "Repository information",
    "section": "Accessing the live website",
    "text": "Accessing the live website\n\nFor Codeberg, it’s https://username.codeberg.page/msci-wiki/@gh-pages (more information).\nFor GitHub, it’s https://username.github.io/msci-wiki (more information)."
  },
  {
    "objectID": "shared/ftnotes/label.html",
    "href": "shared/ftnotes/label.html",
    "title": "Minimal cluster analysis",
    "section": "",
    "text": "from scipy import ndimage\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nI create a random binary image (through thresholding).\n\nimage = np.random.uniform(0, 1, (128, 128))\nim = (image &gt; 0.8).astype(int)\n\nplt.matshow(im)\n\n&lt;matplotlib.image.AxesImage at 0x1272e1130&gt;\n\n\n\n\n\nI can then label connected regions, by specifying the structuring element (kernel).\n\nkernel = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\nlabelled, nlabels = ndimage.label(im, structure=kernel)\n\n\nplt.imshow(labelled, cmap=plt.cm.rainbow)\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x12736cc70&gt;\n\n\n\n\n\nI count the number of pixels with a certain label (ignoring label 0 because it is the background)\n\ncluster_sizes = np.bincount(labelled.flatten())[1:]\n\nAnd plot the probability distribution with logarithmically spaced bins\n\nminimum = cluster_sizes.min()\nmaximum = cluster_sizes.max()\nbin_edges = np.logspace(np.log2(minimum), np.log2(maximum), 32, base=2)\nhist, edges = np.histogram(cluster_sizes, bins=bin_edges, density=True)\nplt.plot(2 ** bin_edges[:-1], hist, \"o\")\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.xlabel(\"cluster sizes\")\nplt.ylabel(\"pdf\")\n\nText(0, 0.5, 'pdf')\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "shared/cluster.html",
    "href": "shared/cluster.html",
    "title": "Cluster information",
    "section": "",
    "text": "Project code: PHYS030564\n\n\n\n Back to top"
  }
]